# Using Vertex AI Gemini API via LiteLLM in SEA-HELM\n\nSEA-HELM leverages the LiteLLM library to provide seamless integration with various Large Language Model (LLM) APIs, including the Vertex AI Gemini API. This document outlines how to configure and use the Vertex AI Gemini API (e.g., Gemini 2.5 Flash) for evaluations within the SEA-HELM framework.\n\n## Overview of LiteLLM Integration\n\nThe `serving/litellm_serving.py` module contains the `LiteLLMServing` class, which acts as an interface between SEA-HELM\'s evaluation logic and the LiteLLM library. LiteLLM handles the complexities of interacting with different LLM providers, allowing SEA-HELM to use a unified API for generation requests.\n\n## Configuration for Vertex AI Gemini\n\nTo use the Vertex AI Gemini API, you need to specify `model_type` as `litellm` and provide appropriate `model_name` and `model_args` when running `seahelm_evaluation.py`.\n\n### Key Parameters:\n\n1.  **`--model_type litellm`**:\n    *   This argument tells SEA-HELM to use the `LiteLLMServing` class for model interactions.\n\n2.  **`--model_name \"vertex_ai/gemini-2.5-flash\"` (or `vertex_ai/gemini-2.5-pro`)**:\n    *   The `model_name` argument is crucial for LiteLLM to identify which model and provider to use.\n    *   For Vertex AI, the format is typically `vertex_ai/<model_id>`, where `<model_id>` is the specific Gemini model you wish to use (e.g., `gemini-2.5-flash`, `gemini-2.5-pro`).\n\n3.  **`--model_args \"api_provider=vertex_ai\"`**:\n    *   This argument is passed directly to the `LiteLLMServing` class and then to LiteLLM.\n    *   The `api_provider=vertex_ai` key-value pair explicitly tells LiteLLM to route the requests to the Vertex AI service.\n\n### Example Command:\n\nHere\'s an example of how to run an evaluation using the Vertex AI Gemini 2.5 Flash API:\n\n```bash\npython seahelm_evaluation.py \\\n    --tasks seahelm \\\n    --output_dir output_vertex \\\n    --model_type litellm \\\n    --model_name \"vertex_ai/gemini-2.5-flash\" \\\n    --model_args \"api_provider=vertex_ai\"\n```\n\n### Important Considerations:\n\n*   **Authentication**: Ensure that your environment is properly authenticated to access the Vertex AI API. This typically involves setting up Google Cloud authentication (e.g., `gcloud auth application-default login` or setting the `GOOGLE_APPLICATION_CREDENTIALS` environment variable). LiteLLM will use these credentials automatically.\n*   **Tokenization**: The evaluation framework might make an additional call to tokenize prompts for statistics. If your Vertex AI setup does not expose a tokenization endpoint or you encounter issues, you can set the flag `--skip_tokenize_prompts` in your command:\n    ```bash\n    python seahelm_evaluation.py \\\n        --tasks seahelm \\\n        --output_dir output_vertex \\\n        --model_type litellm \\\n        --model_name \"vertex_ai/gemini-2.5-flash\" \\\n        --model_args \"api_provider=vertex_ai\" \\\n        --skip_tokenize_prompts\n    ```\n*   **SSL Verification**: If you need to disable SSL verification (e.g., for local development with self-signed certificates), you can pass `ssl_verify=False` in `model_args`:\n    ```bash\n    python seahelm_evaluation.py \\\n        --tasks seahelm \\\n        --output_dir output_vertex \\\
        --model_type litellm \\\
        --model_name \"vertex_ai/gemini-2.5-flash\" \\\
        --model_args \"api_provider=vertex_ai,ssl_verify=False\"\n    ```\n\nBy following these steps, you can effectively integrate and evaluate models using the Vertex AI Gemini API within the SEA-HELM framework.\n