#!/bin/bash
# run_local_test

MODEL_NAME="meta-llama/Llama-3-2-3B-Instruct"
OUTPUT_DIR="output_local_test"

echo 'python seahelm_evaluation.py --tasks seahelm --output_dir $OUTPUT_DIR --model_type vllm --model_name $MODEL_NAME --model_args "dtype=bfloat16,enable_prefix_caching=True,tensor_parallel_size=1"'

python seahelm_evaluation.py --tasks seahelm --output_dir $OUTPUT_DIR --model_type litellm --model_name $MODEL_NAME --model_args "api_provider=ollama,base_url=http://localhost:11434"
