To view the log, run:
  $ tail -f output-vertex_ai/08-29-11-59-30/run-vertex_ai.out

python seahelm_evaluation.py --tasks seahelm --model_type litellm --output_dir output-vertex_ai/08-29-11-59-30 --model_name gemini-2.5-flash --model_args api_provider=vertex_ai --skip_tokenize_prompts --limit 2 
2025-08-29 11:59:39 | INFO     | seahelm_evaluation   | Loading model gemini-2.5-flash using VERTEX_AI...
2025-08-29 11:59:39 | INFO     | seahelm_evaluation   | ---------- Preparation of output folder ----------
                                                      | Preparing output folder ...
                                                      | Folder: output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference
2025-08-29 11:59:39 | INFO     | seahelm_evaluation   | Completed preparation of output folder!
                                                      | 
2025-08-29 11:59:39 | INFO     | seahelm_evaluation   | <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
                                                      | Evaluating gemini-2.5-flash as instruction-tuned model...
                                                      | <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
2025-08-29 11:59:41 | INFO     | seahelm_evaluation   | ---------- Configuration saving ----------
                                                      | Saving run config to output folder...
                                                      | Filepath: output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/gemini-2.5-flash_run_config_2025-08-29T11:59:39.954348.yaml
2025-08-29 11:59:41 | INFO     | seahelm_evaluation   | Config file saved!
                                                      | 
2025-08-29 11:59:41 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: MT-BENCH ----------
                                                      | Testing Competency: MULTI-TURN
2025-08-29 11:59:41 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/multi_turn/mt_bench/data/id_sea_mt_bench.jsonl
2025-08-29 11:59:41 | INFO     | seahelm_evaluation   | Performing inference for task 'MT-BENCH' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 11:59:41 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.12 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.11 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.35 examples/s]
[92m11:59:41 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:59:41 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:59:41 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:59:41 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:59:46 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:59:46 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m11:59:46 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:59:46 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 170.67 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 285.65 examples/s]
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 11:59:46 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.53 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.29 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.46 examples/s]
[92m11:59:47 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:59:47 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:59:47 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:59:47 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:59:52 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:59:52 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m11:59:52 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:59:52 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 208.25 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 275.84 examples/s]
2025-08-29 11:59:52 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_id.jsonl
2025-08-29 11:59:52 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:59:52 | INFO     | seahelm_evaluation   | Inference for task 'MT-BENCH' completed!
                                                      | 
2025-08-29 11:59:52 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: MT-BENCH ----------
                                                      | Testing Competency: MULTI-TURN
2025-08-29 11:59:52 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/multi_turn/mt_bench/data/vi_sea_mt_bench.jsonl
2025-08-29 11:59:52 | INFO     | seahelm_evaluation   | Performing inference for task 'MT-BENCH' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 11:59:52 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.71 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.36 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.56 examples/s]
[92m11:59:53 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:59:53 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:59:53 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:59:53 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:59:58 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:59:58 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m11:59:58 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:59:58 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 245.73 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 302.31 examples/s]
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 11:59:58 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.77 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.40 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.58 examples/s]
[92m11:59:58 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:59:58 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:59:58 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:59:58 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:03 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:03 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:03 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:03 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 222.80 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 285.16 examples/s]
2025-08-29 12:00:03 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_vi.jsonl
2025-08-29 12:00:03 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:03 | INFO     | seahelm_evaluation   | Inference for task 'MT-BENCH' completed!
                                                      | 
2025-08-29 12:00:03 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: MT-BENCH ----------
                                                      | Testing Competency: MULTI-TURN
2025-08-29 12:00:03 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/multi_turn/mt_bench/data/mt_bench_thai_full.jsonl
2025-08-29 12:00:03 | INFO     | seahelm_evaluation   | Performing inference for task 'MT-BENCH' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:03 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.75 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.36 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.56 examples/s]
[92m12:00:04 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:04 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:04 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:04 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:06 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:06 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:09 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:09 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 258.76 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 321.78 examples/s]
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:09 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.55 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.31 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.51 examples/s]
[92m12:00:10 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:10 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:10 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:10 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:14 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:14 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:14 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:14 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 234.36 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 305.80 examples/s]
2025-08-29 12:00:14 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_th.jsonl
2025-08-29 12:00:14 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:14 | INFO     | seahelm_evaluation   | Inference for task 'MT-BENCH' completed!
                                                      | 
2025-08-29 12:00:14 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: MT-BENCH ----------
                                                      | Testing Competency: MULTI-TURN
2025-08-29 12:00:14 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/multi_turn/mt_bench/data/mt_bench_tagalog_full.jsonl
2025-08-29 12:00:15 | INFO     | seahelm_evaluation   | Performing inference for task 'MT-BENCH' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:15 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.65 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.45 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.60 examples/s]
[92m12:00:15 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:15 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:15 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:15 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:20 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:20 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:21 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:21 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 276.22 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 302.77 examples/s]
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:21 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.62 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.42 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.58 examples/s]
[92m12:00:21 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:21 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:21 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:21 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:26 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:26 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:26 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:26 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 237.49 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 307.71 examples/s]
2025-08-29 12:00:26 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_tl.jsonl
2025-08-29 12:00:26 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:26 | INFO     | seahelm_evaluation   | Inference for task 'MT-BENCH' completed!
                                                      | 
2025-08-29 12:00:26 | INFO     | seahelm_evaluation   | Starting mt-bench evaluation using multiprocessing
2025-08-29 12:00:26 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: SENTIMENT ----------
                                                      | Testing Competency: NLU
2025-08-29 12:00:26 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/sentiment_analysis/data/id_nusax.jsonl
2025-08-29 12:00:26 | INFO     | mt_bench             | --------- Evaluation | Lang: ID | Task: MT-BENCH ----------
2025-08-29 12:00:26 | INFO     | mt_bench             | Evaluating MT-BENCH using MTBenchMetric
2025-08-29 12:00:26 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:00:26 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:00:26 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:00:26 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:00:26 | INFO     | mt_bench             | --------- Evaluation | Lang: VI | Task: MT-BENCH ----------
2025-08-29 12:00:26 | INFO     | mt_bench             | Evaluating MT-BENCH using MTBenchMetric
2025-08-29 12:00:26 | INFO     | mt_bench             | First run: processing all judgments.
2025-08-29 12:00:26 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:00:26 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:00:26 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:00:26 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:00:26 | INFO     | mt_bench             | --------- Evaluation | Lang: TH | Task: MT-BENCH ----------
2025-08-29 12:00:26 | INFO     | mt_bench             | Evaluating MT-BENCH using MTBenchMetric
2025-08-29 12:00:26 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:00:26 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:00:26 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:00:26 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:00:26 | INFO     | mt_bench             | --------- Evaluation | Lang: TL | Task: MT-BENCH ----------
2025-08-29 12:00:26 | INFO     | mt_bench             | Evaluating MT-BENCH using MTBenchMetric
2025-08-29 12:00:26 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:00:26 | INFO     | mt_bench             | First run: processing all judgments.
2025-08-29 12:00:26 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:00:26 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:00:26 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:00:26 | INFO     | mt_bench             | First run: processing all judgments.
2025-08-29 12:00:26 | INFO     | mt_bench             | First run: processing all judgments.
2025-08-29 12:00:27 | INFO     | seahelm_evaluation   | Performing inference for task 'SENTIMENT' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:27 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.39 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.24 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.47 examples/s]
[92m12:00:27 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:27 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:27 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:27 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:27 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:27 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:28 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:28 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 287.76 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 345.32 examples/s]
2025-08-29 12:00:28 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_id.jsonl
2025-08-29 12:00:28 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:28 | INFO     | seahelm_evaluation   | Inference for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:00:28 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: SENTIMENT ----------
2025-08-29 12:00:28 | INFO     | seahelm_evaluation   | Evaluating 'SENTIMENT' using SentimentAnalysisMetric
2025-08-29 12:00:28 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:00:28 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:00:28 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:00:28 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:00:28 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:00:28 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1 0]
                                                      |  [0 0 0]
                                                      |  [0 1 0]]
2025-08-29 12:00:28 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |      neutral       0.00      0.00      0.00       1.0
                                                      |         none       0.00      0.00      0.00       0.0
                                                      |     positive       0.00      0.00      0.00       1.0
                                                      | 
                                                      |     accuracy                           0.00       2.0
                                                      |    macro avg       0.00      0.00      0.00       2.0
                                                      | weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:00:28 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:00:28 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_id.jsonl
2025-08-29 12:00:28 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:28 | INFO     | seahelm_evaluation   | Evaluation for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:00:28 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: QA ----------
                                                      | Testing Competency: NLU
2025-08-29 12:00:28 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/question_answering/data/id_tydiqa_100sample.jsonl
2025-08-29 12:00:28 | INFO     | seahelm_evaluation   | Performing inference for task 'QA' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:28 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.61 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.40 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.63 examples/s]
[92m12:00:28 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:28 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:28 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:28 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:29 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:29 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:29 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:29 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 300.37 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 328.90 examples/s]
2025-08-29 12:00:29 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_qa_id.jsonl
2025-08-29 12:00:29 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:29 | INFO     | seahelm_evaluation   | Inference for task 'QA' completed!
                                                      | 
2025-08-29 12:00:29 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: QA ----------
2025-08-29 12:00:32 | INFO     | seahelm_evaluation   | Evaluating 'QA' using QuestionAnsweringMetric
2025-08-29 12:00:32 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:00:32 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:00:32 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:00:32 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:00:32 | INFO     | question_answering   | {'exact_match': 50.0, 'f1': 70.0, 'normalized_f1': 70.0}
2025-08-29 12:00:32 | INFO     | question_answering   | 2 answers out of 2 (100.00%) can be found in the model's predictions.
2025-08-29 12:00:32 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:00:32 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_qa_id.jsonl
2025-08-29 12:00:32 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:32 | INFO     | seahelm_evaluation   | Evaluation for task 'QA' completed!
                                                      | 
2025-08-29 12:00:32 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: METAPHOR ----------
                                                      | Testing Competency: NLU
2025-08-29 12:00:32 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/metaphor/data/id_multilingual_fig_qa.jsonl
2025-08-29 12:00:32 | INFO     | seahelm_evaluation   | Performing inference for task 'METAPHOR' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:32 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.47 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.22 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.41 examples/s]
[92m12:00:33 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:33 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:33 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:33 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:33 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:33 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:33 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:33 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 280.17 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 343.63 examples/s]
2025-08-29 12:00:33 | INFO     | seahelm_evaluation   | Saving inference results for task 'METAPHOR' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_metaphor_id.jsonl
2025-08-29 12:00:33 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:33 | INFO     | seahelm_evaluation   | Inference for task 'METAPHOR' completed!
                                                      | 
2025-08-29 12:00:33 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: METAPHOR ----------
2025-08-29 12:00:33 | INFO     | seahelm_evaluation   | Evaluating 'METAPHOR' using MetaphorUnderstandingMetric
2025-08-29 12:00:33 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:00:33 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:00:33 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:00:33 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:00:33 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:00:33 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 0 1]
                                                      |  [0 0 1]
                                                      |  [0 0 0]]
2025-08-29 12:00:33 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       1.0
                                                      |            1       0.00      0.00      0.00       1.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       2.0
                                                      |    macro avg       0.00      0.00      0.00       2.0
                                                      | weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:00:33 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:00:33 | INFO     | seahelm_evaluation   | Saving inference results for task 'METAPHOR' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_metaphor_id.jsonl
2025-08-29 12:00:33 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:33 | INFO     | seahelm_evaluation   | Evaluation for task 'METAPHOR' completed!
                                                      | 
2025-08-29 12:00:33 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: TOXICITY ----------
                                                      | Testing Competency: SAFETY
2025-08-29 12:00:33 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/safety/toxicity_detection/data/id_ml-hsd_1000sample.jsonl
2025-08-29 12:00:33 | INFO     | seahelm_evaluation   | Performing inference for task 'TOXICITY' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:33 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.49 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.93 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.25 examples/s]
[92m12:00:34 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:34 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:34 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:34 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:34 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:34 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:34 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:34 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 224.02 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 269.17 examples/s]
2025-08-29 12:00:34 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_id.jsonl
2025-08-29 12:00:34 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:34 | INFO     | seahelm_evaluation   | Inference for task 'TOXICITY' completed!
                                                      | 
2025-08-29 12:00:34 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: TOXICITY ----------
2025-08-29 12:00:34 | INFO     | seahelm_evaluation   | Evaluating 'TOXICITY' using ToxicityDetectionMetric
2025-08-29 12:00:34 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:00:34 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:00:34 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:00:34 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:00:34 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:00:34 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 2]
                                                      |  [0 0]]
2025-08-29 12:00:34 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       2.0
                                                      |            3       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       2.0
                                                      |    macro avg       0.00      0.00      0.00       2.0
                                                      | weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:00:34 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:00:34 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_id.jsonl
2025-08-29 12:00:34 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:34 | INFO     | seahelm_evaluation   | Evaluation for task 'TOXICITY' completed!
                                                      | 
2025-08-29 12:00:34 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: TRANSLATION-EN-XX ----------
                                                      | Testing Competency: NLG
2025-08-29 12:00:34 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/en_to_ind_Latn.jsonl
2025-08-29 12:00:34 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-EN-XX' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:34 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.46 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.25 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.42 examples/s]
[92m12:00:35 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:35 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:35 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:35 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:36 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:36 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:36 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:36 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 334.50 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 359.47 examples/s]
2025-08-29 12:00:36 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_id.jsonl
2025-08-29 12:00:36 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:36 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:00:36 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: TRANSLATION-XX-EN ----------
                                                      | Testing Competency: NLG
2025-08-29 12:00:36 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/ind_Latn_to_en.jsonl
2025-08-29 12:00:36 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-XX-EN' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:36 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.42 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.12 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.38 examples/s]
[92m12:00:37 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:37 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:37 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:37 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:37 | INFO     | openai_serving       | Prompts sent via OpenAI batch API
2025-08-29 12:00:37 | INFO     | openai_serving       | Waiting for OpenAI batch to complete...
2025-08-29 12:00:37 | INFO     | openai_serving       | Prompts sent via OpenAI batch API
2025-08-29 12:00:37 | INFO     | openai_serving       | Waiting for OpenAI batch to complete...
2025-08-29 12:00:38 | INFO     | openai_serving       | Prompts sent via OpenAI batch API
2025-08-29 12:00:38 | INFO     | openai_serving       | Waiting for OpenAI batch to complete...
[92m12:00:38 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:38 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:38 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:38 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 338.22 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 360.94 examples/s]
2025-08-29 12:00:38 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_id.jsonl
2025-08-29 12:00:38 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:38 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:00:38 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: ABSSUM ----------
                                                      | Testing Competency: NLG
2025-08-29 12:00:38 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/abstractive_summarization/data/id_xlsum_100sample.jsonl
2025-08-29 12:00:38 | INFO     | seahelm_evaluation   | Performing inference for task 'ABSSUM' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:38 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]2025-08-29 12:00:38 | INFO     | openai_serving       | Prompts sent via OpenAI batch API
2025-08-29 12:00:38 | INFO     | openai_serving       | Waiting for OpenAI batch to complete...
Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.31 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.06 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.31 examples/s]
[92m12:00:39 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:39 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:39 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:39 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:41 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:41 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:42 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:42 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 302.23 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 321.86 examples/s]
2025-08-29 12:00:42 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_id.jsonl
2025-08-29 12:00:42 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:42 | INFO     | seahelm_evaluation   | Inference for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:00:42 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: ABSSUM ----------
2025-08-29 12:00:42 | INFO     | seahelm_evaluation   | Evaluating 'ABSSUM' using SummarizationMetric
2025-08-29 12:00:42 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:00:42 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:00:42 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:00:42 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:00:42 | INFO     | summarization        | Rouge-L Scores:
2025-08-29 12:00:42 | INFO     | summarization        | Precision: 5.26 | Recall: 10.53 | F1: 7.02
2025-08-29 12:00:42 | INFO     | summarization        | Norm F1 Score: 7.02
2025-08-29 12:00:42 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:00:42 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_id.jsonl
2025-08-29 12:00:42 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:42 | INFO     | seahelm_evaluation   | Evaluation for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:00:42 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: CAUSAL ----------
                                                      | Testing Competency: NLR
2025-08-29 12:00:42 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/causal/data/id_xcopa.jsonl
2025-08-29 12:00:42 | INFO     | seahelm_evaluation   | Performing inference for task 'CAUSAL' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:42 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.53 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.03 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.32 examples/s]
[92m12:00:43 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:43 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:43 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:43 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:43 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:43 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:44 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:44 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 246.22 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 335.04 examples/s]
2025-08-29 12:00:44 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_causal_id.jsonl
2025-08-29 12:00:44 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:44 | INFO     | seahelm_evaluation   | Inference for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:00:44 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: CAUSAL ----------
2025-08-29 12:00:44 | INFO     | seahelm_evaluation   | Evaluating 'CAUSAL' using CausalReasoningMetric
2025-08-29 12:00:44 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:00:44 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:00:44 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:00:44 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:00:44 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:00:44 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 2]
                                                      |  [0 0]]
2025-08-29 12:00:44 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       2.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       2.0
                                                      |    macro avg       0.00      0.00      0.00       2.0
                                                      | weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:00:44 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:00:44 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_causal_id.jsonl
2025-08-29 12:00:44 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:44 | INFO     | seahelm_evaluation   | Evaluation for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:00:44 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: NLI ----------
                                                      | Testing Competency: NLR
2025-08-29 12:00:44 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/nli/data/id_indonli_lay_1000sample.jsonl
2025-08-29 12:00:44 | INFO     | seahelm_evaluation   | Performing inference for task 'NLI' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:44 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.22 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.20 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.35 examples/s]
[92m12:00:44 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:44 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:44 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:44 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:45 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:45 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:45 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:45 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 300.58 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 385.20 examples/s]
2025-08-29 12:00:45 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_nli_id.jsonl
2025-08-29 12:00:45 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:45 | INFO     | seahelm_evaluation   | Inference for task 'NLI' completed!
                                                      | 
2025-08-29 12:00:45 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: NLI ----------
2025-08-29 12:00:45 | INFO     | seahelm_evaluation   | Evaluating 'NLI' using NLIMetric
2025-08-29 12:00:45 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:00:45 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:00:45 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:00:45 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:00:45 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:00:45 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 2]
                                                      |  [0 0]]
2025-08-29 12:00:45 | INFO     | seahelm_metric       | Classification report:
                                                      |                precision    recall  f1-score   support
                                                      | 
                                                      | contradiction       0.00      0.00      0.00       2.0
                                                      |          none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |      accuracy                           0.00       2.0
                                                      |     macro avg       0.00      0.00      0.00       2.0
                                                      |  weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:00:45 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:00:45 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_nli_id.jsonl
2025-08-29 12:00:45 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:45 | INFO     | seahelm_evaluation   | Evaluation for task 'NLI' completed!
                                                      | 
2025-08-29 12:00:45 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: MP-R ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 12:00:45 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/syntax/data/id_syntax_mcq_randomized.jsonl
2025-08-29 12:00:45 | INFO     | seahelm_evaluation   | Performing inference for task 'MP-R' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:45 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.38 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.21 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.37 examples/s]
[92m12:00:46 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:46 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:46 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:46 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:46 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:46 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:47 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:47 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 240.42 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 305.17 examples/s]
2025-08-29 12:00:47 | INFO     | seahelm_evaluation   | Saving inference results for task 'MP-R' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_mp-r_id.jsonl
2025-08-29 12:00:47 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:47 | INFO     | seahelm_evaluation   | Inference for task 'MP-R' completed!
                                                      | 
2025-08-29 12:00:47 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: MP-R ----------
2025-08-29 12:00:47 | INFO     | seahelm_evaluation   | Evaluating 'MP-R' using MinimalPairsMetric
2025-08-29 12:00:47 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:00:47 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:00:47 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:00:47 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:00:47 | INFO     | minimal_pairs        | Accuracy for phenomenon <NPIs_and_negation>: 0.0
2025-08-29 12:00:47 | INFO     | minimal_pairs        | Overall Accuracy: 0.0
2025-08-29 12:00:47 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:00:47 | INFO     | seahelm_evaluation   | Saving inference results for task 'MP-R' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_mp-r_id.jsonl
2025-08-29 12:00:47 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:47 | INFO     | seahelm_evaluation   | Evaluation for task 'MP-R' completed!
                                                      | 
2025-08-29 12:00:47 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: PRAGMATIC-SINGLE ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 12:00:47 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/pragmatics/data/id_pragmatic_reasoning_single.jsonl
2025-08-29 12:00:47 | INFO     | seahelm_evaluation   | Performing inference for task 'PRAGMATIC-SINGLE' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:47 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.41 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.16 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.36 examples/s]
[92m12:00:47 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:47 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:47 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:47 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:48 | INFO     | openai_serving       | Still waiting (10s has elapsed)...
[92m12:00:48 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:00:48 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:00:48 | INFO     | openai_serving       | Still waiting (10s has elapsed)...
[92m12:00:48 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:00:48 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:00:48 | INFO     | openai_serving       | Still waiting (10s has elapsed)...
[92m12:00:48 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:00:48 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:00:48 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:48 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:48 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:48 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 243.19 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 303.10 examples/s]
2025-08-29 12:00:48 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-SINGLE' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-single_id.jsonl
2025-08-29 12:00:48 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:48 | INFO     | seahelm_evaluation   | Inference for task 'PRAGMATIC-SINGLE' completed!
                                                      | 
2025-08-29 12:00:48 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: PRAGMATIC-SINGLE ----------
2025-08-29 12:00:48 | INFO     | seahelm_evaluation   | Evaluating 'PRAGMATIC-SINGLE' using PragmaticReasoningSingleSentenceMetric
2025-08-29 12:00:48 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:00:48 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:00:48 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:00:48 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  subset_references = subset[self.label_column].replace(
2025-08-29 12:00:48 | INFO     | pragmatic_reasoning  | Accuracy for phenomenon <scalar_implicatures>: 0.0 / 2
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:93: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  references = self.inference_df[self.label_column].replace(
2025-08-29 12:00:48 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:00:48 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-SINGLE' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-single_id.jsonl
2025-08-29 12:00:48 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:48 | INFO     | seahelm_evaluation   | Evaluation for task 'PRAGMATIC-SINGLE' completed!
                                                      | 
2025-08-29 12:00:48 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: PRAGMATIC-PAIR ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 12:00:48 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/pragmatics/data/id_pragmatic_reasoning_pair.jsonl
2025-08-29 12:00:48 | INFO     | seahelm_evaluation   | Performing inference for task 'PRAGMATIC-PAIR' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:48 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.36 examples/s]2025-08-29 12:00:49 | INFO     | openai_serving       | Still waiting (10s has elapsed)...
[92m12:00:49 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:00:49 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.13 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.34 examples/s]
[92m12:00:49 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:49 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:49 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:49 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:50 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:50 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:50 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:50 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 284.92 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 316.43 examples/s]
2025-08-29 12:00:50 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-PAIR' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-pair_id.jsonl
2025-08-29 12:00:50 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:50 | INFO     | seahelm_evaluation   | Inference for task 'PRAGMATIC-PAIR' completed!
                                                      | 
2025-08-29 12:00:50 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: PRAGMATIC-PAIR ----------
2025-08-29 12:00:50 | INFO     | seahelm_evaluation   | Evaluating 'PRAGMATIC-PAIR' using PragmaticReasoningSentencePairMetric
2025-08-29 12:00:50 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:00:50 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:00:50 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:00:50 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:143: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  subset_references = subset[self.label_column].replace({True: 1, False: 0})
2025-08-29 12:00:50 | INFO     | pragmatic_reasoning  | Accuracy for phenomenon <scalar_implicatures>: 0.0 / 2
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:157: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  references = self.inference_df[self.label_column].replace({True: 1, False: 0})
2025-08-29 12:00:50 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:00:50 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-PAIR' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-pair_id.jsonl
2025-08-29 12:00:50 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:50 | INFO     | seahelm_evaluation   | Evaluation for task 'PRAGMATIC-PAIR' completed!
                                                      | 
2025-08-29 12:00:50 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: IF-EVAL ----------
                                                      | Testing Competency: INSTRUCTION-FOLLOWING
2025-08-29 12:00:50 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/instruction_following/ifeval/data/id_sea_ifeval.jsonl
2025-08-29 12:00:50 | INFO     | seahelm_evaluation   | Performing inference for task 'IF-EVAL' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:50 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.46 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.06 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.27 examples/s]
[92m12:00:50 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:50 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:50 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:50 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:54 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:54 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:55 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:55 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 164.88 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 211.67 examples/s]
2025-08-29 12:00:56 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_id.jsonl
2025-08-29 12:00:56 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:56 | INFO     | seahelm_evaluation   | Inference for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 12:00:56 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: IF-EVAL ----------
2025-08-29 12:00:56 | INFO     | seahelm_evaluation   | Evaluating 'IF-EVAL' using IFEvalMetric
2025-08-29 12:00:56 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:00:56 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:00:56 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:00:56 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:00:56 | INFO     | if_eval              | Overall pass: 1 / 2
2025-08-29 12:00:56 | INFO     | if_eval              | Overall accuracy: 50.000000
2025-08-29 12:00:56 | INFO     | if_eval              | Correct language rate: 0.500000
2025-08-29 12:00:56 | INFO     | if_eval              | Lang normalized accuracy: 50.000000
2025-08-29 12:00:56 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:00:56 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_id.jsonl
2025-08-29 12:00:56 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:56 | INFO     | seahelm_evaluation   | Evaluation for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 12:00:56 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: SENTIMENT ----------
                                                      | Testing Competency: NLU
2025-08-29 12:00:56 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/sentiment_analysis/data/vi_uit-vsfc_1000sample.jsonl
2025-08-29 12:00:56 | INFO     | seahelm_evaluation   | Performing inference for task 'SENTIMENT' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:56 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.16 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  1.89 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.93 examples/s]
[92m12:00:57 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:57 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:57 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:57 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:57 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:57 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:57 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:57 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 328.93 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 395.69 examples/s]
2025-08-29 12:00:57 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_vi.jsonl
2025-08-29 12:00:57 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:57 | INFO     | seahelm_evaluation   | Inference for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:00:57 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: SENTIMENT ----------
2025-08-29 12:00:57 | INFO     | seahelm_evaluation   | Evaluating 'SENTIMENT' using SentimentAnalysisMetric
2025-08-29 12:00:57 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:00:57 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:00:57 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:00:57 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:00:57 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:00:57 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 2]
                                                      |  [0 0]]
2025-08-29 12:00:57 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |     negative       0.00      0.00      0.00       2.0
                                                      |         none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       2.0
                                                      |    macro avg       0.00      0.00      0.00       2.0
                                                      | weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:00:57 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:00:57 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_vi.jsonl
2025-08-29 12:00:57 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:57 | INFO     | seahelm_evaluation   | Evaluation for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:00:57 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: QA ----------
                                                      | Testing Competency: NLU
2025-08-29 12:00:57 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/question_answering/data/vi_xquad_100sample.jsonl
2025-08-29 12:00:57 | INFO     | seahelm_evaluation   | Performing inference for task 'QA' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:57 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]2025-08-29 12:00:58 | INFO     | openai_serving       | Still waiting (20s has elapsed)...
[92m12:00:58 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:00:58 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.31 examples/s]2025-08-29 12:00:58 | INFO     | openai_serving       | Still waiting (20s has elapsed)...
[92m12:00:58 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:00:58 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.12 examples/s]2025-08-29 12:00:58 | INFO     | openai_serving       | Still waiting (20s has elapsed)...
[92m12:00:58 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:00:58 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.33 examples/s]
[92m12:00:58 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:58 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:00:58 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:58 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:00:59 | INFO     | openai_serving       | Still waiting (20s has elapsed)...
[92m12:00:59 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:00:59 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:00:59 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:59 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:00:59 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:00:59 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 284.40 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 350.28 examples/s]
2025-08-29 12:00:59 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_qa_vi.jsonl
2025-08-29 12:00:59 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:59 | INFO     | seahelm_evaluation   | Inference for task 'QA' completed!
                                                      | 
2025-08-29 12:00:59 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: QA ----------
2025-08-29 12:00:59 | INFO     | seahelm_evaluation   | Evaluating 'QA' using QuestionAnsweringMetric
2025-08-29 12:00:59 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:00:59 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:00:59 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:00:59 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:00:59 | INFO     | question_answering   | {'exact_match': 100.0, 'f1': 100.0, 'normalized_f1': 100.0}
2025-08-29 12:00:59 | INFO     | question_answering   | 2 answers out of 2 (100.00%) can be found in the model's predictions.
2025-08-29 12:00:59 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:00:59 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_qa_vi.jsonl
2025-08-29 12:00:59 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:00:59 | INFO     | seahelm_evaluation   | Evaluation for task 'QA' completed!
                                                      | 
2025-08-29 12:00:59 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: TOXICITY ----------
                                                      | Testing Competency: SAFETY
2025-08-29 12:00:59 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/safety/toxicity_detection/data/vi_vihsd_1000sample.jsonl
2025-08-29 12:00:59 | INFO     | seahelm_evaluation   | Performing inference for task 'TOXICITY' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:00:59 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.32 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.12 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.33 examples/s]
[92m12:01:00 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:00 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:00 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:00 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:00 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:00 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:00 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:00 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 329.87 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 359.59 examples/s]
2025-08-29 12:01:00 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_vi.jsonl
2025-08-29 12:01:00 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:00 | INFO     | seahelm_evaluation   | Inference for task 'TOXICITY' completed!
                                                      | 
2025-08-29 12:01:00 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: TOXICITY ----------
2025-08-29 12:01:00 | INFO     | seahelm_evaluation   | Evaluating 'TOXICITY' using ToxicityDetectionMetric
2025-08-29 12:01:00 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:00 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:00 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:00 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:01:00 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:01:00 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 2]
                                                      |  [0 0]]
2025-08-29 12:01:00 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       2.0
                                                      |            3       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       2.0
                                                      |    macro avg       0.00      0.00      0.00       2.0
                                                      | weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:01:00 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:00 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_vi.jsonl
2025-08-29 12:01:00 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:00 | INFO     | seahelm_evaluation   | Evaluation for task 'TOXICITY' completed!
                                                      | 
2025-08-29 12:01:00 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: TRANSLATION-EN-XX ----------
                                                      | Testing Competency: NLG
2025-08-29 12:01:00 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/en_to_vie_Latn.jsonl
2025-08-29 12:01:00 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-EN-XX' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:00 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.30 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.10 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.33 examples/s]
[92m12:01:01 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:01 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:01 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:01 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:02 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:02 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:02 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:02 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 334.11 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 358.12 examples/s]
2025-08-29 12:01:02 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_vi.jsonl
2025-08-29 12:01:02 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:02 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:01:02 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: TRANSLATION-XX-EN ----------
                                                      | Testing Competency: NLG
2025-08-29 12:01:02 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/vie_Latn_to_en.jsonl
2025-08-29 12:01:02 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-XX-EN' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:02 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.31 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.00 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.27 examples/s]
[92m12:01:03 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:03 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:03 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:03 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:04 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:04 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:04 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:04 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 351.59 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 384.57 examples/s]
2025-08-29 12:01:04 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_vi.jsonl
2025-08-29 12:01:04 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:04 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:01:04 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: ABSSUM ----------
                                                      | Testing Competency: NLG
2025-08-29 12:01:04 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/abstractive_summarization/data/vi_xlsum_100sample.jsonl
2025-08-29 12:01:04 | INFO     | seahelm_evaluation   | Performing inference for task 'ABSSUM' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:04 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.25 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.07 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.29 examples/s]
[92m12:01:05 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:05 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:05 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:05 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:07 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:07 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:08 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:08 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 291.92 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 314.83 examples/s]
2025-08-29 12:01:08 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_vi.jsonl
2025-08-29 12:01:08 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:08 | INFO     | seahelm_evaluation   | Inference for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:01:08 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: ABSSUM ----------
2025-08-29 12:01:08 | INFO     | seahelm_evaluation   | Evaluating 'ABSSUM' using SummarizationMetric
2025-08-29 12:01:08 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:08 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:08 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:08 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:01:08 | INFO     | summarization        | Rouge-L Scores:
2025-08-29 12:01:08 | INFO     | summarization        | Precision: 15.69 | Recall: 14.04 | F1: 14.81
2025-08-29 12:01:08 | INFO     | summarization        | Norm F1 Score: 14.81
2025-08-29 12:01:08 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:08 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_vi.jsonl
2025-08-29 12:01:08 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:08 | INFO     | seahelm_evaluation   | Evaluation for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:01:08 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: CAUSAL ----------
                                                      | Testing Competency: NLR
2025-08-29 12:01:08 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/causal/data/vi_xcopa.jsonl
2025-08-29 12:01:08 | INFO     | seahelm_evaluation   | Performing inference for task 'CAUSAL' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:08 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]2025-08-29 12:01:08 | INFO     | openai_serving       | Still waiting (30s has elapsed)...
[92m12:01:08 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:08 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:08 | INFO     | openai_serving       | Still waiting (30s has elapsed)...
[92m12:01:08 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:08 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.47 examples/s]2025-08-29 12:01:08 | INFO     | openai_serving       | Still waiting (30s has elapsed)...
[92m12:01:08 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:08 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.20 examples/s]
[92m12:01:08 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:08 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:08 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:08 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:09 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:09 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:09 | INFO     | openai_serving       | Still waiting (30s has elapsed)...
[92m12:01:09 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:09 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:01:09 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:09 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 293.09 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 317.20 examples/s]
2025-08-29 12:01:09 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_causal_vi.jsonl
2025-08-29 12:01:09 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:09 | INFO     | seahelm_evaluation   | Inference for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:01:09 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: CAUSAL ----------
2025-08-29 12:01:09 | INFO     | seahelm_evaluation   | Evaluating 'CAUSAL' using CausalReasoningMetric
2025-08-29 12:01:09 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:09 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:09 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:09 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:01:09 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:01:09 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 2]
                                                      |  [0 0]]
2025-08-29 12:01:09 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       2.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       2.0
                                                      |    macro avg       0.00      0.00      0.00       2.0
                                                      | weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:01:09 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:09 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_causal_vi.jsonl
2025-08-29 12:01:09 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:09 | INFO     | seahelm_evaluation   | Evaluation for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:01:09 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: NLI ----------
                                                      | Testing Competency: NLR
2025-08-29 12:01:09 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/nli/data/vi_xnli_1000sample.jsonl
2025-08-29 12:01:09 | INFO     | seahelm_evaluation   | Performing inference for task 'NLI' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:09 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.28 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.06 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.30 examples/s]
[92m12:01:10 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:10 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:10 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:10 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:11 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:11 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:11 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:11 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 320.73 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 353.80 examples/s]
2025-08-29 12:01:11 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_nli_vi.jsonl
2025-08-29 12:01:11 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:11 | INFO     | seahelm_evaluation   | Inference for task 'NLI' completed!
                                                      | 
2025-08-29 12:01:11 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: NLI ----------
2025-08-29 12:01:11 | INFO     | seahelm_evaluation   | Evaluating 'NLI' using NLIMetric
2025-08-29 12:01:11 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:11 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:11 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:11 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:01:11 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:01:11 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 2]
                                                      |  [0 0]]
2025-08-29 12:01:11 | INFO     | seahelm_metric       | Classification report:
                                                      |                precision    recall  f1-score   support
                                                      | 
                                                      | contradiction       0.00      0.00      0.00       2.0
                                                      |          none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |      accuracy                           0.00       2.0
                                                      |     macro avg       0.00      0.00      0.00       2.0
                                                      |  weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:01:11 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:11 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_nli_vi.jsonl
2025-08-29 12:01:11 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:11 | INFO     | seahelm_evaluation   | Evaluation for task 'NLI' completed!
                                                      | 
2025-08-29 12:01:11 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: IF-EVAL ----------
                                                      | Testing Competency: INSTRUCTION-FOLLOWING
2025-08-29 12:01:11 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/instruction_following/ifeval/data/vi_sea_ifeval.jsonl
2025-08-29 12:01:11 | INFO     | seahelm_evaluation   | Performing inference for task 'IF-EVAL' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:11 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.25 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.00 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.23 examples/s]
[92m12:01:12 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:12 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:12 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:12 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:14 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:14 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:17 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:17 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 174.93 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 213.03 examples/s]
2025-08-29 12:01:17 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_vi.jsonl
2025-08-29 12:01:17 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:17 | INFO     | seahelm_evaluation   | Inference for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 12:01:17 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: IF-EVAL ----------
2025-08-29 12:01:17 | INFO     | seahelm_evaluation   | Evaluating 'IF-EVAL' using IFEvalMetric
2025-08-29 12:01:17 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:17 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:17 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:17 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:01:17 | INFO     | if_eval              | Overall pass: 1 / 2
2025-08-29 12:01:17 | INFO     | if_eval              | Overall accuracy: 50.000000
2025-08-29 12:01:17 | INFO     | if_eval              | Correct language rate: 0.500000
2025-08-29 12:01:17 | INFO     | if_eval              | Lang normalized accuracy: 50.000000
2025-08-29 12:01:17 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:17 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_vi.jsonl
2025-08-29 12:01:17 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:17 | INFO     | seahelm_evaluation   | Evaluation for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 12:01:17 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: SENTIMENT ----------
                                                      | Testing Competency: NLU
2025-08-29 12:01:17 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/sentiment_analysis/data/th_wisesight_no_q_1000sample.jsonl
2025-08-29 12:01:17 | INFO     | seahelm_evaluation   | Performing inference for task 'SENTIMENT' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:17 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.45 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.06 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.33 examples/s]
[92m12:01:17 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:17 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:17 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:17 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:18 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:18 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:18 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:18 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 312.38 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 337.77 examples/s]
2025-08-29 12:01:18 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_th.jsonl
2025-08-29 12:01:18 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:18 | INFO     | seahelm_evaluation   | Inference for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:01:18 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: SENTIMENT ----------
2025-08-29 12:01:18 | INFO     | seahelm_evaluation   | Evaluating 'SENTIMENT' using SentimentAnalysisMetric
2025-08-29 12:01:18 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:18 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:18 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:18 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:01:18 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:01:18 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 2]
                                                      |  [0 0]]
2025-08-29 12:01:18 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |     negative       0.00      0.00      0.00       2.0
                                                      |         none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       2.0
                                                      |    macro avg       0.00      0.00      0.00       2.0
                                                      | weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:01:18 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:18 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_th.jsonl
2025-08-29 12:01:18 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:18 | INFO     | seahelm_evaluation   | Evaluation for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:01:18 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: QA ----------
                                                      | Testing Competency: NLU
2025-08-29 12:01:18 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/question_answering/data/th_xquad_100sample.jsonl
2025-08-29 12:01:18 | INFO     | seahelm_evaluation   | Performing inference for task 'QA' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:18 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]2025-08-29 12:01:18 | INFO     | openai_serving       | Still waiting (40s has elapsed)...
[92m12:01:18 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:18 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.41 examples/s]2025-08-29 12:01:18 | INFO     | openai_serving       | Still waiting (40s has elapsed)...
[92m12:01:18 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:18 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.91 examples/s]2025-08-29 12:01:19 | INFO     | openai_serving       | Still waiting (40s has elapsed)...
[92m12:01:19 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:19 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.24 examples/s]
[92m12:01:19 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:19 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:19 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:19 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:19 | INFO     | openai_serving       | Still waiting (40s has elapsed)...
[92m12:01:19 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:19 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:01:20 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:20 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:20 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:20 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 310.01 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 333.80 examples/s]
2025-08-29 12:01:20 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_qa_th.jsonl
2025-08-29 12:01:20 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:20 | INFO     | seahelm_evaluation   | Inference for task 'QA' completed!
                                                      | 
2025-08-29 12:01:20 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: QA ----------
2025-08-29 12:01:20 | INFO     | seahelm_evaluation   | Evaluating 'QA' using QuestionAnsweringMetric
2025-08-29 12:01:20 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:20 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:20 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:20 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:01:20 | INFO     | question_answering   | Tokenizing Thai text and re-evaluating...
2025-08-29 12:01:20 | INFO     | question_answering   | {'exact_match': 0.0, 'f1': 0.0, 'normalized_f1': 0.0}
2025-08-29 12:01:20 | INFO     | question_answering   | 0 answers out of 2 (0.00%) can be found in the model's predictions.
2025-08-29 12:01:20 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:20 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_qa_th.jsonl
2025-08-29 12:01:20 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:20 | INFO     | seahelm_evaluation   | Evaluation for task 'QA' completed!
                                                      | 
2025-08-29 12:01:20 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: TOXICITY ----------
                                                      | Testing Competency: SAFETY
2025-08-29 12:01:20 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/safety/toxicity_detection/data/th_toxicity_1000sample.jsonl
2025-08-29 12:01:20 | INFO     | seahelm_evaluation   | Performing inference for task 'TOXICITY' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:20 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.41 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.04 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.31 examples/s]
[92m12:01:20 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:20 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:20 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:20 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:21 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:21 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:21 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:21 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 300.96 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 317.01 examples/s]
2025-08-29 12:01:21 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_th.jsonl
2025-08-29 12:01:21 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:21 | INFO     | seahelm_evaluation   | Inference for task 'TOXICITY' completed!
                                                      | 
2025-08-29 12:01:21 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: TOXICITY ----------
2025-08-29 12:01:21 | INFO     | seahelm_evaluation   | Evaluating 'TOXICITY' using ToxicityDetectionMetric
2025-08-29 12:01:21 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:21 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:21 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:21 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:01:21 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:01:21 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 2]
                                                      |  [0 0]]
2025-08-29 12:01:21 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       2.0
                                                      |            3       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       2.0
                                                      |    macro avg       0.00      0.00      0.00       2.0
                                                      | weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:01:21 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:21 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_th.jsonl
2025-08-29 12:01:21 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:21 | INFO     | seahelm_evaluation   | Evaluation for task 'TOXICITY' completed!
                                                      | 
2025-08-29 12:01:21 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: TRANSLATION-EN-XX ----------
                                                      | Testing Competency: NLG
2025-08-29 12:01:21 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/en_to_tha_Thai.jsonl
2025-08-29 12:01:21 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-EN-XX' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:21 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.33 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.13 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.36 examples/s]
[92m12:01:21 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:21 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:21 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:21 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:23 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:23 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:23 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:23 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 341.31 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 400.09 examples/s]
2025-08-29 12:01:23 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_th.jsonl
2025-08-29 12:01:23 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:23 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:01:23 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: TRANSLATION-XX-EN ----------
                                                      | Testing Competency: NLG
2025-08-29 12:01:23 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/tha_Thai_to_en.jsonl
2025-08-29 12:01:23 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-XX-EN' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:23 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.22 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.99 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.30 examples/s]
[92m12:01:24 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:24 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:24 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:24 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:25 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:25 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:25 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:25 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 334.79 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 398.77 examples/s]
2025-08-29 12:01:25 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_th.jsonl
2025-08-29 12:01:25 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:25 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:01:25 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: ABSSUM ----------
                                                      | Testing Competency: NLG
2025-08-29 12:01:25 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/abstractive_summarization/data/th_xlsum_100sample.jsonl
2025-08-29 12:01:25 | INFO     | seahelm_evaluation   | Performing inference for task 'ABSSUM' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:25 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.26 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.03 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.28 examples/s]
[92m12:01:26 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:26 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:26 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:26 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:28 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:28 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:28 | INFO     | openai_serving       | Still waiting (50s has elapsed)...
[92m12:01:28 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:28 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:01:29 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:29 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 285.65 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 322.22 examples/s]
2025-08-29 12:01:29 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_th.jsonl
2025-08-29 12:01:29 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:29 | INFO     | seahelm_evaluation   | Inference for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:01:29 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: ABSSUM ----------
2025-08-29 12:01:29 | INFO     | seahelm_evaluation   | Evaluating 'ABSSUM' using SummarizationMetric
2025-08-29 12:01:29 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:29 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:29 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:29 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:01:29 | INFO     | summarization        | Rouge-L Scores:
2025-08-29 12:01:29 | INFO     | summarization        | Precision: 0.00 | Recall: 0.00 | F1: 0.00
2025-08-29 12:01:29 | INFO     | summarization        | Norm F1 Score: 0.00
2025-08-29 12:01:29 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:29 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_th.jsonl
2025-08-29 12:01:29 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:29 | INFO     | seahelm_evaluation   | Evaluation for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:01:29 | INFO     | openai_serving       | Still waiting (50s has elapsed)...
2025-08-29 12:01:29 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: CAUSAL ----------
                                                      | Testing Competency: NLR
[92m12:01:29 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:29 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/causal/data/th_xcopa.jsonl
2025-08-29 12:01:29 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:29 | INFO     | openai_serving       | Still waiting (50s has elapsed)...
[92m12:01:29 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:29 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:29 | INFO     | seahelm_evaluation   | Performing inference for task 'CAUSAL' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:29 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.45 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.03 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.29 examples/s]
[92m12:01:29 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:29 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:29 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:29 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:29 | INFO     | openai_serving       | Still waiting (50s has elapsed)...
[92m12:01:29 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:29 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:01:30 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:30 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:30 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:30 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 294.78 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 320.59 examples/s]
2025-08-29 12:01:30 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_causal_th.jsonl
2025-08-29 12:01:30 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:30 | INFO     | seahelm_evaluation   | Inference for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:01:30 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: CAUSAL ----------
2025-08-29 12:01:30 | INFO     | seahelm_evaluation   | Evaluating 'CAUSAL' using CausalReasoningMetric
2025-08-29 12:01:30 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:30 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:30 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:30 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:01:30 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:01:30 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 2]
                                                      |  [0 0]]
2025-08-29 12:01:30 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       2.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       2.0
                                                      |    macro avg       0.00      0.00      0.00       2.0
                                                      | weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:01:30 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:30 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_causal_th.jsonl
2025-08-29 12:01:30 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:30 | INFO     | seahelm_evaluation   | Evaluation for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:01:30 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: NLI ----------
                                                      | Testing Competency: NLR
2025-08-29 12:01:30 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/nli/data/th_xnli_1000sample.jsonl
2025-08-29 12:01:30 | INFO     | seahelm_evaluation   | Performing inference for task 'NLI' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:30 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.22 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.05 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.29 examples/s]
[92m12:01:31 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:31 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:31 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:31 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:31 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:31 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:32 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:32 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 325.52 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 353.61 examples/s]
2025-08-29 12:01:32 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_nli_th.jsonl
2025-08-29 12:01:32 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:32 | INFO     | seahelm_evaluation   | Inference for task 'NLI' completed!
                                                      | 
2025-08-29 12:01:32 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: NLI ----------
2025-08-29 12:01:32 | INFO     | seahelm_evaluation   | Evaluating 'NLI' using NLIMetric
2025-08-29 12:01:32 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:32 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:32 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:32 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:01:32 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:01:32 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 2]
                                                      |  [0 0]]
2025-08-29 12:01:32 | INFO     | seahelm_metric       | Classification report:
                                                      |                precision    recall  f1-score   support
                                                      | 
                                                      | contradiction       0.00      0.00      0.00       2.0
                                                      |          none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |      accuracy                           0.00       2.0
                                                      |     macro avg       0.00      0.00      0.00       2.0
                                                      |  weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:01:32 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:32 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_nli_th.jsonl
2025-08-29 12:01:32 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:32 | INFO     | seahelm_evaluation   | Evaluation for task 'NLI' completed!
                                                      | 
2025-08-29 12:01:32 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: IF-EVAL ----------
                                                      | Testing Competency: INSTRUCTION-FOLLOWING
2025-08-29 12:01:32 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/instruction_following/ifeval/data/th_sea_ifeval.jsonl
2025-08-29 12:01:32 | INFO     | seahelm_evaluation   | Performing inference for task 'IF-EVAL' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:32 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.37 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.01 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.24 examples/s]
[92m12:01:32 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:32 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:32 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:32 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:37 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:37 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:38 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:38 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 173.78 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 212.26 examples/s]
2025-08-29 12:01:38 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_th.jsonl
2025-08-29 12:01:38 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:38 | INFO     | seahelm_evaluation   | Inference for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 12:01:38 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: IF-EVAL ----------
2025-08-29 12:01:38 | INFO     | seahelm_evaluation   | Evaluating 'IF-EVAL' using IFEvalMetric
2025-08-29 12:01:38 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:38 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:38 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:38 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:01:38 | ERROR    | seahelm_evaluation   | Failed to run evaluation for task if-eval and lang th
2025-08-29 12:01:38 | ERROR    | seahelm_evaluation   | False
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 3812, in get_loc
                                                      |     return self._engine.get_loc(casted_key)
                                                      |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "pandas/_libs/index.pyx", line 167, in pandas._libs.index.IndexEngine.get_loc
                                                      |   File "pandas/_libs/index.pyx", line 196, in pandas._libs.index.IndexEngine.get_loc
                                                      |   File "pandas/_libs/hashtable_class_helper.pxi", line 5846, in pandas._libs.hashtable.UInt8HashTable.get_item
                                                      |   File "pandas/_libs/hashtable_class_helper.pxi", line 5870, in pandas._libs.hashtable.UInt8HashTable.get_item
                                                      | KeyError: 0
                                                      | 
                                                      | The above exception was the direct cause of the following exception:
                                                      | 
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_evaluation.py", line 677, in run_single_task_evaluation
                                                      |     metric_json, inference_df = evaluation_metric.evaluate_responses()
                                                      |                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/seahelm_metric.py", line 118, in evaluate_responses
                                                      |     output_json, inference_df = self.calculate_metrics()
                                                      |                                 ^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/instruction_following/ifeval/if_eval.py", line 72, in calculate_metrics
                                                      |     metric_dict = self.summarize_results(self.inference_df)
                                                      |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/instruction_following/ifeval/if_eval.py", line 145, in summarize_results
                                                      |     overall_fail = int(inference_df["result"].value_counts()[False])
                                                      |                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/series.py", line 1130, in __getitem__
                                                      |     return self._get_value(key)
                                                      |            ^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/series.py", line 1246, in _get_value
                                                      |     loc = self.index.get_loc(label)
                                                      |           ^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 3819, in get_loc
                                                      |     raise KeyError(key) from err
                                                      | KeyError: False
2025-08-29 12:01:38 | WARNING  | seahelm_evaluation   | Setting metric overall_lang_normalized_acc to 0 for task if-eval
2025-08-29 12:01:38 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: SENTIMENT ----------
                                                      | Testing Competency: NLU
2025-08-29 12:01:38 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/sentiment_analysis/data/ta_indicsentiment.jsonl
2025-08-29 12:01:38 | INFO     | seahelm_evaluation   | Performing inference for task 'SENTIMENT' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:38 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.30 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.96 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.29 examples/s]
[92m12:01:38 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:38 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:38 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:38 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:39 | INFO     | openai_serving       | Still waiting (60s has elapsed)...
[92m12:01:39 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:39 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:01:39 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:39 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:39 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:39 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 325.78 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 381.51 examples/s]
2025-08-29 12:01:39 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_ta.jsonl
2025-08-29 12:01:39 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:39 | INFO     | seahelm_evaluation   | Inference for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:01:39 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: SENTIMENT ----------
2025-08-29 12:01:39 | INFO     | seahelm_evaluation   | Evaluating 'SENTIMENT' using SentimentAnalysisMetric
2025-08-29 12:01:39 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:39 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:39 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:39 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:01:39 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:01:39 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1 0]
                                                      |  [0 0 0]
                                                      |  [0 1 0]]
2025-08-29 12:01:39 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |     negative       0.00      0.00      0.00       1.0
                                                      |         none       0.00      0.00      0.00       0.0
                                                      |     positive       0.00      0.00      0.00       1.0
                                                      | 
                                                      |     accuracy                           0.00       2.0
                                                      |    macro avg       0.00      0.00      0.00       2.0
                                                      | weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:01:39 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:39 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_ta.jsonl
2025-08-29 12:01:39 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:39 | INFO     | seahelm_evaluation   | Evaluation for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:01:39 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: QA ----------
                                                      | Testing Competency: NLU
2025-08-29 12:01:39 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/question_answering/data/ta_indicqa_100sample.jsonl
2025-08-29 12:01:39 | INFO     | seahelm_evaluation   | Performing inference for task 'QA' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:39 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:39 | INFO     | openai_serving       | Still waiting (60s has elapsed)...
[92m12:01:39 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:39 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:39 | INFO     | openai_serving       | Still waiting (60s has elapsed)...
[92m12:01:39 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:39 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.44 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.00 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.32 examples/s]
[92m12:01:39 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:39 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:39 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:39 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:40 | INFO     | openai_serving       | Still waiting (60s has elapsed)...
[92m12:01:40 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:40 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:01:40 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:40 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:41 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:41 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 305.04 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 353.01 examples/s]
2025-08-29 12:01:41 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_qa_ta.jsonl
2025-08-29 12:01:41 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:41 | INFO     | seahelm_evaluation   | Inference for task 'QA' completed!
                                                      | 
2025-08-29 12:01:41 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: QA ----------
2025-08-29 12:01:41 | INFO     | seahelm_evaluation   | Evaluating 'QA' using QuestionAnsweringMetric
2025-08-29 12:01:41 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:41 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:41 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:41 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:01:41 | INFO     | question_answering   | {'exact_match': 50.0, 'f1': 50.0, 'normalized_f1': 50.0}
2025-08-29 12:01:41 | INFO     | question_answering   | 1 answers out of 2 (50.00%) can be found in the model's predictions.
2025-08-29 12:01:41 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:41 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_qa_ta.jsonl
2025-08-29 12:01:41 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:41 | INFO     | seahelm_evaluation   | Evaluation for task 'QA' completed!
                                                      | 
2025-08-29 12:01:41 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: TRANSLATION-EN-XX ----------
                                                      | Testing Competency: NLG
2025-08-29 12:01:41 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/en_to_tam_Taml.jsonl
2025-08-29 12:01:41 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-EN-XX' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:41 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.89 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.28 examples/s]
[92m12:01:41 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:41 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:41 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:41 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:42 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:42 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:43 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:43 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 333.62 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 394.24 examples/s]
2025-08-29 12:01:43 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_ta.jsonl
2025-08-29 12:01:43 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:43 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:01:43 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: TRANSLATION-XX-EN ----------
                                                      | Testing Competency: NLG
2025-08-29 12:01:43 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/tam_Taml_to_en.jsonl
2025-08-29 12:01:43 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-XX-EN' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:43 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.28 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.07 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.34 examples/s]
[92m12:01:43 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:43 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:43 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:43 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:44 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:44 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:45 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:45 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 335.83 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 386.66 examples/s]
2025-08-29 12:01:45 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_ta.jsonl
2025-08-29 12:01:45 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:45 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:01:45 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: ABSSUM ----------
                                                      | Testing Competency: NLG
2025-08-29 12:01:45 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/abstractive_summarization/data/ta_xlsum_100sample.jsonl
2025-08-29 12:01:45 | INFO     | seahelm_evaluation   | Performing inference for task 'ABSSUM' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:45 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.95 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.66 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.07 examples/s]
[92m12:01:45 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:45 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:45 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:45 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:48 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:48 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:48 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:48 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 298.95 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 349.69 examples/s]
2025-08-29 12:01:48 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_ta.jsonl
2025-08-29 12:01:48 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:48 | INFO     | seahelm_evaluation   | Inference for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:01:48 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: ABSSUM ----------
2025-08-29 12:01:48 | INFO     | seahelm_evaluation   | Evaluating 'ABSSUM' using SummarizationMetric
2025-08-29 12:01:48 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:48 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:48 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:48 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:01:48 | INFO     | summarization        | Rouge-L Scores:
2025-08-29 12:01:48 | INFO     | summarization        | Precision: 0.00 | Recall: 0.00 | F1: 0.00
2025-08-29 12:01:48 | INFO     | summarization        | Norm F1 Score: 0.00
2025-08-29 12:01:48 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:48 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_ta.jsonl
2025-08-29 12:01:48 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:48 | INFO     | seahelm_evaluation   | Evaluation for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:01:48 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: CAUSAL ----------
                                                      | Testing Competency: NLR
2025-08-29 12:01:48 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/causal/data/ta_xcopa.jsonl
2025-08-29 12:01:48 | INFO     | seahelm_evaluation   | Performing inference for task 'CAUSAL' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:48 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.19 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.55 examples/s]2025-08-29 12:01:49 | INFO     | openai_serving       | Still waiting (70s has elapsed)...
[92m12:01:49 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:49 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.06 examples/s]
[92m12:01:49 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:49 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:49 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:49 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:49 | INFO     | openai_serving       | Still waiting (70s has elapsed)...
[92m12:01:49 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:49 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:49 | INFO     | openai_serving       | Still waiting (70s has elapsed)...
[92m12:01:49 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:49 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:01:50 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:50 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:50 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:50 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 281.49 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 327.39 examples/s]
2025-08-29 12:01:50 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_causal_ta.jsonl
2025-08-29 12:01:50 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:50 | INFO     | seahelm_evaluation   | Inference for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:01:50 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: CAUSAL ----------
2025-08-29 12:01:50 | INFO     | seahelm_evaluation   | Evaluating 'CAUSAL' using CausalReasoningMetric
2025-08-29 12:01:50 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:50 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:50 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:50 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:01:50 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:01:50 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 2]
                                                      |  [0 0]]
2025-08-29 12:01:50 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       2.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       2.0
                                                      |    macro avg       0.00      0.00      0.00       2.0
                                                      | weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:01:50 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:50 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_causal_ta.jsonl
2025-08-29 12:01:50 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:50 | INFO     | seahelm_evaluation   | Evaluation for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:01:50 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: NLI ----------
                                                      | Testing Competency: NLR
2025-08-29 12:01:50 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/nli/data/ta_xnli_1000sample.jsonl
2025-08-29 12:01:50 | INFO     | seahelm_evaluation   | Performing inference for task 'NLI' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:50 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:50 | INFO     | openai_serving       | Still waiting (70s has elapsed)...
[92m12:01:50 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:50 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.25 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.06 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.31 examples/s]
[92m12:01:50 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:50 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:50 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:50 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:51 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:51 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:51 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:51 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 314.99 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 369.85 examples/s]
2025-08-29 12:01:51 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_nli_ta.jsonl
2025-08-29 12:01:51 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:51 | INFO     | seahelm_evaluation   | Inference for task 'NLI' completed!
                                                      | 
2025-08-29 12:01:51 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: NLI ----------
2025-08-29 12:01:51 | INFO     | seahelm_evaluation   | Evaluating 'NLI' using NLIMetric
2025-08-29 12:01:51 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:51 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:51 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:51 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:01:51 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:01:51 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 2]
                                                      |  [0 0]]
2025-08-29 12:01:51 | INFO     | seahelm_metric       | Classification report:
                                                      |                precision    recall  f1-score   support
                                                      | 
                                                      | contradiction       0.00      0.00      0.00       2.0
                                                      |          none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |      accuracy                           0.00       2.0
                                                      |     macro avg       0.00      0.00      0.00       2.0
                                                      |  weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:01:51 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:51 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_nli_ta.jsonl
2025-08-29 12:01:51 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:51 | INFO     | seahelm_evaluation   | Evaluation for task 'NLI' completed!
                                                      | 
2025-08-29 12:01:51 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: MP-R ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 12:01:51 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/syntax/data/ta_syntax_mcq_randomized.jsonl
2025-08-29 12:01:51 | INFO     | seahelm_evaluation   | Performing inference for task 'MP-R' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:51 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.91 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.00 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.21 examples/s]
[92m12:01:52 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:52 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:52 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:52 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:53 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:53 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:53 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:53 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 232.60 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 298.55 examples/s]
2025-08-29 12:01:53 | INFO     | seahelm_evaluation   | Saving inference results for task 'MP-R' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_mp-r_ta.jsonl
2025-08-29 12:01:53 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:53 | INFO     | seahelm_evaluation   | Inference for task 'MP-R' completed!
                                                      | 
2025-08-29 12:01:53 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: MP-R ----------
2025-08-29 12:01:53 | INFO     | seahelm_evaluation   | Evaluating 'MP-R' using MinimalPairsMetric
2025-08-29 12:01:53 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:53 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:53 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:53 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:01:53 | INFO     | minimal_pairs        | Accuracy for phenomenon <argument_structure>: 0.0
2025-08-29 12:01:53 | INFO     | minimal_pairs        | Overall Accuracy: 0.0
2025-08-29 12:01:53 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:53 | INFO     | seahelm_evaluation   | Saving inference results for task 'MP-R' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_mp-r_ta.jsonl
2025-08-29 12:01:53 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:53 | INFO     | seahelm_evaluation   | Evaluation for task 'MP-R' completed!
                                                      | 
2025-08-29 12:01:53 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: PRAGMATIC-SINGLE ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 12:01:53 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/pragmatics/data/ta_pragmatic_reasoning_single.jsonl
2025-08-29 12:01:53 | INFO     | seahelm_evaluation   | Performing inference for task 'PRAGMATIC-SINGLE' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:53 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.32 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.97 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.26 examples/s]
[92m12:01:54 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:54 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:54 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:54 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:54 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:54 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:54 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:54 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 247.71 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 278.99 examples/s]
2025-08-29 12:01:54 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-SINGLE' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-single_ta.jsonl
2025-08-29 12:01:54 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:54 | INFO     | seahelm_evaluation   | Inference for task 'PRAGMATIC-SINGLE' completed!
                                                      | 
2025-08-29 12:01:54 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: PRAGMATIC-SINGLE ----------
2025-08-29 12:01:54 | INFO     | seahelm_evaluation   | Evaluating 'PRAGMATIC-SINGLE' using PragmaticReasoningSingleSentenceMetric
2025-08-29 12:01:54 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:54 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:54 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:54 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  subset_references = subset[self.label_column].replace(
2025-08-29 12:01:54 | INFO     | pragmatic_reasoning  | Accuracy for phenomenon <scalar_implicatures>: 0.0 / 2
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:93: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  references = self.inference_df[self.label_column].replace(
2025-08-29 12:01:54 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:54 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-SINGLE' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-single_ta.jsonl
2025-08-29 12:01:54 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:54 | INFO     | seahelm_evaluation   | Evaluation for task 'PRAGMATIC-SINGLE' completed!
                                                      | 
2025-08-29 12:01:54 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: PRAGMATIC-PAIR ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 12:01:54 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/pragmatics/data/ta_pragmatic_reasoning_pair.jsonl
2025-08-29 12:01:54 | INFO     | seahelm_evaluation   | Performing inference for task 'PRAGMATIC-PAIR' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:54 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.78 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.23 examples/s]
[92m12:01:55 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:55 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:55 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:55 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:56 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:56 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:56 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:56 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 286.97 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 339.91 examples/s]
2025-08-29 12:01:56 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-PAIR' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-pair_ta.jsonl
2025-08-29 12:01:56 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:56 | INFO     | seahelm_evaluation   | Inference for task 'PRAGMATIC-PAIR' completed!
                                                      | 
2025-08-29 12:01:56 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: PRAGMATIC-PAIR ----------
2025-08-29 12:01:56 | INFO     | seahelm_evaluation   | Evaluating 'PRAGMATIC-PAIR' using PragmaticReasoningSentencePairMetric
2025-08-29 12:01:56 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:56 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:56 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:56 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:143: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  subset_references = subset[self.label_column].replace({True: 1, False: 0})
2025-08-29 12:01:56 | INFO     | pragmatic_reasoning  | Accuracy for phenomenon <scalar_implicatures>: 0.0 / 2
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:157: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  references = self.inference_df[self.label_column].replace({True: 1, False: 0})
2025-08-29 12:01:56 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:56 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-PAIR' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-pair_ta.jsonl
2025-08-29 12:01:56 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:56 | INFO     | seahelm_evaluation   | Evaluation for task 'PRAGMATIC-PAIR' completed!
                                                      | 
2025-08-29 12:01:56 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: SENTIMENT ----------
                                                      | Testing Competency: NLU
2025-08-29 12:01:56 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/sentiment_analysis/data/tl_elections_sentiment.jsonl
2025-08-29 12:01:56 | INFO     | seahelm_evaluation   | Performing inference for task 'SENTIMENT' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:56 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.27 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.00 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.28 examples/s]
[92m12:01:57 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:57 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:57 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:57 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:57 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:57 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:57 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:57 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 323.96 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 378.34 examples/s]
2025-08-29 12:01:57 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_tl.jsonl
2025-08-29 12:01:57 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:57 | INFO     | seahelm_evaluation   | Inference for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:01:57 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: SENTIMENT ----------
2025-08-29 12:01:57 | INFO     | seahelm_evaluation   | Evaluating 'SENTIMENT' using SentimentAnalysisMetric
2025-08-29 12:01:57 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:57 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:57 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:57 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:01:57 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:01:57 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 2]
                                                      |  [0 0]]
2025-08-29 12:01:57 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |      neutral       0.00      0.00      0.00       2.0
                                                      |         none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       2.0
                                                      |    macro avg       0.00      0.00      0.00       2.0
                                                      | weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:01:57 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:57 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_tl.jsonl
2025-08-29 12:01:57 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:57 | INFO     | seahelm_evaluation   | Evaluation for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:01:57 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: BELEBELE-QA-MC ----------
                                                      | Testing Competency: NLU
2025-08-29 12:01:57 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/belebele_mcqa/data/eval/tl_belebele.jsonl
2025-08-29 12:01:57 | INFO     | seahelm_evaluation   | Performing inference for task 'BELEBELE-QA-MC' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:57 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.17 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.99 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.26 examples/s]
[92m12:01:58 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:58 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:58 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:58 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:58 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:58 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:01:58 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:58 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 282.71 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 325.75 examples/s]
2025-08-29 12:01:58 | INFO     | seahelm_evaluation   | Saving inference results for task 'BELEBELE-QA-MC' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_belebele-qa-mc_tl.jsonl
2025-08-29 12:01:58 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:58 | INFO     | seahelm_evaluation   | Inference for task 'BELEBELE-QA-MC' completed!
                                                      | 
2025-08-29 12:01:58 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: BELEBELE-QA-MC ----------
2025-08-29 12:01:58 | INFO     | seahelm_evaluation   | Evaluating 'BELEBELE-QA-MC' using QuestionAnsweringMultipleChoiceMetric
2025-08-29 12:01:58 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:58 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:58 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:58 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:01:58 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:01:58 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 0 1]
                                                      |  [0 0 1]
                                                      |  [0 0 0]]
2025-08-29 12:01:58 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       1.0
                                                      |            1       0.00      0.00      0.00       1.0
                                                      |            4       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       2.0
                                                      |    macro avg       0.00      0.00      0.00       2.0
                                                      | weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:01:58 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:58 | INFO     | seahelm_evaluation   | Saving inference results for task 'BELEBELE-QA-MC' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_belebele-qa-mc_tl.jsonl
2025-08-29 12:01:58 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:58 | INFO     | seahelm_evaluation   | Evaluation for task 'BELEBELE-QA-MC' completed!
                                                      | 
2025-08-29 12:01:58 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: TOXICITY ----------
                                                      | Testing Competency: SAFETY
2025-08-29 12:01:58 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/safety/toxicity_detection/data/tl_elections_hsd.jsonl
2025-08-29 12:01:58 | INFO     | seahelm_evaluation   | Performing inference for task 'TOXICITY' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:58 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.38 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.95 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.28 examples/s]
[92m12:01:59 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:59 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:01:59 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:59 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:01:59 | INFO     | openai_serving       | Still waiting (80s has elapsed)...
[92m12:01:59 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:59 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:01:59 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:59 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:59 | INFO     | openai_serving       | Still waiting (80s has elapsed)...
[92m12:01:59 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:59 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:59 | INFO     | openai_serving       | Still waiting (80s has elapsed)...
[92m12:01:59 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:01:59 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:01:59 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:01:59 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 317.07 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 371.36 examples/s]
2025-08-29 12:01:59 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_tl.jsonl
2025-08-29 12:01:59 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:59 | INFO     | seahelm_evaluation   | Inference for task 'TOXICITY' completed!
                                                      | 
2025-08-29 12:01:59 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: TOXICITY ----------
2025-08-29 12:01:59 | INFO     | seahelm_evaluation   | Evaluating 'TOXICITY' using ToxicityDetectionMetric
2025-08-29 12:01:59 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:01:59 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:01:59 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:01:59 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:01:59 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:01:59 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 2]
                                                      |  [0 0]]
2025-08-29 12:01:59 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       2.0
                                                      |            3       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       2.0
                                                      |    macro avg       0.00      0.00      0.00       2.0
                                                      | weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:01:59 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:01:59 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_tl.jsonl
2025-08-29 12:01:59 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:01:59 | INFO     | seahelm_evaluation   | Evaluation for task 'TOXICITY' completed!
                                                      | 
2025-08-29 12:01:59 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: TRANSLATION-EN-XX ----------
                                                      | Testing Competency: NLG
2025-08-29 12:01:59 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/en_to_tgl_Latn.jsonl
2025-08-29 12:01:59 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-EN-XX' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:01:59 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.02 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.05 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.27 examples/s]
[92m12:02:00 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:02:00 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:02:00 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:02:00 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:02:00 | INFO     | openai_serving       | Still waiting (80s has elapsed)...
[92m12:02:00 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:00 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:02:01 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:02:01 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:02:01 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:02:01 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 324.08 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 378.51 examples/s]
2025-08-29 12:02:01 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_tl.jsonl
2025-08-29 12:02:01 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:02:01 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:02:01 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: TRANSLATION-XX-EN ----------
                                                      | Testing Competency: NLG
2025-08-29 12:02:01 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/tgl_Latn_to_en.jsonl
2025-08-29 12:02:01 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-XX-EN' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:02:01 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.37 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.92 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.24 examples/s]
[92m12:02:02 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:02:02 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:02:02 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:02:02 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:02:03 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:02:03 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:02:03 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:02:03 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 316.26 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 363.92 examples/s]
2025-08-29 12:02:03 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_tl.jsonl
2025-08-29 12:02:03 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:02:03 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:02:03 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: ABSSUM ----------
                                                      | Testing Competency: NLG
2025-08-29 12:02:03 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/abstractive_summarization/data/tl_xlsum.jsonl
2025-08-29 12:02:03 | INFO     | seahelm_evaluation   | Performing inference for task 'ABSSUM' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:02:03 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.35 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.89 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.23 examples/s]
[92m12:02:04 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:02:04 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:02:04 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:02:04 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:02:06 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:02:06 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:02:06 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:02:06 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 300.30 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 350.49 examples/s]
2025-08-29 12:02:06 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_tl.jsonl
2025-08-29 12:02:06 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:02:06 | INFO     | seahelm_evaluation   | Inference for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:02:06 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: ABSSUM ----------
2025-08-29 12:02:06 | INFO     | seahelm_evaluation   | Evaluating 'ABSSUM' using SummarizationMetric
2025-08-29 12:02:06 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:02:06 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:02:06 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:02:06 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:02:06 | INFO     | summarization        | Rouge-L Scores:
2025-08-29 12:02:06 | INFO     | summarization        | Precision: 6.06 | Recall: 22.22 | F1: 9.52
2025-08-29 12:02:06 | INFO     | summarization        | Norm F1 Score: 9.52
2025-08-29 12:02:06 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:02:06 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_tl.jsonl
2025-08-29 12:02:06 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:02:06 | INFO     | seahelm_evaluation   | Evaluation for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:02:06 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: CAUSAL ----------
                                                      | Testing Competency: NLR
2025-08-29 12:02:06 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/causal/data/tl_balanced_copa.jsonl
2025-08-29 12:02:06 | INFO     | seahelm_evaluation   | Performing inference for task 'CAUSAL' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:02:06 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.35 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.92 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.25 examples/s]
[92m12:02:07 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:02:07 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:02:07 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:02:07 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:02:08 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:02:08 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:02:08 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:02:08 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 230.57 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 323.63 examples/s]
2025-08-29 12:02:08 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_causal_tl.jsonl
2025-08-29 12:02:08 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:02:08 | INFO     | seahelm_evaluation   | Inference for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:02:08 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: CAUSAL ----------
2025-08-29 12:02:08 | INFO     | seahelm_evaluation   | Evaluating 'CAUSAL' using CausalReasoningMetric
2025-08-29 12:02:08 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:02:08 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:02:08 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:02:08 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:02:08 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:02:08 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 2]
                                                      |  [0 0]]
2025-08-29 12:02:08 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       2.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       2.0
                                                      |    macro avg       0.00      0.00      0.00       2.0
                                                      | weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:02:08 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:02:08 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_causal_tl.jsonl
2025-08-29 12:02:08 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:02:08 | INFO     | seahelm_evaluation   | Evaluation for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:02:08 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: NLI ----------
                                                      | Testing Competency: NLR
2025-08-29 12:02:08 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/nli/data/tl_xnli.jsonl
2025-08-29 12:02:08 | INFO     | seahelm_evaluation   | Performing inference for task 'NLI' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:02:08 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.21 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.08 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.32 examples/s]
[92m12:02:09 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:02:09 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:02:09 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:02:09 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:02:09 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:02:09 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
2025-08-29 12:02:09 | INFO     | openai_serving       | Still waiting (90s has elapsed)...
[92m12:02:09 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:09 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:02:09 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:02:09 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 313.03 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 365.87 examples/s]
2025-08-29 12:02:09 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_nli_tl.jsonl
2025-08-29 12:02:09 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:02:09 | INFO     | seahelm_evaluation   | Inference for task 'NLI' completed!
                                                      | 
2025-08-29 12:02:09 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: NLI ----------
2025-08-29 12:02:09 | INFO     | seahelm_evaluation   | Evaluating 'NLI' using NLIMetric
2025-08-29 12:02:09 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:02:09 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:02:09 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:02:09 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:02:09 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:02:09 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 2]
                                                      |  [0 0]]
2025-08-29 12:02:09 | INFO     | seahelm_metric       | Classification report:
                                                      |                precision    recall  f1-score   support
                                                      | 
                                                      | contradiction       0.00      0.00      0.00       2.0
                                                      |          none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |      accuracy                           0.00       2.0
                                                      |     macro avg       0.00      0.00      0.00       2.0
                                                      |  weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:02:09 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:02:09 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_nli_tl.jsonl
2025-08-29 12:02:09 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:02:09 | INFO     | seahelm_evaluation   | Evaluation for task 'NLI' completed!
                                                      | 
2025-08-29 12:02:09 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: IF-EVAL ----------
                                                      | Testing Competency: INSTRUCTION-FOLLOWING
2025-08-29 12:02:09 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/instruction_following/ifeval/data/tl_sea_ifeval.jsonl
2025-08-29 12:02:10 | INFO     | openai_serving       | Still waiting (90s has elapsed)...
2025-08-29 12:02:10 | INFO     | openai_serving       | Still waiting (90s has elapsed)...
[92m12:02:10 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:10 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:02:10 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:10 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:10 | INFO     | seahelm_evaluation   | Performing inference for task 'IF-EVAL' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:02:10 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.16 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.01 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.22 examples/s]
[92m12:02:10 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:02:10 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:02:10 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:02:10 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:02:10 | INFO     | openai_serving       | Still waiting (90s has elapsed)...
[92m12:02:10 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:10 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:02:15 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:02:15 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:02:15 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:02:15 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 174.99 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 212.22 examples/s]
2025-08-29 12:02:15 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_tl.jsonl
2025-08-29 12:02:15 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:02:15 | INFO     | seahelm_evaluation   | Inference for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 12:02:15 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: IF-EVAL ----------
2025-08-29 12:02:15 | INFO     | seahelm_evaluation   | Evaluating 'IF-EVAL' using IFEvalMetric
2025-08-29 12:02:15 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:02:15 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:02:15 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:02:15 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:02:15 | INFO     | if_eval              | Overall pass: 1 / 2
2025-08-29 12:02:15 | INFO     | if_eval              | Overall accuracy: 50.000000
2025-08-29 12:02:15 | INFO     | if_eval              | Correct language rate: 1.000000
2025-08-29 12:02:15 | INFO     | if_eval              | Lang normalized accuracy: 50.000000
2025-08-29 12:02:15 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:02:15 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_tl.jsonl
2025-08-29 12:02:15 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:02:15 | INFO     | seahelm_evaluation   | Evaluation for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 12:02:15 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: KALAHI-MC ----------
                                                      | Testing Competency: CULTURAL
2025-08-29 12:02:15 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/cultural/kalahi/data/tl_kalahi_mc.jsonl
2025-08-29 12:02:16 | INFO     | seahelm_evaluation   | Performing inference for task 'KALAHI-MC' with 0 examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
2025-08-29 12:02:16 | WARNING  | arrow_dataset        | num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]Map (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.30 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.18 examples/s]Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.37 examples/s]
[92m12:02:16 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:02:16 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:02:16 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:02:16 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:02:17 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:02:17 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:02:17 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:02:17 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 287.71 examples/s]
Map:   0%|          | 0/2 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 330.61 examples/s]
2025-08-29 12:02:17 | INFO     | seahelm_evaluation   | Saving inference results for task 'KALAHI-MC' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_kalahi-mc_tl.jsonl
2025-08-29 12:02:17 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:02:17 | INFO     | seahelm_evaluation   | Inference for task 'KALAHI-MC' completed!
                                                      | 
2025-08-29 12:02:17 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: KALAHI-MC ----------
2025-08-29 12:02:17 | INFO     | seahelm_evaluation   | Evaluating 'KALAHI-MC' using KalahiMCMetric
2025-08-29 12:02:17 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:02:17 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:02:17 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:02:17 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:02:17 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:02:17 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 0 1]
                                                      |  [0 0 1]
                                                      |  [0 0 0]]
2025-08-29 12:02:17 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            B       0.00      0.00      0.00       1.0
                                                      |            C       0.00      0.00      0.00       1.0
                                                      |         none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       2.0
                                                      |    macro avg       0.00      0.00      0.00       2.0
                                                      | weighted avg       0.00      0.00      0.00       2.0
                                                      | 
2025-08-29 12:02:17 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:02:17 | INFO     | seahelm_evaluation   | Saving inference results for task 'KALAHI-MC' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_kalahi-mc_tl.jsonl
2025-08-29 12:02:17 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:02:17 | INFO     | seahelm_evaluation   | Evaluation for task 'KALAHI-MC' completed!
                                                      | 
2025-08-29 12:02:17 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: TRANSLATION-EN-XX ----------
2025-08-29 12:02:17 | WARNING  | translation          | COMET not installed. Please install COMET to use the COMET metrics.
2025-08-29 12:02:17 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-EN-XX' using TranslationMetric
2025-08-29 12:02:17 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:02:17 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:02:17 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:02:17 | INFO     | seahelm_metric       | Calculating metrics...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-08-29 12:02:19 | INFO     | openai_serving       | Still waiting (100s has elapsed)...
[92m12:02:19 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:19 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.72it/s]2025-08-29 12:02:20 | INFO     | openai_serving       | Still waiting (100s has elapsed)...
[92m12:02:20 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:20 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:20 | INFO     | openai_serving       | Still waiting (100s has elapsed)...
[92m12:02:20 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:20 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.77it/s]2025-08-29 12:02:20 | INFO     | openai_serving       | Still waiting (100s has elapsed)...
[92m12:02:20 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:20 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.89it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.85it/s]
Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
2025-08-29 12:02:29 | INFO     | translation          | MetricX WMT24 score: 17.875000
2025-08-29 12:02:29 | INFO     | translation          | MetricX WMT24 with references score: 24.750000
2025-08-29 12:02:29 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:02:29 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_id.jsonl
2025-08-29 12:02:29 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:02:29 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:02:29 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: TRANSLATION-XX-EN ----------
2025-08-29 12:02:29 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-XX-EN' using TranslationMetric
2025-08-29 12:02:29 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:02:29 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:02:29 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:02:29 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:02:30 | INFO     | openai_serving       | Still waiting (110s has elapsed)...
[92m12:02:30 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:30 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:30 | INFO     | openai_serving       | Still waiting (110s has elapsed)...
[92m12:02:30 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:30 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:30 | INFO     | openai_serving       | Still waiting (110s has elapsed)...
[92m12:02:30 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:30 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:31 | INFO     | openai_serving       | Still waiting (110s has elapsed)...
[92m12:02:31 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:31 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.92it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.86it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.94it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.93it/s]
2025-08-29 12:02:40 | INFO     | openai_serving       | Still waiting (120s has elapsed)...
[92m12:02:40 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:40 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:40 | INFO     | translation          | MetricX WMT24 score: 15.500000
2025-08-29 12:02:40 | INFO     | openai_serving       | Still waiting (120s has elapsed)...
[92m12:02:40 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:40 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:40 | INFO     | openai_serving       | Still waiting (120s has elapsed)...
[92m12:02:40 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:40 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:40 | INFO     | translation          | MetricX WMT24 with references score: 25.000000
2025-08-29 12:02:40 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:02:40 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_id.jsonl
2025-08-29 12:02:40 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:02:40 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:02:40 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: TRANSLATION-EN-XX ----------
2025-08-29 12:02:40 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-EN-XX' using TranslationMetric
2025-08-29 12:02:40 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:02:40 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:02:40 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:02:40 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:02:41 | INFO     | openai_serving       | Still waiting (120s has elapsed)...
[92m12:02:41 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:41 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.91it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.87it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.94it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.93it/s]
2025-08-29 12:02:50 | INFO     | openai_serving       | Still waiting (130s has elapsed)...
[92m12:02:50 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:50 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:50 | INFO     | openai_serving       | Still waiting (130s has elapsed)...
[92m12:02:50 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:50 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:50 | INFO     | openai_serving       | Still waiting (130s has elapsed)...
[92m12:02:50 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:50 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:51 | INFO     | translation          | MetricX WMT24 score: 8.460938
2025-08-29 12:02:51 | INFO     | openai_serving       | Still waiting (130s has elapsed)...
[92m12:02:51 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:51 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:02:51 | INFO     | translation          | MetricX WMT24 with references score: 12.690430
2025-08-29 12:02:51 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:02:51 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_vi.jsonl
2025-08-29 12:02:51 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:02:51 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:02:51 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: TRANSLATION-XX-EN ----------
2025-08-29 12:02:51 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-XX-EN' using TranslationMetric
2025-08-29 12:02:51 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:02:51 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:02:51 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:02:51 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.90it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.86it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.96it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.93it/s]
2025-08-29 12:03:00 | INFO     | openai_serving       | Still waiting (140s has elapsed)...
[92m12:03:00 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:03:00 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:03:01 | INFO     | openai_serving       | Still waiting (140s has elapsed)...
[92m12:03:01 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:03:01 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:03:01 | INFO     | openai_serving       | Still waiting (140s has elapsed)...
[92m12:03:01 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:03:01 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:03:02 | INFO     | openai_serving       | Still waiting (140s has elapsed)...
2025-08-29 12:03:02 | INFO     | openai_serving       | OpenAI batch is completed
[92m12:03:02 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:03:02 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:03:02 - LiteLLM:ERROR[0m: logging_worker.py:61 - LoggingWorker cancelled: 
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
    coroutine = await self._queue.get()
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
    await getter
asyncio.exceptions.CancelledError
2025-08-29 12:03:02 | ERROR    | logging_worker       | LoggingWorker cancelled: 
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
                                                      |     coroutine = await self._queue.get()
                                                      |                 ^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
                                                      |     await getter
                                                      | asyncio.exceptions.CancelledError
2025-08-29 12:03:02 | INFO     | mt_bench             | All judgments have been obtained.
2025-08-29 12:03:02 | INFO     | mt_bench             | Successfully obtained all judgments.
                                                      | 
2025-08-29 12:03:02 | INFO     | mt_bench             | Win rate for category <Knowledge III>: 0.375
2025-08-29 12:03:02 | INFO     | mt_bench             | Overall win rate: 0.375
2025-08-29 12:03:02 | INFO     | mt_bench             | Weighted win rate: 0.375
2025-08-29 12:03:02 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:03:02 | INFO     | translation          | MetricX WMT24 score: 14.968750
2025-08-29 12:03:03 | INFO     | translation          | MetricX WMT24 with references score: 23.562500
2025-08-29 12:03:03 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:03:03 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_vi.jsonl
2025-08-29 12:03:03 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:03:03 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:03:03 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: TRANSLATION-EN-XX ----------
2025-08-29 12:03:03 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-EN-XX' using TranslationMetric
2025-08-29 12:03:03 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:03:03 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:03:03 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:03:03 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.91it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.86it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.94it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.92it/s]
2025-08-29 12:03:10 | INFO     | openai_serving       | Still waiting (150s has elapsed)...
[92m12:03:10 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:03:10 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:03:11 | INFO     | openai_serving       | Still waiting (150s has elapsed)...
2025-08-29 12:03:11 | INFO     | openai_serving       | OpenAI batch is completed
[92m12:03:11 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:03:11 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:03:11 | INFO     | openai_serving       | Still waiting (150s has elapsed)...
[92m12:03:11 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:03:11 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:03:11 - LiteLLM:ERROR[0m: logging_worker.py:61 - LoggingWorker cancelled: 
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
    coroutine = await self._queue.get()
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
    await getter
asyncio.exceptions.CancelledError
2025-08-29 12:03:11 | ERROR    | logging_worker       | LoggingWorker cancelled: 
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
                                                      |     coroutine = await self._queue.get()
                                                      |                 ^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
                                                      |     await getter
                                                      | asyncio.exceptions.CancelledError
2025-08-29 12:03:11 | INFO     | mt_bench             | All judgments have been obtained.
2025-08-29 12:03:11 | INFO     | mt_bench             | Successfully obtained all judgments.
                                                      | 
2025-08-29 12:03:11 | INFO     | mt_bench             | Win rate for category <writing>: 0.125
2025-08-29 12:03:11 | INFO     | mt_bench             | Overall win rate: 0.125
2025-08-29 12:03:11 | INFO     | mt_bench             | Weighted win rate: 0.125
2025-08-29 12:03:11 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:03:14 | INFO     | translation          | MetricX WMT24 score: 18.937500
2025-08-29 12:03:14 | INFO     | translation          | MetricX WMT24 with references score: 24.750000
2025-08-29 12:03:14 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:03:14 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_th.jsonl
2025-08-29 12:03:14 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:03:14 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:03:14 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: TRANSLATION-XX-EN ----------
2025-08-29 12:03:14 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-XX-EN' using TranslationMetric
2025-08-29 12:03:14 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:03:14 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:03:14 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:03:14 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.91it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.86it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.93it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.92it/s]
2025-08-29 12:03:21 | INFO     | openai_serving       | Still waiting (160s has elapsed)...
2025-08-29 12:03:21 | INFO     | openai_serving       | OpenAI batch is completed
[92m12:03:21 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:03:21 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:03:21 - LiteLLM:ERROR[0m: logging_worker.py:61 - LoggingWorker cancelled: 
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
    coroutine = await self._queue.get()
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
    await getter
asyncio.exceptions.CancelledError
2025-08-29 12:03:21 | ERROR    | logging_worker       | LoggingWorker cancelled: 
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
                                                      |     coroutine = await self._queue.get()
                                                      |                 ^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
                                                      |     await getter
                                                      | asyncio.exceptions.CancelledError
2025-08-29 12:03:21 | INFO     | mt_bench             | All judgments have been obtained.
2025-08-29 12:03:21 | INFO     | mt_bench             | Successfully obtained all judgments.
                                                      | 
2025-08-29 12:03:21 | INFO     | openai_serving       | Still waiting (160s has elapsed)...
[92m12:03:21 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:03:21 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:03:21 | INFO     | mt_bench             | Win rate for category <writing>: 0.5
2025-08-29 12:03:21 | INFO     | mt_bench             | Overall win rate: 0.5
2025-08-29 12:03:21 | INFO     | mt_bench             | Weighted win rate: 0.5
2025-08-29 12:03:21 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:03:25 | INFO     | translation          | MetricX WMT24 score: 11.531250
2025-08-29 12:03:25 | INFO     | translation          | MetricX WMT24 with references score: 24.125000
2025-08-29 12:03:25 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:03:25 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_th.jsonl
2025-08-29 12:03:25 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:03:25 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:03:25 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: TRANSLATION-EN-XX ----------
2025-08-29 12:03:25 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-EN-XX' using TranslationMetric
2025-08-29 12:03:25 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:03:25 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:03:25 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:03:25 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.90it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.86it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.95it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.93it/s]
2025-08-29 12:03:31 | INFO     | openai_serving       | Still waiting (170s has elapsed)...
2025-08-29 12:03:31 | INFO     | openai_serving       | OpenAI batch is completed
[92m12:03:31 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:03:31 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:03:32 - LiteLLM:ERROR[0m: logging_worker.py:61 - LoggingWorker cancelled: 
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
    coroutine = await self._queue.get()
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
    await getter
asyncio.exceptions.CancelledError
2025-08-29 12:03:32 | ERROR    | logging_worker       | LoggingWorker cancelled: 
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
                                                      |     coroutine = await self._queue.get()
                                                      |                 ^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
                                                      |     await getter
                                                      | asyncio.exceptions.CancelledError
2025-08-29 12:03:33 | INFO     | mt_bench             | All judgments have been obtained.
2025-08-29 12:03:33 | INFO     | mt_bench             | Successfully obtained all judgments.
                                                      | 
2025-08-29 12:03:33 | INFO     | mt_bench             | Win rate for category <writing>: 0.25
2025-08-29 12:03:33 | INFO     | mt_bench             | Overall win rate: 0.25
2025-08-29 12:03:33 | INFO     | mt_bench             | Weighted win rate: 0.25
2025-08-29 12:03:33 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:03:36 | INFO     | translation          | MetricX WMT24 score: 19.625000
2025-08-29 12:03:36 | INFO     | translation          | MetricX WMT24 with references score: 24.750000
2025-08-29 12:03:36 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:03:36 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_ta.jsonl
2025-08-29 12:03:36 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:03:36 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:03:36 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: TRANSLATION-XX-EN ----------
2025-08-29 12:03:36 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-XX-EN' using TranslationMetric
2025-08-29 12:03:36 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:03:36 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:03:36 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:03:36 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.91it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.86it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.95it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.93it/s]
2025-08-29 12:03:47 | INFO     | translation          | MetricX WMT24 score: 20.281250
2025-08-29 12:03:47 | INFO     | translation          | MetricX WMT24 with references score: 25.000000
2025-08-29 12:03:47 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:03:47 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_ta.jsonl
2025-08-29 12:03:47 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:03:47 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:03:47 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: TRANSLATION-EN-XX ----------
2025-08-29 12:03:47 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-EN-XX' using TranslationMetric
2025-08-29 12:03:47 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:03:47 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:03:47 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:03:47 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.87it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.85it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.95it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.92it/s]
2025-08-29 12:03:58 | INFO     | translation          | MetricX WMT24 score: 15.687500
2025-08-29 12:03:58 | INFO     | translation          | MetricX WMT24 with references score: 18.593750
2025-08-29 12:03:58 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:03:58 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_tl.jsonl
2025-08-29 12:03:58 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:03:58 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:03:58 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: TRANSLATION-XX-EN ----------
2025-08-29 12:03:58 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-XX-EN' using TranslationMetric
2025-08-29 12:03:58 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:03:58 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:03:58 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:03:58 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.87it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.83it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.92it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.90it/s]
2025-08-29 12:04:09 | INFO     | translation          | MetricX WMT24 score: 14.625000
2025-08-29 12:04:09 | INFO     | translation          | MetricX WMT24 with references score: 24.625000
2025-08-29 12:04:09 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:04:09 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_tl.jsonl
2025-08-29 12:04:09 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:04:09 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:04:09 | INFO     | seahelm_evaluation   | Waiting for mt-bench evaluation to complete
2025-08-29 12:04:09 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_id.jsonl
2025-08-29 12:04:09 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:04:09 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_vi.jsonl
2025-08-29 12:04:09 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:04:09 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_th.jsonl
2025-08-29 12:04:09 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:04:09 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-11-59-30/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_tl.jsonl
2025-08-29 12:04:09 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:04:09 | INFO     | aggregate_metrics    | ---------- Aggregation of metrics ----------
2025-08-29 12:04:09 | INFO     | aggregate_metrics    | ---------- Aggregation | Lang: ID ----------
2025-08-29 12:04:09 | INFO     | aggregate_metrics    | ### Competency: NLU
2025-08-29 12:04:09 | INFO     | aggregate_metrics    | Overall normalized accuracy for <id_nlu>: 23.333333
                                                      | 
2025-08-29 12:04:09 | INFO     | aggregate_metrics    | ### Competency: SAFETY
2025-08-29 12:04:09 | INFO     | aggregate_metrics    | Overall normalized accuracy for <id_safety>: 0.000000
                                                      | 
2025-08-29 12:04:09 | INFO     | aggregate_metrics    | ### Competency: NLG
2025-08-29 12:04:09 | INFO     | aggregate_metrics    | Overall normalized accuracy for <id_nlg>: 20.133772
                                                      | 
2025-08-29 12:04:09 | INFO     | aggregate_metrics    | ### Competency: NLR
2025-08-29 12:04:09 | INFO     | aggregate_metrics    | Overall normalized accuracy for <id_nlr>: 0.000000
                                                      | 
2025-08-29 12:04:09 | INFO     | aggregate_metrics    | ### Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 12:04:09 | INFO     | aggregate_metrics    | ---------- Task: PRAGMATICS (ID) ----------
2025-08-29 12:04:09 | INFO     | aggregate_metrics    | Accuracy for phenomenon <ID_scalar_implicatures>: 0 / 4 : 0.000000
Traceback (most recent call last):
  File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_evaluation.py", line 1059, in <module>
    seahelm_eval.run_evaluation(
  File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_evaluation.py", line 891, in run_evaluation
    metrics = aggregate_metrics(metrics, config=self.config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/aggregate_metrics.py", line 126, in aggregate_metrics
    metrics = aggregate_pragmatics_metrics(metrics, lang)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/aggregate_metrics.py", line 35, in aggregate_pragmatics_metrics
    subset_accuracy = correct_count / total_count
                      ~~~~~~~~~~~~~~^~~~~~~~~~~~~
ZeroDivisionError: division by zero

Execution finished at 08-29 12:04:12
