To view the log, run:
  $ tail -f output-vertex_ai/08-29-11-50-26/run-vertex_ai.out

python seahelm_evaluation.py --tasks seahelm --model_type litellm --output_dir output-vertex_ai/08-29-11-50-26 --model_name gemini-2.5-flash --model_args api_provider=vertex_ai --skip_tokenize_prompts --limit 1 
2025-08-29 11:50:35 | INFO     | seahelm_evaluation   | Loading model gemini-2.5-flash using VERTEX_AI...
2025-08-29 11:50:35 | INFO     | seahelm_evaluation   | ---------- Preparation of output folder ----------
                                                      | Preparing output folder ...
                                                      | Folder: output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference
2025-08-29 11:50:35 | INFO     | seahelm_evaluation   | Completed preparation of output folder!
                                                      | 
2025-08-29 11:50:35 | INFO     | seahelm_evaluation   | <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
                                                      | Evaluating gemini-2.5-flash as instruction-tuned model...
                                                      | <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
2025-08-29 11:50:36 | INFO     | seahelm_evaluation   | ---------- Configuration saving ----------
                                                      | Saving run config to output folder...
                                                      | Filepath: output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/gemini-2.5-flash_run_config_2025-08-29T11:50:35.669028.yaml
2025-08-29 11:50:36 | INFO     | seahelm_evaluation   | Config file saved!
                                                      | 
2025-08-29 11:50:36 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: MT-BENCH ----------
                                                      | Testing Competency: MULTI-TURN
2025-08-29 11:50:36 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/multi_turn/mt_bench/data/id_sea_mt_bench.jsonl
2025-08-29 11:50:36 | INFO     | seahelm_evaluation   | Performing inference for task 'MT-BENCH' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:50:36 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 94.36 examples/s]
[92m11:50:37 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:50:37 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:50:41 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:50:41 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 139.96 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 150.76 examples/s]
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:50:41 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 121.01 examples/s]
[92m11:50:42 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:50:42 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:50:46 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:50:46 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 132.17 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 147.00 examples/s]
2025-08-29 11:50:46 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_id.jsonl
2025-08-29 11:50:46 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:50:46 | INFO     | seahelm_evaluation   | Inference for task 'MT-BENCH' completed!
                                                      | 
2025-08-29 11:50:46 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: MT-BENCH ----------
                                                      | Testing Competency: MULTI-TURN
2025-08-29 11:50:46 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/multi_turn/mt_bench/data/vi_sea_mt_bench.jsonl
2025-08-29 11:50:47 | INFO     | seahelm_evaluation   | Performing inference for task 'MT-BENCH' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:50:47 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 146.91 examples/s]
[92m11:50:47 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:50:47 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:50:52 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:50:52 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 144.10 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 154.60 examples/s]
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:50:52 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 116.18 examples/s]
[92m11:50:52 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:50:52 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:50:57 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:50:57 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 134.08 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 145.84 examples/s]
2025-08-29 11:50:57 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_vi.jsonl
2025-08-29 11:50:57 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:50:57 | INFO     | seahelm_evaluation   | Inference for task 'MT-BENCH' completed!
                                                      | 
2025-08-29 11:50:57 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: MT-BENCH ----------
                                                      | Testing Competency: MULTI-TURN
2025-08-29 11:50:57 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/multi_turn/mt_bench/data/mt_bench_thai_full.jsonl
2025-08-29 11:50:58 | INFO     | seahelm_evaluation   | Performing inference for task 'MT-BENCH' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:50:58 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 156.76 examples/s]
[92m11:50:58 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:50:58 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:50:59 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:50:59 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 160.06 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 161.61 examples/s]
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:50:59 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 130.33 examples/s]
[92m11:50:59 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:50:59 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:04 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:04 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 141.08 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 157.92 examples/s]
2025-08-29 11:51:04 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_th.jsonl
2025-08-29 11:51:04 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:04 | INFO     | seahelm_evaluation   | Inference for task 'MT-BENCH' completed!
                                                      | 
2025-08-29 11:51:04 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: MT-BENCH ----------
                                                      | Testing Competency: MULTI-TURN
2025-08-29 11:51:04 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/multi_turn/mt_bench/data/mt_bench_tagalog_full.jsonl
2025-08-29 11:51:04 | INFO     | seahelm_evaluation   | Performing inference for task 'MT-BENCH' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:04 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 153.69 examples/s]
[92m11:51:04 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:04 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:09 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:09 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 158.59 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 161.73 examples/s]
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:09 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 129.60 examples/s]
[92m11:51:09 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:09 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:14 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:14 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 144.63 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 155.66 examples/s]
2025-08-29 11:51:14 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_tl.jsonl
2025-08-29 11:51:14 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:14 | INFO     | seahelm_evaluation   | Inference for task 'MT-BENCH' completed!
                                                      | 
2025-08-29 11:51:14 | INFO     | seahelm_evaluation   | Starting mt-bench evaluation using multiprocessing
2025-08-29 11:51:14 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: SENTIMENT ----------
                                                      | Testing Competency: NLU
2025-08-29 11:51:14 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/sentiment_analysis/data/id_nusax.jsonl
2025-08-29 11:51:14 | INFO     | mt_bench             | --------- Evaluation | Lang: ID | Task: MT-BENCH ----------
2025-08-29 11:51:14 | INFO     | mt_bench             | Evaluating MT-BENCH using MTBenchMetric
2025-08-29 11:51:14 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:14 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:14 | INFO     | mt_bench             | --------- Evaluation | Lang: VI | Task: MT-BENCH ----------
2025-08-29 11:51:14 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:14 | INFO     | mt_bench             | Evaluating MT-BENCH using MTBenchMetric
2025-08-29 11:51:14 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:51:14 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:14 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:14 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:14 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:51:14 | INFO     | mt_bench             | --------- Evaluation | Lang: TH | Task: MT-BENCH ----------
2025-08-29 11:51:14 | INFO     | mt_bench             | Evaluating MT-BENCH using MTBenchMetric
2025-08-29 11:51:14 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:14 | INFO     | mt_bench             | First run: processing all judgments.
2025-08-29 11:51:14 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:14 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:14 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:51:14 | INFO     | mt_bench             | First run: processing all judgments.
2025-08-29 11:51:14 | INFO     | mt_bench             | --------- Evaluation | Lang: TL | Task: MT-BENCH ----------
2025-08-29 11:51:14 | INFO     | mt_bench             | Evaluating MT-BENCH using MTBenchMetric
2025-08-29 11:51:14 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:14 | INFO     | mt_bench             | First run: processing all judgments.
2025-08-29 11:51:14 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:14 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:14 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:51:14 | INFO     | mt_bench             | First run: processing all judgments.
2025-08-29 11:51:14 | INFO     | seahelm_evaluation   | Performing inference for task 'SENTIMENT' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:14 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 151.51 examples/s]
[92m11:51:14 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:14 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:15 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:15 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 184.23 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 190.04 examples/s]
2025-08-29 11:51:15 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_id.jsonl
2025-08-29 11:51:15 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:15 | INFO     | seahelm_evaluation   | Inference for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 11:51:15 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: SENTIMENT ----------
2025-08-29 11:51:15 | INFO     | seahelm_evaluation   | Evaluating 'SENTIMENT' using SentimentAnalysisMetric
2025-08-29 11:51:15 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:15 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:15 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:15 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:51:15 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:51:15 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 0]
                                                      |  [1 0]]
2025-08-29 11:51:15 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |         none       0.00      0.00      0.00       0.0
                                                      |     positive       0.00      0.00      0.00       1.0
                                                      | 
                                                      |     accuracy                           0.00       1.0
                                                      |    macro avg       0.00      0.00      0.00       1.0
                                                      | weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:51:15 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:15 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_id.jsonl
2025-08-29 11:51:15 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:15 | INFO     | seahelm_evaluation   | Evaluation for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 11:51:15 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: QA ----------
                                                      | Testing Competency: NLU
2025-08-29 11:51:15 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/question_answering/data/id_tydiqa_100sample.jsonl
2025-08-29 11:51:15 | INFO     | seahelm_evaluation   | Performing inference for task 'QA' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:15 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 159.26 examples/s]
[92m11:51:15 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:15 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:16 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:16 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 173.12 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 180.91 examples/s]
2025-08-29 11:51:16 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_qa_id.jsonl
2025-08-29 11:51:16 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:16 | INFO     | seahelm_evaluation   | Inference for task 'QA' completed!
                                                      | 
2025-08-29 11:51:16 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: QA ----------
2025-08-29 11:51:19 | INFO     | seahelm_evaluation   | Evaluating 'QA' using QuestionAnsweringMetric
2025-08-29 11:51:19 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:19 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:19 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:19 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:51:19 | INFO     | question_answering   | {'exact_match': 0.0, 'f1': 40.0, 'normalized_f1': 40.0}
2025-08-29 11:51:19 | INFO     | question_answering   | 1 answers out of 1 (100.00%) can be found in the model's predictions.
2025-08-29 11:51:19 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:19 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_qa_id.jsonl
2025-08-29 11:51:19 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:19 | INFO     | seahelm_evaluation   | Evaluation for task 'QA' completed!
                                                      | 
2025-08-29 11:51:19 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: METAPHOR ----------
                                                      | Testing Competency: NLU
2025-08-29 11:51:19 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/metaphor/data/id_multilingual_fig_qa.jsonl
2025-08-29 11:51:19 | INFO     | seahelm_evaluation   | Performing inference for task 'METAPHOR' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:19 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 165.55 examples/s]
[92m11:51:19 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:19 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:20 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:20 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 173.29 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 181.27 examples/s]
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Saving inference results for task 'METAPHOR' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_metaphor_id.jsonl
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Inference for task 'METAPHOR' completed!
                                                      | 
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: METAPHOR ----------
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Evaluating 'METAPHOR' using MetaphorUnderstandingMetric
2025-08-29 11:51:20 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:20 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:20 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:20 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:51:20 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:51:20 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:51:20 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       1.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       1.0
                                                      |    macro avg       0.00      0.00      0.00       1.0
                                                      | weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:51:20 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Saving inference results for task 'METAPHOR' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_metaphor_id.jsonl
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Evaluation for task 'METAPHOR' completed!
                                                      | 
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: TOXICITY ----------
                                                      | Testing Competency: SAFETY
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/safety/toxicity_detection/data/id_ml-hsd_1000sample.jsonl
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Performing inference for task 'TOXICITY' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:20 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 133.03 examples/s]
[92m11:51:20 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:20 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:20 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:20 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 134.58 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 139.64 examples/s]
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_id.jsonl
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Inference for task 'TOXICITY' completed!
                                                      | 
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: TOXICITY ----------
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Evaluating 'TOXICITY' using ToxicityDetectionMetric
2025-08-29 11:51:20 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:20 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:20 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:20 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:51:20 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:51:20 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:51:20 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       1.0
                                                      |            3       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       1.0
                                                      |    macro avg       0.00      0.00      0.00       1.0
                                                      | weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:51:20 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_id.jsonl
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Evaluation for task 'TOXICITY' completed!
                                                      | 
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: TRANSLATION-EN-XX ----------
                                                      | Testing Competency: NLG
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/en_to_ind_Latn.jsonl
2025-08-29 11:51:20 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-EN-XX' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:20 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 198.12 examples/s]
[92m11:51:20 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:20 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:22 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:22 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 194.31 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 196.40 examples/s]
2025-08-29 11:51:22 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_id.jsonl
2025-08-29 11:51:22 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:22 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 11:51:22 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: TRANSLATION-XX-EN ----------
                                                      | Testing Competency: NLG
2025-08-29 11:51:22 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/ind_Latn_to_en.jsonl
2025-08-29 11:51:22 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-XX-EN' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:22 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 187.25 examples/s]
[92m11:51:22 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:22 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:24 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:24 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 195.40 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 184.46 examples/s]
2025-08-29 11:51:24 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_id.jsonl
2025-08-29 11:51:24 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:24 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 11:51:24 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: ABSSUM ----------
                                                      | Testing Competency: NLG
2025-08-29 11:51:24 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/abstractive_summarization/data/id_xlsum_100sample.jsonl
2025-08-29 11:51:24 | INFO     | seahelm_evaluation   | Performing inference for task 'ABSSUM' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:24 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 176.29 examples/s]
[92m11:51:24 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:24 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:25 | INFO     | openai_serving       | Prompts sent via OpenAI batch API
2025-08-29 11:51:25 | INFO     | openai_serving       | Waiting for OpenAI batch to complete...
2025-08-29 11:51:25 | INFO     | openai_serving       | Prompts sent via OpenAI batch API
2025-08-29 11:51:25 | INFO     | openai_serving       | Waiting for OpenAI batch to complete...
2025-08-29 11:51:25 | INFO     | openai_serving       | Prompts sent via OpenAI batch API
2025-08-29 11:51:25 | INFO     | openai_serving       | Waiting for OpenAI batch to complete...
[92m11:51:26 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:26 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 172.61 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 169.02 examples/s]
2025-08-29 11:51:26 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_id.jsonl
2025-08-29 11:51:26 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:26 | INFO     | seahelm_evaluation   | Inference for task 'ABSSUM' completed!
                                                      | 
2025-08-29 11:51:26 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: ABSSUM ----------
2025-08-29 11:51:26 | INFO     | seahelm_evaluation   | Evaluating 'ABSSUM' using SummarizationMetric
2025-08-29 11:51:26 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:26 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:26 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:26 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:51:26 | INFO     | summarization        | Rouge-L Scores:
2025-08-29 11:51:26 | INFO     | summarization        | Precision: 10.53 | Recall: 21.05 | F1: 14.04
2025-08-29 11:51:26 | INFO     | summarization        | Norm F1 Score: 14.04
2025-08-29 11:51:26 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:26 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_id.jsonl
2025-08-29 11:51:26 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:26 | INFO     | seahelm_evaluation   | Evaluation for task 'ABSSUM' completed!
                                                      | 
2025-08-29 11:51:26 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: CAUSAL ----------
                                                      | Testing Competency: NLR
2025-08-29 11:51:26 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/causal/data/id_xcopa.jsonl
2025-08-29 11:51:26 | INFO     | seahelm_evaluation   | Performing inference for task 'CAUSAL' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:26 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 165.63 examples/s]
[92m11:51:26 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:26 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:27 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:27 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 155.42 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 168.61 examples/s]
2025-08-29 11:51:27 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_causal_id.jsonl
2025-08-29 11:51:27 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:27 | INFO     | seahelm_evaluation   | Inference for task 'CAUSAL' completed!
                                                      | 
2025-08-29 11:51:27 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: CAUSAL ----------
2025-08-29 11:51:27 | INFO     | seahelm_evaluation   | Evaluating 'CAUSAL' using CausalReasoningMetric
2025-08-29 11:51:27 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:27 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:27 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:27 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:51:27 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:51:27 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:51:27 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       1.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       1.0
                                                      |    macro avg       0.00      0.00      0.00       1.0
                                                      | weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:51:27 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:27 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_causal_id.jsonl
2025-08-29 11:51:27 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:27 | INFO     | seahelm_evaluation   | Evaluation for task 'CAUSAL' completed!
                                                      | 
2025-08-29 11:51:27 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: NLI ----------
                                                      | Testing Competency: NLR
2025-08-29 11:51:27 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/nli/data/id_indonli_lay_1000sample.jsonl
2025-08-29 11:51:27 | INFO     | openai_serving       | Prompts sent via OpenAI batch API
2025-08-29 11:51:27 | INFO     | openai_serving       | Waiting for OpenAI batch to complete...
2025-08-29 11:51:27 | INFO     | seahelm_evaluation   | Performing inference for task 'NLI' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:27 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 188.72 examples/s]
[92m11:51:28 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:28 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:28 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:28 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 179.66 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 193.37 examples/s]
2025-08-29 11:51:28 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_nli_id.jsonl
2025-08-29 11:51:28 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:28 | INFO     | seahelm_evaluation   | Inference for task 'NLI' completed!
                                                      | 
2025-08-29 11:51:28 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: NLI ----------
2025-08-29 11:51:28 | INFO     | seahelm_evaluation   | Evaluating 'NLI' using NLIMetric
2025-08-29 11:51:28 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:28 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:28 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:28 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:51:28 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:51:28 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:51:28 | INFO     | seahelm_metric       | Classification report:
                                                      |                precision    recall  f1-score   support
                                                      | 
                                                      | contradiction       0.00      0.00      0.00       1.0
                                                      |          none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |      accuracy                           0.00       1.0
                                                      |     macro avg       0.00      0.00      0.00       1.0
                                                      |  weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:51:28 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:28 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_nli_id.jsonl
2025-08-29 11:51:28 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:28 | INFO     | seahelm_evaluation   | Evaluation for task 'NLI' completed!
                                                      | 
2025-08-29 11:51:28 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: MP-R ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 11:51:28 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/syntax/data/id_syntax_mcq_randomized.jsonl
2025-08-29 11:51:28 | INFO     | seahelm_evaluation   | Performing inference for task 'MP-R' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:28 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 159.26 examples/s]
[92m11:51:28 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:28 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:29 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:29 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 143.77 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 158.72 examples/s]
2025-08-29 11:51:29 | INFO     | seahelm_evaluation   | Saving inference results for task 'MP-R' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_mp-r_id.jsonl
2025-08-29 11:51:29 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:29 | INFO     | seahelm_evaluation   | Inference for task 'MP-R' completed!
                                                      | 
2025-08-29 11:51:29 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: MP-R ----------
2025-08-29 11:51:29 | INFO     | seahelm_evaluation   | Evaluating 'MP-R' using MinimalPairsMetric
2025-08-29 11:51:29 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:29 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:29 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:29 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:51:29 | INFO     | minimal_pairs        | Accuracy for phenomenon <NPIs_and_negation>: 0.0
2025-08-29 11:51:29 | INFO     | minimal_pairs        | Overall Accuracy: 0.0
2025-08-29 11:51:29 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:29 | INFO     | seahelm_evaluation   | Saving inference results for task 'MP-R' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_mp-r_id.jsonl
2025-08-29 11:51:29 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:29 | INFO     | seahelm_evaluation   | Evaluation for task 'MP-R' completed!
                                                      | 
2025-08-29 11:51:29 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: PRAGMATIC-SINGLE ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 11:51:29 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/pragmatics/data/id_pragmatic_reasoning_single.jsonl
2025-08-29 11:51:29 | INFO     | seahelm_evaluation   | Performing inference for task 'PRAGMATIC-SINGLE' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:29 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 153.22 examples/s]
[92m11:51:29 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:29 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:30 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:30 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 146.76 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 154.40 examples/s]
2025-08-29 11:51:30 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-SINGLE' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-single_id.jsonl
2025-08-29 11:51:30 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:30 | INFO     | seahelm_evaluation   | Inference for task 'PRAGMATIC-SINGLE' completed!
                                                      | 
2025-08-29 11:51:30 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: PRAGMATIC-SINGLE ----------
2025-08-29 11:51:30 | INFO     | seahelm_evaluation   | Evaluating 'PRAGMATIC-SINGLE' using PragmaticReasoningSingleSentenceMetric
2025-08-29 11:51:30 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:30 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:30 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:30 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  subset_references = subset[self.label_column].replace(
2025-08-29 11:51:30 | INFO     | pragmatic_reasoning  | Accuracy for phenomenon <scalar_implicatures>: 0.0 / 1
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:93: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  references = self.inference_df[self.label_column].replace(
2025-08-29 11:51:30 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:30 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-SINGLE' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-single_id.jsonl
2025-08-29 11:51:30 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:30 | INFO     | seahelm_evaluation   | Evaluation for task 'PRAGMATIC-SINGLE' completed!
                                                      | 
2025-08-29 11:51:30 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: PRAGMATIC-PAIR ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 11:51:30 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/pragmatics/data/id_pragmatic_reasoning_pair.jsonl
2025-08-29 11:51:30 | INFO     | seahelm_evaluation   | Performing inference for task 'PRAGMATIC-PAIR' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:30 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 164.21 examples/s]
[92m11:51:31 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:31 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:32 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:32 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 165.27 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 169.70 examples/s]
2025-08-29 11:51:32 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-PAIR' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-pair_id.jsonl
2025-08-29 11:51:32 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:32 | INFO     | seahelm_evaluation   | Inference for task 'PRAGMATIC-PAIR' completed!
                                                      | 
2025-08-29 11:51:32 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: PRAGMATIC-PAIR ----------
2025-08-29 11:51:32 | INFO     | seahelm_evaluation   | Evaluating 'PRAGMATIC-PAIR' using PragmaticReasoningSentencePairMetric
2025-08-29 11:51:32 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:32 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:32 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:32 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:143: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  subset_references = subset[self.label_column].replace({True: 1, False: 0})
2025-08-29 11:51:32 | INFO     | pragmatic_reasoning  | Accuracy for phenomenon <scalar_implicatures>: 0.0 / 1
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:157: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  references = self.inference_df[self.label_column].replace({True: 1, False: 0})
2025-08-29 11:51:32 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:32 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-PAIR' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-pair_id.jsonl
2025-08-29 11:51:32 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:32 | INFO     | seahelm_evaluation   | Evaluation for task 'PRAGMATIC-PAIR' completed!
                                                      | 
2025-08-29 11:51:32 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: IF-EVAL ----------
                                                      | Testing Competency: INSTRUCTION-FOLLOWING
2025-08-29 11:51:32 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/instruction_following/ifeval/data/id_sea_ifeval.jsonl
2025-08-29 11:51:32 | INFO     | seahelm_evaluation   | Performing inference for task 'IF-EVAL' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:32 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 102.84 examples/s]
[92m11:51:32 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:32 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:35 | INFO     | openai_serving       | Still waiting (10s has elapsed)...
[92m11:51:35 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:35 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:35 | INFO     | openai_serving       | Still waiting (10s has elapsed)...
[92m11:51:35 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:35 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:35 | INFO     | openai_serving       | Still waiting (10s has elapsed)...
[92m11:51:35 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:35 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m11:51:37 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:37 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 101.56 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 108.03 examples/s]
2025-08-29 11:51:37 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_id.jsonl
2025-08-29 11:51:37 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:37 | INFO     | seahelm_evaluation   | Inference for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 11:51:37 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: IF-EVAL ----------
2025-08-29 11:51:37 | INFO     | seahelm_evaluation   | Evaluating 'IF-EVAL' using IFEvalMetric
2025-08-29 11:51:37 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:37 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:37 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:37 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:51:37 | ERROR    | seahelm_evaluation   | Failed to run evaluation for task if-eval and lang id
2025-08-29 11:51:37 | ERROR    | seahelm_evaluation   | True
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 3812, in get_loc
                                                      |     return self._engine.get_loc(casted_key)
                                                      |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "pandas/_libs/index.pyx", line 167, in pandas._libs.index.IndexEngine.get_loc
                                                      |   File "pandas/_libs/index.pyx", line 196, in pandas._libs.index.IndexEngine.get_loc
                                                      |   File "pandas/_libs/hashtable_class_helper.pxi", line 5846, in pandas._libs.hashtable.UInt8HashTable.get_item
                                                      |   File "pandas/_libs/hashtable_class_helper.pxi", line 5870, in pandas._libs.hashtable.UInt8HashTable.get_item
                                                      | KeyError: 1
                                                      | 
                                                      | The above exception was the direct cause of the following exception:
                                                      | 
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_evaluation.py", line 677, in run_single_task_evaluation
                                                      |     metric_json, inference_df = evaluation_metric.evaluate_responses()
                                                      |                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/seahelm_metric.py", line 118, in evaluate_responses
                                                      |     output_json, inference_df = self.calculate_metrics()
                                                      |                                 ^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/instruction_following/ifeval/if_eval.py", line 72, in calculate_metrics
                                                      |     metric_dict = self.summarize_results(self.inference_df)
                                                      |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/instruction_following/ifeval/if_eval.py", line 144, in summarize_results
                                                      |     overall_pass = int(inference_df["result"].value_counts()[True])
                                                      |                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/series.py", line 1130, in __getitem__
                                                      |     return self._get_value(key)
                                                      |            ^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/series.py", line 1246, in _get_value
                                                      |     loc = self.index.get_loc(label)
                                                      |           ^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 3819, in get_loc
                                                      |     raise KeyError(key) from err
                                                      | KeyError: True
2025-08-29 11:51:37 | WARNING  | seahelm_evaluation   | Setting metric overall_lang_normalized_acc to 0 for task if-eval
2025-08-29 11:51:37 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: SENTIMENT ----------
                                                      | Testing Competency: NLU
2025-08-29 11:51:37 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/sentiment_analysis/data/vi_uit-vsfc_1000sample.jsonl
2025-08-29 11:51:37 | INFO     | seahelm_evaluation   | Performing inference for task 'SENTIMENT' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:37 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 185.73 examples/s]
[92m11:51:37 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:37 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:37 | INFO     | openai_serving       | Still waiting (10s has elapsed)...
[92m11:51:37 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:37 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m11:51:38 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:38 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 196.43 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 197.57 examples/s]
2025-08-29 11:51:38 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_vi.jsonl
2025-08-29 11:51:38 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:38 | INFO     | seahelm_evaluation   | Inference for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 11:51:38 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: SENTIMENT ----------
2025-08-29 11:51:38 | INFO     | seahelm_evaluation   | Evaluating 'SENTIMENT' using SentimentAnalysisMetric
2025-08-29 11:51:38 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:38 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:38 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:38 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:51:38 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:51:38 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:51:38 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |     negative       0.00      0.00      0.00       1.0
                                                      |         none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       1.0
                                                      |    macro avg       0.00      0.00      0.00       1.0
                                                      | weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:51:38 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:38 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_vi.jsonl
2025-08-29 11:51:38 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:38 | INFO     | seahelm_evaluation   | Evaluation for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 11:51:38 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: QA ----------
                                                      | Testing Competency: NLU
2025-08-29 11:51:38 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/question_answering/data/vi_xquad_100sample.jsonl
2025-08-29 11:51:38 | INFO     | seahelm_evaluation   | Performing inference for task 'QA' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:38 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 184.22 examples/s]
[92m11:51:38 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:38 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:39 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:39 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 170.47 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 189.14 examples/s]
2025-08-29 11:51:39 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_qa_vi.jsonl
2025-08-29 11:51:39 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:39 | INFO     | seahelm_evaluation   | Inference for task 'QA' completed!
                                                      | 
2025-08-29 11:51:39 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: QA ----------
2025-08-29 11:51:39 | INFO     | seahelm_evaluation   | Evaluating 'QA' using QuestionAnsweringMetric
2025-08-29 11:51:39 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:39 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:39 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:39 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:51:39 | INFO     | question_answering   | {'exact_match': 100.0, 'f1': 100.0, 'normalized_f1': 100.0}
2025-08-29 11:51:39 | INFO     | question_answering   | 1 answers out of 1 (100.00%) can be found in the model's predictions.
2025-08-29 11:51:39 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:39 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_qa_vi.jsonl
2025-08-29 11:51:39 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:39 | INFO     | seahelm_evaluation   | Evaluation for task 'QA' completed!
                                                      | 
2025-08-29 11:51:39 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: TOXICITY ----------
                                                      | Testing Competency: SAFETY
2025-08-29 11:51:39 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/safety/toxicity_detection/data/vi_vihsd_1000sample.jsonl
2025-08-29 11:51:39 | INFO     | seahelm_evaluation   | Performing inference for task 'TOXICITY' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:39 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 192.91 examples/s]
[92m11:51:39 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:39 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:39 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:39 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 197.81 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 201.72 examples/s]
2025-08-29 11:51:39 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_vi.jsonl
2025-08-29 11:51:39 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:39 | INFO     | seahelm_evaluation   | Inference for task 'TOXICITY' completed!
                                                      | 
2025-08-29 11:51:39 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: TOXICITY ----------
2025-08-29 11:51:39 | INFO     | seahelm_evaluation   | Evaluating 'TOXICITY' using ToxicityDetectionMetric
2025-08-29 11:51:39 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:39 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:39 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:39 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:51:40 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:51:40 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:51:40 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       1.0
                                                      |            3       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       1.0
                                                      |    macro avg       0.00      0.00      0.00       1.0
                                                      | weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:51:40 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:40 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_vi.jsonl
2025-08-29 11:51:40 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:40 | INFO     | seahelm_evaluation   | Evaluation for task 'TOXICITY' completed!
                                                      | 
2025-08-29 11:51:40 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: TRANSLATION-EN-XX ----------
                                                      | Testing Competency: NLG
2025-08-29 11:51:40 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/en_to_vie_Latn.jsonl
2025-08-29 11:51:40 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-EN-XX' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:40 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 190.55 examples/s]
[92m11:51:40 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:40 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:41 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:41 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 197.81 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 204.69 examples/s]
2025-08-29 11:51:41 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_vi.jsonl
2025-08-29 11:51:41 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:41 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 11:51:41 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: TRANSLATION-XX-EN ----------
                                                      | Testing Competency: NLG
2025-08-29 11:51:41 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/vie_Latn_to_en.jsonl
2025-08-29 11:51:41 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-XX-EN' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:41 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 208.92 examples/s]
[92m11:51:41 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:41 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:43 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:43 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 211.08 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 218.53 examples/s]
2025-08-29 11:51:43 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_vi.jsonl
2025-08-29 11:51:43 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:43 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 11:51:43 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: ABSSUM ----------
                                                      | Testing Competency: NLG
2025-08-29 11:51:43 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/abstractive_summarization/data/vi_xlsum_100sample.jsonl
2025-08-29 11:51:43 | INFO     | seahelm_evaluation   | Performing inference for task 'ABSSUM' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:43 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 171.62 examples/s]
[92m11:51:43 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:43 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:45 | INFO     | openai_serving       | Still waiting (20s has elapsed)...
[92m11:51:45 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:45 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:45 | INFO     | openai_serving       | Still waiting (20s has elapsed)...
[92m11:51:45 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:45 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:46 | INFO     | openai_serving       | Still waiting (20s has elapsed)...
[92m11:51:46 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:46 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m11:51:46 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:46 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 174.84 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 181.45 examples/s]
2025-08-29 11:51:46 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_vi.jsonl
2025-08-29 11:51:46 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:46 | INFO     | seahelm_evaluation   | Inference for task 'ABSSUM' completed!
                                                      | 
2025-08-29 11:51:46 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: ABSSUM ----------
2025-08-29 11:51:46 | INFO     | seahelm_evaluation   | Evaluating 'ABSSUM' using SummarizationMetric
2025-08-29 11:51:46 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:46 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:46 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:46 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:51:46 | INFO     | summarization        | Rouge-L Scores:
2025-08-29 11:51:46 | INFO     | summarization        | Precision: 0.00 | Recall: 0.00 | F1: 0.00
2025-08-29 11:51:46 | INFO     | summarization        | Norm F1 Score: 0.00
2025-08-29 11:51:46 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:46 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_vi.jsonl
2025-08-29 11:51:46 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:46 | INFO     | seahelm_evaluation   | Evaluation for task 'ABSSUM' completed!
                                                      | 
2025-08-29 11:51:46 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: CAUSAL ----------
                                                      | Testing Competency: NLR
2025-08-29 11:51:46 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/causal/data/vi_xcopa.jsonl
2025-08-29 11:51:46 | INFO     | seahelm_evaluation   | Performing inference for task 'CAUSAL' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:46 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 160.00 examples/s]
[92m11:51:46 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:46 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:47 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:47 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 166.35 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 173.10 examples/s]
2025-08-29 11:51:47 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_causal_vi.jsonl
2025-08-29 11:51:47 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:47 | INFO     | seahelm_evaluation   | Inference for task 'CAUSAL' completed!
                                                      | 
2025-08-29 11:51:47 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: CAUSAL ----------
2025-08-29 11:51:47 | INFO     | seahelm_evaluation   | Evaluating 'CAUSAL' using CausalReasoningMetric
2025-08-29 11:51:47 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:47 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:47 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:47 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:51:47 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:51:47 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:51:47 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       1.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       1.0
                                                      |    macro avg       0.00      0.00      0.00       1.0
                                                      | weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:51:47 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:47 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_causal_vi.jsonl
2025-08-29 11:51:47 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:47 | INFO     | seahelm_evaluation   | Evaluation for task 'CAUSAL' completed!
                                                      | 
2025-08-29 11:51:47 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: NLI ----------
                                                      | Testing Competency: NLR
2025-08-29 11:51:47 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/nli/data/vi_xnli_1000sample.jsonl
2025-08-29 11:51:47 | INFO     | seahelm_evaluation   | Performing inference for task 'NLI' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:47 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 181.09 examples/s]
[92m11:51:47 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:47 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:48 | INFO     | openai_serving       | Still waiting (20s has elapsed)...
[92m11:51:48 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:48 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m11:51:48 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:48 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 184.46 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 192.67 examples/s]
2025-08-29 11:51:48 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_nli_vi.jsonl
2025-08-29 11:51:48 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:48 | INFO     | seahelm_evaluation   | Inference for task 'NLI' completed!
                                                      | 
2025-08-29 11:51:48 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: NLI ----------
2025-08-29 11:51:48 | INFO     | seahelm_evaluation   | Evaluating 'NLI' using NLIMetric
2025-08-29 11:51:48 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:48 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:48 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:48 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:51:48 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:51:48 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:51:48 | INFO     | seahelm_metric       | Classification report:
                                                      |                precision    recall  f1-score   support
                                                      | 
                                                      | contradiction       0.00      0.00      0.00       1.0
                                                      |          none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |      accuracy                           0.00       1.0
                                                      |     macro avg       0.00      0.00      0.00       1.0
                                                      |  weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:51:48 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:48 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_nli_vi.jsonl
2025-08-29 11:51:48 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:48 | INFO     | seahelm_evaluation   | Evaluation for task 'NLI' completed!
                                                      | 
2025-08-29 11:51:48 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: IF-EVAL ----------
                                                      | Testing Competency: INSTRUCTION-FOLLOWING
2025-08-29 11:51:48 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/instruction_following/ifeval/data/vi_sea_ifeval.jsonl
2025-08-29 11:51:48 | INFO     | seahelm_evaluation   | Performing inference for task 'IF-EVAL' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:48 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 95.07 examples/s]
[92m11:51:48 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:48 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:54 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:54 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 102.18 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 106.01 examples/s]
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_vi.jsonl
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | Inference for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: IF-EVAL ----------
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | Evaluating 'IF-EVAL' using IFEvalMetric
2025-08-29 11:51:54 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:54 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:54 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:54 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:51:54 | ERROR    | seahelm_evaluation   | Failed to run evaluation for task if-eval and lang vi
2025-08-29 11:51:54 | ERROR    | seahelm_evaluation   | True
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 3812, in get_loc
                                                      |     return self._engine.get_loc(casted_key)
                                                      |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "pandas/_libs/index.pyx", line 167, in pandas._libs.index.IndexEngine.get_loc
                                                      |   File "pandas/_libs/index.pyx", line 196, in pandas._libs.index.IndexEngine.get_loc
                                                      |   File "pandas/_libs/hashtable_class_helper.pxi", line 5846, in pandas._libs.hashtable.UInt8HashTable.get_item
                                                      |   File "pandas/_libs/hashtable_class_helper.pxi", line 5870, in pandas._libs.hashtable.UInt8HashTable.get_item
                                                      | KeyError: 1
                                                      | 
                                                      | The above exception was the direct cause of the following exception:
                                                      | 
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_evaluation.py", line 677, in run_single_task_evaluation
                                                      |     metric_json, inference_df = evaluation_metric.evaluate_responses()
                                                      |                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/seahelm_metric.py", line 118, in evaluate_responses
                                                      |     output_json, inference_df = self.calculate_metrics()
                                                      |                                 ^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/instruction_following/ifeval/if_eval.py", line 72, in calculate_metrics
                                                      |     metric_dict = self.summarize_results(self.inference_df)
                                                      |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/instruction_following/ifeval/if_eval.py", line 144, in summarize_results
                                                      |     overall_pass = int(inference_df["result"].value_counts()[True])
                                                      |                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/series.py", line 1130, in __getitem__
                                                      |     return self._get_value(key)
                                                      |            ^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/series.py", line 1246, in _get_value
                                                      |     loc = self.index.get_loc(label)
                                                      |           ^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 3819, in get_loc
                                                      |     raise KeyError(key) from err
                                                      | KeyError: True
2025-08-29 11:51:54 | WARNING  | seahelm_evaluation   | Setting metric overall_lang_normalized_acc to 0 for task if-eval
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: SENTIMENT ----------
                                                      | Testing Competency: NLU
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/sentiment_analysis/data/th_wisesight_no_q_1000sample.jsonl
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | Performing inference for task 'SENTIMENT' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:54 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 179.90 examples/s]
[92m11:51:54 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:54 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:54 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:54 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 182.41 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 188.34 examples/s]
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_th.jsonl
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | Inference for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: SENTIMENT ----------
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | Evaluating 'SENTIMENT' using SentimentAnalysisMetric
2025-08-29 11:51:54 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:54 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:54 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:54 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:51:54 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:51:54 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:51:54 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |     negative       0.00      0.00      0.00       1.0
                                                      |         none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       1.0
                                                      |    macro avg       0.00      0.00      0.00       1.0
                                                      | weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:51:54 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_th.jsonl
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | Evaluation for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: QA ----------
                                                      | Testing Competency: NLU
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/question_answering/data/th_xquad_100sample.jsonl
2025-08-29 11:51:54 | INFO     | seahelm_evaluation   | Performing inference for task 'QA' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:54 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 182.33 examples/s]
[92m11:51:55 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:55 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:56 | INFO     | openai_serving       | Still waiting (30s has elapsed)...
[92m11:51:56 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:56 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:56 | INFO     | openai_serving       | Still waiting (30s has elapsed)...
[92m11:51:56 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:56 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m11:51:56 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:56 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 175.33 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 185.60 examples/s]
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_qa_th.jsonl
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Inference for task 'QA' completed!
                                                      | 
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: QA ----------
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Evaluating 'QA' using QuestionAnsweringMetric
2025-08-29 11:51:56 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:56 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:56 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:56 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:51:56 | INFO     | question_answering   | Tokenizing Thai text and re-evaluating...
2025-08-29 11:51:56 | INFO     | question_answering   | {'exact_match': 0.0, 'f1': 0.0, 'normalized_f1': 0.0}
2025-08-29 11:51:56 | INFO     | question_answering   | 0 answers out of 1 (0.00%) can be found in the model's predictions.
2025-08-29 11:51:56 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_qa_th.jsonl
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Evaluation for task 'QA' completed!
                                                      | 
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: TOXICITY ----------
                                                      | Testing Competency: SAFETY
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/safety/toxicity_detection/data/th_toxicity_1000sample.jsonl
2025-08-29 11:51:56 | INFO     | openai_serving       | Still waiting (30s has elapsed)...
[92m11:51:56 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:56 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Performing inference for task 'TOXICITY' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:56 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 171.49 examples/s]
[92m11:51:56 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:56 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:56 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:56 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 172.53 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 178.69 examples/s]
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_th.jsonl
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Inference for task 'TOXICITY' completed!
                                                      | 
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: TOXICITY ----------
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Evaluating 'TOXICITY' using ToxicityDetectionMetric
2025-08-29 11:51:56 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:51:56 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:51:56 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:51:56 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:51:56 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:51:56 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:51:56 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       1.0
                                                      |            3       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       1.0
                                                      |    macro avg       0.00      0.00      0.00       1.0
                                                      | weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:51:56 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_th.jsonl
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Evaluation for task 'TOXICITY' completed!
                                                      | 
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: TRANSLATION-EN-XX ----------
                                                      | Testing Competency: NLG
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/en_to_tha_Thai.jsonl
2025-08-29 11:51:56 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-EN-XX' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:56 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 205.72 examples/s]
[92m11:51:56 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:56 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:58 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:58 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 192.28 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 198.40 examples/s]
2025-08-29 11:51:58 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_th.jsonl
2025-08-29 11:51:58 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:58 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 11:51:58 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: TRANSLATION-XX-EN ----------
                                                      | Testing Competency: NLG
2025-08-29 11:51:58 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/tha_Thai_to_en.jsonl
2025-08-29 11:51:58 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-XX-EN' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:58 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:58 | INFO     | openai_serving       | Still waiting (30s has elapsed)...
[92m11:51:58 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:51:58 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 195.26 examples/s]
[92m11:51:58 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:51:58 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:51:59 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:51:59 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 194.97 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 202.92 examples/s]
2025-08-29 11:51:59 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_th.jsonl
2025-08-29 11:51:59 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:51:59 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 11:51:59 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: ABSSUM ----------
                                                      | Testing Competency: NLG
2025-08-29 11:51:59 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/abstractive_summarization/data/th_xlsum_100sample.jsonl
2025-08-29 11:51:59 | INFO     | seahelm_evaluation   | Performing inference for task 'ABSSUM' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:51:59 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 172.64 examples/s]
[92m11:52:00 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:00 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:02 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:02 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 174.10 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 179.91 examples/s]
2025-08-29 11:52:02 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_th.jsonl
2025-08-29 11:52:02 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:02 | INFO     | seahelm_evaluation   | Inference for task 'ABSSUM' completed!
                                                      | 
2025-08-29 11:52:02 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: ABSSUM ----------
2025-08-29 11:52:02 | INFO     | seahelm_evaluation   | Evaluating 'ABSSUM' using SummarizationMetric
2025-08-29 11:52:02 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:02 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:02 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:02 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:52:02 | INFO     | summarization        | Rouge-L Scores:
2025-08-29 11:52:02 | INFO     | summarization        | Precision: 0.00 | Recall: 0.00 | F1: 0.00
2025-08-29 11:52:02 | INFO     | summarization        | Norm F1 Score: 0.00
2025-08-29 11:52:02 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:02 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_th.jsonl
2025-08-29 11:52:02 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:02 | INFO     | seahelm_evaluation   | Evaluation for task 'ABSSUM' completed!
                                                      | 
2025-08-29 11:52:02 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: CAUSAL ----------
                                                      | Testing Competency: NLR
2025-08-29 11:52:02 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/causal/data/th_xcopa.jsonl
2025-08-29 11:52:02 | INFO     | seahelm_evaluation   | Performing inference for task 'CAUSAL' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:02 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 158.08 examples/s]
[92m11:52:02 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:02 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:03 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:03 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 168.33 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 173.73 examples/s]
2025-08-29 11:52:03 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_causal_th.jsonl
2025-08-29 11:52:03 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:03 | INFO     | seahelm_evaluation   | Inference for task 'CAUSAL' completed!
                                                      | 
2025-08-29 11:52:03 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: CAUSAL ----------
2025-08-29 11:52:03 | INFO     | seahelm_evaluation   | Evaluating 'CAUSAL' using CausalReasoningMetric
2025-08-29 11:52:03 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:03 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:03 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:03 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:52:03 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:52:03 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:52:03 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       1.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       1.0
                                                      |    macro avg       0.00      0.00      0.00       1.0
                                                      | weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:52:03 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:03 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_causal_th.jsonl
2025-08-29 11:52:03 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:03 | INFO     | seahelm_evaluation   | Evaluation for task 'CAUSAL' completed!
                                                      | 
2025-08-29 11:52:03 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: NLI ----------
                                                      | Testing Competency: NLR
2025-08-29 11:52:03 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/nli/data/th_xnli_1000sample.jsonl
2025-08-29 11:52:03 | INFO     | seahelm_evaluation   | Performing inference for task 'NLI' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:03 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 178.20 examples/s]
[92m11:52:04 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:04 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:04 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:04 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 186.00 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 190.70 examples/s]
2025-08-29 11:52:04 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_nli_th.jsonl
2025-08-29 11:52:04 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:04 | INFO     | seahelm_evaluation   | Inference for task 'NLI' completed!
                                                      | 
2025-08-29 11:52:04 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: NLI ----------
2025-08-29 11:52:04 | INFO     | seahelm_evaluation   | Evaluating 'NLI' using NLIMetric
2025-08-29 11:52:04 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:04 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:04 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:04 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:52:04 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:52:04 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:52:04 | INFO     | seahelm_metric       | Classification report:
                                                      |                precision    recall  f1-score   support
                                                      | 
                                                      | contradiction       0.00      0.00      0.00       1.0
                                                      |          none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |      accuracy                           0.00       1.0
                                                      |     macro avg       0.00      0.00      0.00       1.0
                                                      |  weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:52:04 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:04 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_nli_th.jsonl
2025-08-29 11:52:04 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:04 | INFO     | seahelm_evaluation   | Evaluation for task 'NLI' completed!
                                                      | 
2025-08-29 11:52:04 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: IF-EVAL ----------
                                                      | Testing Competency: INSTRUCTION-FOLLOWING
2025-08-29 11:52:04 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/instruction_following/ifeval/data/th_sea_ifeval.jsonl
2025-08-29 11:52:04 | INFO     | seahelm_evaluation   | Performing inference for task 'IF-EVAL' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:04 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 106.63 examples/s]
[92m11:52:05 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:05 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:06 | INFO     | openai_serving       | Still waiting (40s has elapsed)...
[92m11:52:06 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:06 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:06 | INFO     | openai_serving       | Still waiting (40s has elapsed)...
[92m11:52:06 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:06 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:06 | INFO     | openai_serving       | Still waiting (40s has elapsed)...
[92m11:52:06 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:06 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:08 | INFO     | openai_serving       | Still waiting (40s has elapsed)...
[92m11:52:08 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:08 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m11:52:10 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:10 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 101.85 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 107.39 examples/s]
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_th.jsonl
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | Inference for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: IF-EVAL ----------
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | Evaluating 'IF-EVAL' using IFEvalMetric
2025-08-29 11:52:10 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:10 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:10 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:10 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:52:10 | ERROR    | seahelm_evaluation   | Failed to run evaluation for task if-eval and lang th
2025-08-29 11:52:10 | ERROR    | seahelm_evaluation   | False
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 3812, in get_loc
                                                      |     return self._engine.get_loc(casted_key)
                                                      |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "pandas/_libs/index.pyx", line 167, in pandas._libs.index.IndexEngine.get_loc
                                                      |   File "pandas/_libs/index.pyx", line 196, in pandas._libs.index.IndexEngine.get_loc
                                                      |   File "pandas/_libs/hashtable_class_helper.pxi", line 5846, in pandas._libs.hashtable.UInt8HashTable.get_item
                                                      |   File "pandas/_libs/hashtable_class_helper.pxi", line 5870, in pandas._libs.hashtable.UInt8HashTable.get_item
                                                      | KeyError: 0
                                                      | 
                                                      | The above exception was the direct cause of the following exception:
                                                      | 
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_evaluation.py", line 677, in run_single_task_evaluation
                                                      |     metric_json, inference_df = evaluation_metric.evaluate_responses()
                                                      |                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/seahelm_metric.py", line 118, in evaluate_responses
                                                      |     output_json, inference_df = self.calculate_metrics()
                                                      |                                 ^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/instruction_following/ifeval/if_eval.py", line 72, in calculate_metrics
                                                      |     metric_dict = self.summarize_results(self.inference_df)
                                                      |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/instruction_following/ifeval/if_eval.py", line 145, in summarize_results
                                                      |     overall_fail = int(inference_df["result"].value_counts()[False])
                                                      |                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/series.py", line 1130, in __getitem__
                                                      |     return self._get_value(key)
                                                      |            ^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/series.py", line 1246, in _get_value
                                                      |     loc = self.index.get_loc(label)
                                                      |           ^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 3819, in get_loc
                                                      |     raise KeyError(key) from err
                                                      | KeyError: False
2025-08-29 11:52:10 | WARNING  | seahelm_evaluation   | Setting metric overall_lang_normalized_acc to 0 for task if-eval
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: SENTIMENT ----------
                                                      | Testing Competency: NLU
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/sentiment_analysis/data/ta_indicsentiment.jsonl
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | Performing inference for task 'SENTIMENT' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:10 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 175.93 examples/s]
[92m11:52:10 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:10 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:10 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:10 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 185.81 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 189.65 examples/s]
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_ta.jsonl
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | Inference for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: SENTIMENT ----------
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | Evaluating 'SENTIMENT' using SentimentAnalysisMetric
2025-08-29 11:52:10 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:10 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:10 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:10 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:52:10 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:52:10 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:52:10 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |     negative       0.00      0.00      0.00       1.0
                                                      |         none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       1.0
                                                      |    macro avg       0.00      0.00      0.00       1.0
                                                      | weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:52:10 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_ta.jsonl
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | Evaluation for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: QA ----------
                                                      | Testing Competency: NLU
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/question_answering/data/ta_indicqa_100sample.jsonl
2025-08-29 11:52:10 | INFO     | seahelm_evaluation   | Performing inference for task 'QA' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:10 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 174.99 examples/s]
[92m11:52:10 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:10 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:12 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:12 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 175.08 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 180.59 examples/s]
2025-08-29 11:52:12 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_qa_ta.jsonl
2025-08-29 11:52:12 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:12 | INFO     | seahelm_evaluation   | Inference for task 'QA' completed!
                                                      | 
2025-08-29 11:52:12 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: QA ----------
2025-08-29 11:52:12 | INFO     | seahelm_evaluation   | Evaluating 'QA' using QuestionAnsweringMetric
2025-08-29 11:52:12 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:12 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:12 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:12 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:52:12 | INFO     | question_answering   | {'exact_match': 0.0, 'f1': 0.0, 'normalized_f1': 0.0}
2025-08-29 11:52:12 | INFO     | question_answering   | 0 answers out of 1 (0.00%) can be found in the model's predictions.
2025-08-29 11:52:12 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:12 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_qa_ta.jsonl
2025-08-29 11:52:12 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:12 | INFO     | seahelm_evaluation   | Evaluation for task 'QA' completed!
                                                      | 
2025-08-29 11:52:12 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: TRANSLATION-EN-XX ----------
                                                      | Testing Competency: NLG
2025-08-29 11:52:12 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/en_to_tam_Taml.jsonl
2025-08-29 11:52:12 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-EN-XX' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:12 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 191.97 examples/s]
[92m11:52:12 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:12 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:14 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:14 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 190.24 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 199.60 examples/s]
2025-08-29 11:52:14 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_ta.jsonl
2025-08-29 11:52:14 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:14 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 11:52:14 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: TRANSLATION-XX-EN ----------
                                                      | Testing Competency: NLG
2025-08-29 11:52:14 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/tam_Taml_to_en.jsonl
2025-08-29 11:52:14 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-XX-EN' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:14 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 193.27 examples/s]
[92m11:52:14 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:14 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:15 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:15 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 188.47 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 192.81 examples/s]
2025-08-29 11:52:15 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_ta.jsonl
2025-08-29 11:52:15 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:15 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 11:52:15 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: ABSSUM ----------
                                                      | Testing Competency: NLG
2025-08-29 11:52:15 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/abstractive_summarization/data/ta_xlsum_100sample.jsonl
2025-08-29 11:52:15 | INFO     | seahelm_evaluation   | Performing inference for task 'ABSSUM' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:15 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 162.68 examples/s]
[92m11:52:15 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:15 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:16 | INFO     | openai_serving       | Still waiting (50s has elapsed)...
[92m11:52:16 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:16 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:16 | INFO     | openai_serving       | Still waiting (50s has elapsed)...
[92m11:52:16 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:16 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:16 | INFO     | openai_serving       | Still waiting (50s has elapsed)...
[92m11:52:16 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:16 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m11:52:18 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:18 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 171.22 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 177.96 examples/s]
2025-08-29 11:52:18 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_ta.jsonl
2025-08-29 11:52:18 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:18 | INFO     | seahelm_evaluation   | Inference for task 'ABSSUM' completed!
                                                      | 
2025-08-29 11:52:18 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: ABSSUM ----------
2025-08-29 11:52:18 | INFO     | seahelm_evaluation   | Evaluating 'ABSSUM' using SummarizationMetric
2025-08-29 11:52:18 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:18 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:18 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:18 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:52:18 | INFO     | summarization        | Rouge-L Scores:
2025-08-29 11:52:18 | INFO     | summarization        | Precision: 0.00 | Recall: 0.00 | F1: 0.00
2025-08-29 11:52:18 | INFO     | summarization        | Norm F1 Score: 0.00
2025-08-29 11:52:18 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:18 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_ta.jsonl
2025-08-29 11:52:18 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:18 | INFO     | seahelm_evaluation   | Evaluation for task 'ABSSUM' completed!
                                                      | 
2025-08-29 11:52:18 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: CAUSAL ----------
                                                      | Testing Competency: NLR
2025-08-29 11:52:18 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/causal/data/ta_xcopa.jsonl
2025-08-29 11:52:18 | INFO     | openai_serving       | Still waiting (50s has elapsed)...
[92m11:52:18 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:18 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:18 | INFO     | seahelm_evaluation   | Performing inference for task 'CAUSAL' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:18 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 167.75 examples/s]
[92m11:52:19 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:19 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:19 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:19 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 164.08 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 171.36 examples/s]
2025-08-29 11:52:19 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_causal_ta.jsonl
2025-08-29 11:52:19 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:19 | INFO     | seahelm_evaluation   | Inference for task 'CAUSAL' completed!
                                                      | 
2025-08-29 11:52:19 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: CAUSAL ----------
2025-08-29 11:52:19 | INFO     | seahelm_evaluation   | Evaluating 'CAUSAL' using CausalReasoningMetric
2025-08-29 11:52:19 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:19 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:19 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:19 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:52:19 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:52:19 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:52:19 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       1.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       1.0
                                                      |    macro avg       0.00      0.00      0.00       1.0
                                                      | weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:52:19 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:19 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_causal_ta.jsonl
2025-08-29 11:52:19 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:19 | INFO     | seahelm_evaluation   | Evaluation for task 'CAUSAL' completed!
                                                      | 
2025-08-29 11:52:19 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: NLI ----------
                                                      | Testing Competency: NLR
2025-08-29 11:52:19 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/nli/data/ta_xnli_1000sample.jsonl
2025-08-29 11:52:19 | INFO     | seahelm_evaluation   | Performing inference for task 'NLI' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:19 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 180.41 examples/s]
[92m11:52:19 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:19 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:20 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:20 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 182.04 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 189.94 examples/s]
2025-08-29 11:52:20 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_nli_ta.jsonl
2025-08-29 11:52:20 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:20 | INFO     | seahelm_evaluation   | Inference for task 'NLI' completed!
                                                      | 
2025-08-29 11:52:20 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: NLI ----------
2025-08-29 11:52:20 | INFO     | seahelm_evaluation   | Evaluating 'NLI' using NLIMetric
2025-08-29 11:52:20 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:20 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:20 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:20 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:52:20 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:52:20 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:52:20 | INFO     | seahelm_metric       | Classification report:
                                                      |                precision    recall  f1-score   support
                                                      | 
                                                      | contradiction       0.00      0.00      0.00       1.0
                                                      |          none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |      accuracy                           0.00       1.0
                                                      |     macro avg       0.00      0.00      0.00       1.0
                                                      |  weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:52:20 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:20 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_nli_ta.jsonl
2025-08-29 11:52:20 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:20 | INFO     | seahelm_evaluation   | Evaluation for task 'NLI' completed!
                                                      | 
2025-08-29 11:52:20 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: MP-R ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 11:52:20 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/syntax/data/ta_syntax_mcq_randomized.jsonl
2025-08-29 11:52:20 | INFO     | seahelm_evaluation   | Performing inference for task 'MP-R' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:20 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 154.07 examples/s]
[92m11:52:21 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:21 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:21 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:21 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 150.97 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 157.51 examples/s]
2025-08-29 11:52:21 | INFO     | seahelm_evaluation   | Saving inference results for task 'MP-R' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_mp-r_ta.jsonl
2025-08-29 11:52:21 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:21 | INFO     | seahelm_evaluation   | Inference for task 'MP-R' completed!
                                                      | 
2025-08-29 11:52:21 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: MP-R ----------
2025-08-29 11:52:21 | INFO     | seahelm_evaluation   | Evaluating 'MP-R' using MinimalPairsMetric
2025-08-29 11:52:21 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:21 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:21 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:21 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:52:21 | INFO     | minimal_pairs        | Accuracy for phenomenon <argument_structure>: 0.0
2025-08-29 11:52:21 | INFO     | minimal_pairs        | Overall Accuracy: 0.0
2025-08-29 11:52:21 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:21 | INFO     | seahelm_evaluation   | Saving inference results for task 'MP-R' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_mp-r_ta.jsonl
2025-08-29 11:52:21 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:21 | INFO     | seahelm_evaluation   | Evaluation for task 'MP-R' completed!
                                                      | 
2025-08-29 11:52:21 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: PRAGMATIC-SINGLE ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 11:52:21 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/pragmatics/data/ta_pragmatic_reasoning_single.jsonl
2025-08-29 11:52:21 | INFO     | seahelm_evaluation   | Performing inference for task 'PRAGMATIC-SINGLE' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:21 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 148.39 examples/s]
[92m11:52:22 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:22 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:22 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:22 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 145.78 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 150.36 examples/s]
2025-08-29 11:52:22 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-SINGLE' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-single_ta.jsonl
2025-08-29 11:52:22 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:22 | INFO     | seahelm_evaluation   | Inference for task 'PRAGMATIC-SINGLE' completed!
                                                      | 
2025-08-29 11:52:22 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: PRAGMATIC-SINGLE ----------
2025-08-29 11:52:22 | INFO     | seahelm_evaluation   | Evaluating 'PRAGMATIC-SINGLE' using PragmaticReasoningSingleSentenceMetric
2025-08-29 11:52:22 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:22 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:22 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:22 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  subset_references = subset[self.label_column].replace(
2025-08-29 11:52:22 | INFO     | pragmatic_reasoning  | Accuracy for phenomenon <scalar_implicatures>: 0.0 / 1
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:93: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  references = self.inference_df[self.label_column].replace(
2025-08-29 11:52:22 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:22 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-SINGLE' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-single_ta.jsonl
2025-08-29 11:52:22 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:22 | INFO     | seahelm_evaluation   | Evaluation for task 'PRAGMATIC-SINGLE' completed!
                                                      | 
2025-08-29 11:52:22 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: PRAGMATIC-PAIR ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 11:52:22 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/pragmatics/data/ta_pragmatic_reasoning_pair.jsonl
2025-08-29 11:52:22 | INFO     | seahelm_evaluation   | Performing inference for task 'PRAGMATIC-PAIR' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:22 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 160.34 examples/s]
[92m11:52:23 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:23 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:23 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:23 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 157.11 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 171.70 examples/s]
2025-08-29 11:52:23 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-PAIR' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-pair_ta.jsonl
2025-08-29 11:52:23 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:23 | INFO     | seahelm_evaluation   | Inference for task 'PRAGMATIC-PAIR' completed!
                                                      | 
2025-08-29 11:52:23 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: PRAGMATIC-PAIR ----------
2025-08-29 11:52:23 | INFO     | seahelm_evaluation   | Evaluating 'PRAGMATIC-PAIR' using PragmaticReasoningSentencePairMetric
2025-08-29 11:52:23 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:23 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:23 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:23 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:143: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  subset_references = subset[self.label_column].replace({True: 1, False: 0})
2025-08-29 11:52:23 | INFO     | pragmatic_reasoning  | Accuracy for phenomenon <scalar_implicatures>: 0.0 / 1
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:157: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  references = self.inference_df[self.label_column].replace({True: 1, False: 0})
2025-08-29 11:52:23 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:23 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-PAIR' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-pair_ta.jsonl
2025-08-29 11:52:23 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:23 | INFO     | seahelm_evaluation   | Evaluation for task 'PRAGMATIC-PAIR' completed!
                                                      | 
2025-08-29 11:52:23 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: SENTIMENT ----------
                                                      | Testing Competency: NLU
2025-08-29 11:52:23 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/sentiment_analysis/data/tl_elections_sentiment.jsonl
2025-08-29 11:52:23 | INFO     | seahelm_evaluation   | Performing inference for task 'SENTIMENT' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:23 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 174.18 examples/s]
[92m11:52:24 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:24 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:24 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:24 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 181.90 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 190.81 examples/s]
2025-08-29 11:52:24 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_tl.jsonl
2025-08-29 11:52:24 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:24 | INFO     | seahelm_evaluation   | Inference for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 11:52:24 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: SENTIMENT ----------
2025-08-29 11:52:24 | INFO     | seahelm_evaluation   | Evaluating 'SENTIMENT' using SentimentAnalysisMetric
2025-08-29 11:52:24 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:24 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:24 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:24 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:52:24 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:52:24 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:52:24 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |      neutral       0.00      0.00      0.00       1.0
                                                      |         none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       1.0
                                                      |    macro avg       0.00      0.00      0.00       1.0
                                                      | weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:52:24 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:24 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_tl.jsonl
2025-08-29 11:52:24 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:24 | INFO     | seahelm_evaluation   | Evaluation for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 11:52:24 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: BELEBELE-QA-MC ----------
                                                      | Testing Competency: NLU
2025-08-29 11:52:24 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/belebele_mcqa/data/eval/tl_belebele.jsonl
2025-08-29 11:52:24 | INFO     | seahelm_evaluation   | Performing inference for task 'BELEBELE-QA-MC' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:24 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 152.84 examples/s]
[92m11:52:24 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:24 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:25 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:25 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 164.86 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 160.92 examples/s]
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Saving inference results for task 'BELEBELE-QA-MC' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_belebele-qa-mc_tl.jsonl
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Inference for task 'BELEBELE-QA-MC' completed!
                                                      | 
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: BELEBELE-QA-MC ----------
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Evaluating 'BELEBELE-QA-MC' using QuestionAnsweringMultipleChoiceMetric
2025-08-29 11:52:25 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:25 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:25 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:25 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:52:25 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:52:25 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:52:25 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            1       0.00      0.00      0.00       1.0
                                                      |            4       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       1.0
                                                      |    macro avg       0.00      0.00      0.00       1.0
                                                      | weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:52:25 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Saving inference results for task 'BELEBELE-QA-MC' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_belebele-qa-mc_tl.jsonl
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Evaluation for task 'BELEBELE-QA-MC' completed!
                                                      | 
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: TOXICITY ----------
                                                      | Testing Competency: SAFETY
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/safety/toxicity_detection/data/tl_elections_hsd.jsonl
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Performing inference for task 'TOXICITY' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:25 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 181.33 examples/s]
[92m11:52:25 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:25 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:25 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:25 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 188.21 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 189.93 examples/s]
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_tl.jsonl
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Inference for task 'TOXICITY' completed!
                                                      | 
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: TOXICITY ----------
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Evaluating 'TOXICITY' using ToxicityDetectionMetric
2025-08-29 11:52:25 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:25 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:25 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:25 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:52:25 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:52:25 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:52:25 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       1.0
                                                      |            3       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       1.0
                                                      |    macro avg       0.00      0.00      0.00       1.0
                                                      | weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:52:25 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_tl.jsonl
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Evaluation for task 'TOXICITY' completed!
                                                      | 
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: TRANSLATION-EN-XX ----------
                                                      | Testing Competency: NLG
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/en_to_tgl_Latn.jsonl
2025-08-29 11:52:25 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-EN-XX' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:25 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 184.15 examples/s]
[92m11:52:25 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:25 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:26 | INFO     | openai_serving       | Still waiting (60s has elapsed)...
[92m11:52:26 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:26 | INFO     | openai_serving       | Still waiting (60s has elapsed)...
2025-08-29 11:52:26 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m11:52:26 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:26 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:26 | INFO     | openai_serving       | Still waiting (60s has elapsed)...
[92m11:52:26 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:26 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m11:52:27 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:27 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 180.76 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 186.99 examples/s]
2025-08-29 11:52:27 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_tl.jsonl
2025-08-29 11:52:27 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:27 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 11:52:27 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: TRANSLATION-XX-EN ----------
                                                      | Testing Competency: NLG
2025-08-29 11:52:27 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/tgl_Latn_to_en.jsonl
2025-08-29 11:52:27 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-XX-EN' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:27 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 189.73 examples/s]
[92m11:52:27 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:27 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:28 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:28 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 181.40 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 185.70 examples/s]
2025-08-29 11:52:28 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_tl.jsonl
2025-08-29 11:52:28 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:28 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 11:52:28 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: ABSSUM ----------
                                                      | Testing Competency: NLG
2025-08-29 11:52:28 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/abstractive_summarization/data/tl_xlsum.jsonl
2025-08-29 11:52:28 | INFO     | seahelm_evaluation   | Performing inference for task 'ABSSUM' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:28 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 169.67 examples/s]
[92m11:52:28 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:28 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:29 | INFO     | openai_serving       | Still waiting (60s has elapsed)...
[92m11:52:29 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:29 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m11:52:31 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:31 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 170.83 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 179.47 examples/s]
2025-08-29 11:52:31 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_tl.jsonl
2025-08-29 11:52:31 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:31 | INFO     | seahelm_evaluation   | Inference for task 'ABSSUM' completed!
                                                      | 
2025-08-29 11:52:31 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: ABSSUM ----------
2025-08-29 11:52:31 | INFO     | seahelm_evaluation   | Evaluating 'ABSSUM' using SummarizationMetric
2025-08-29 11:52:31 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:31 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:31 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:31 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:52:31 | INFO     | summarization        | Rouge-L Scores:
2025-08-29 11:52:31 | INFO     | summarization        | Precision: 0.00 | Recall: 0.00 | F1: 0.00
2025-08-29 11:52:31 | INFO     | summarization        | Norm F1 Score: 0.00
2025-08-29 11:52:31 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:31 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_tl.jsonl
2025-08-29 11:52:31 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:31 | INFO     | seahelm_evaluation   | Evaluation for task 'ABSSUM' completed!
                                                      | 
2025-08-29 11:52:31 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: CAUSAL ----------
                                                      | Testing Competency: NLR
2025-08-29 11:52:31 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/causal/data/tl_balanced_copa.jsonl
2025-08-29 11:52:31 | INFO     | seahelm_evaluation   | Performing inference for task 'CAUSAL' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:31 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 159.87 examples/s]
[92m11:52:31 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:31 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:32 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:32 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 164.88 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 167.73 examples/s]
2025-08-29 11:52:32 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_causal_tl.jsonl
2025-08-29 11:52:32 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:32 | INFO     | seahelm_evaluation   | Inference for task 'CAUSAL' completed!
                                                      | 
2025-08-29 11:52:32 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: CAUSAL ----------
2025-08-29 11:52:32 | INFO     | seahelm_evaluation   | Evaluating 'CAUSAL' using CausalReasoningMetric
2025-08-29 11:52:32 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:32 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:32 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:32 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:52:32 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:52:32 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:52:32 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       1.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       1.0
                                                      |    macro avg       0.00      0.00      0.00       1.0
                                                      | weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:52:32 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:32 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_causal_tl.jsonl
2025-08-29 11:52:32 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:32 | INFO     | seahelm_evaluation   | Evaluation for task 'CAUSAL' completed!
                                                      | 
2025-08-29 11:52:32 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: NLI ----------
                                                      | Testing Competency: NLR
2025-08-29 11:52:32 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/nli/data/tl_xnli.jsonl
2025-08-29 11:52:32 | INFO     | seahelm_evaluation   | Performing inference for task 'NLI' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:32 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 176.28 examples/s]
[92m11:52:32 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:32 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m11:52:33 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:33 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 180.30 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 187.93 examples/s]
2025-08-29 11:52:33 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_nli_tl.jsonl
2025-08-29 11:52:33 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:33 | INFO     | seahelm_evaluation   | Inference for task 'NLI' completed!
                                                      | 
2025-08-29 11:52:33 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: NLI ----------
2025-08-29 11:52:33 | INFO     | seahelm_evaluation   | Evaluating 'NLI' using NLIMetric
2025-08-29 11:52:33 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:33 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:33 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:33 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:52:33 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:52:33 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:52:33 | INFO     | seahelm_metric       | Classification report:
                                                      |                precision    recall  f1-score   support
                                                      | 
                                                      | contradiction       0.00      0.00      0.00       1.0
                                                      |          none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |      accuracy                           0.00       1.0
                                                      |     macro avg       0.00      0.00      0.00       1.0
                                                      |  weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:52:33 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:33 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_nli_tl.jsonl
2025-08-29 11:52:33 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:33 | INFO     | seahelm_evaluation   | Evaluation for task 'NLI' completed!
                                                      | 
2025-08-29 11:52:33 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: IF-EVAL ----------
                                                      | Testing Competency: INSTRUCTION-FOLLOWING
2025-08-29 11:52:33 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/instruction_following/ifeval/data/tl_sea_ifeval.jsonl
2025-08-29 11:52:33 | INFO     | seahelm_evaluation   | Performing inference for task 'IF-EVAL' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:33 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 103.57 examples/s]
[92m11:52:33 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:33 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:36 | INFO     | openai_serving       | Still waiting (70s has elapsed)...
[92m11:52:36 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:36 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:36 | INFO     | openai_serving       | Still waiting (70s has elapsed)...
[92m11:52:36 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:36 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:37 | INFO     | openai_serving       | Still waiting (70s has elapsed)...
[92m11:52:37 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:37 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m11:52:38 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:38 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 102.76 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 108.04 examples/s]
2025-08-29 11:52:38 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_tl.jsonl
2025-08-29 11:52:38 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:38 | INFO     | seahelm_evaluation   | Inference for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 11:52:38 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: IF-EVAL ----------
2025-08-29 11:52:38 | INFO     | seahelm_evaluation   | Evaluating 'IF-EVAL' using IFEvalMetric
2025-08-29 11:52:38 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:38 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:38 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:38 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:52:38 | ERROR    | seahelm_evaluation   | Failed to run evaluation for task if-eval and lang tl
2025-08-29 11:52:38 | ERROR    | seahelm_evaluation   | True
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 3812, in get_loc
                                                      |     return self._engine.get_loc(casted_key)
                                                      |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "pandas/_libs/index.pyx", line 167, in pandas._libs.index.IndexEngine.get_loc
                                                      |   File "pandas/_libs/index.pyx", line 196, in pandas._libs.index.IndexEngine.get_loc
                                                      |   File "pandas/_libs/hashtable_class_helper.pxi", line 5846, in pandas._libs.hashtable.UInt8HashTable.get_item
                                                      |   File "pandas/_libs/hashtable_class_helper.pxi", line 5870, in pandas._libs.hashtable.UInt8HashTable.get_item
                                                      | KeyError: 1
                                                      | 
                                                      | The above exception was the direct cause of the following exception:
                                                      | 
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_evaluation.py", line 677, in run_single_task_evaluation
                                                      |     metric_json, inference_df = evaluation_metric.evaluate_responses()
                                                      |                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/seahelm_metric.py", line 118, in evaluate_responses
                                                      |     output_json, inference_df = self.calculate_metrics()
                                                      |                                 ^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/instruction_following/ifeval/if_eval.py", line 72, in calculate_metrics
                                                      |     metric_dict = self.summarize_results(self.inference_df)
                                                      |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/instruction_following/ifeval/if_eval.py", line 144, in summarize_results
                                                      |     overall_pass = int(inference_df["result"].value_counts()[True])
                                                      |                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/series.py", line 1130, in __getitem__
                                                      |     return self._get_value(key)
                                                      |            ^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/series.py", line 1246, in _get_value
                                                      |     loc = self.index.get_loc(label)
                                                      |           ^^^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 3819, in get_loc
                                                      |     raise KeyError(key) from err
                                                      | KeyError: True
2025-08-29 11:52:38 | WARNING  | seahelm_evaluation   | Setting metric overall_lang_normalized_acc to 0 for task if-eval
2025-08-29 11:52:38 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: KALAHI-MC ----------
                                                      | Testing Competency: CULTURAL
2025-08-29 11:52:38 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/cultural/kalahi/data/tl_kalahi_mc.jsonl
2025-08-29 11:52:38 | INFO     | seahelm_evaluation   | Performing inference for task 'KALAHI-MC' with 0 examples
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
2025-08-29 11:52:38 | WARNING  | arrow_dataset        | num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 160.33 examples/s]
[92m11:52:39 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:39 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 11:52:39 | INFO     | openai_serving       | Still waiting (70s has elapsed)...
[92m11:52:39 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:39 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m11:52:39 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 11:52:39 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 166.28 examples/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 170.86 examples/s]
2025-08-29 11:52:39 | INFO     | seahelm_evaluation   | Saving inference results for task 'KALAHI-MC' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_kalahi-mc_tl.jsonl
2025-08-29 11:52:39 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:39 | INFO     | seahelm_evaluation   | Inference for task 'KALAHI-MC' completed!
                                                      | 
2025-08-29 11:52:39 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: KALAHI-MC ----------
2025-08-29 11:52:39 | INFO     | seahelm_evaluation   | Evaluating 'KALAHI-MC' using KalahiMCMetric
2025-08-29 11:52:39 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:39 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:39 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:39 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 11:52:39 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 11:52:39 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1]
                                                      |  [0 0]]
2025-08-29 11:52:39 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            B       0.00      0.00      0.00       1.0
                                                      |         none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       1.0
                                                      |    macro avg       0.00      0.00      0.00       1.0
                                                      | weighted avg       0.00      0.00      0.00       1.0
                                                      | 
2025-08-29 11:52:39 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:39 | INFO     | seahelm_evaluation   | Saving inference results for task 'KALAHI-MC' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_kalahi-mc_tl.jsonl
2025-08-29 11:52:39 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:52:39 | INFO     | seahelm_evaluation   | Evaluation for task 'KALAHI-MC' completed!
                                                      | 
2025-08-29 11:52:39 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: TRANSLATION-EN-XX ----------
2025-08-29 11:52:39 | WARNING  | translation          | COMET not installed. Please install COMET to use the COMET metrics.
2025-08-29 11:52:39 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-EN-XX' using TranslationMetric
2025-08-29 11:52:39 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:52:39 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:52:39 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:52:39 | INFO     | seahelm_metric       | Calculating metrics...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:02,  1.09s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:02<00:01,  1.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.16it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.09it/s]
2025-08-29 11:52:47 | INFO     | openai_serving       | Still waiting (80s has elapsed)...
2025-08-29 11:52:47 | INFO     | openai_serving       | OpenAI batch is completed
[92m11:52:47 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:47 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:47 | INFO     | openai_serving       | Still waiting (80s has elapsed)...
[92m11:52:47 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:47 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:47 | INFO     | openai_serving       | Still waiting (80s has elapsed)...
[92m11:52:47 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:47 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m11:52:47 - LiteLLM:ERROR[0m: logging_worker.py:61 - LoggingWorker cancelled: 
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
    coroutine = await self._queue.get()
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
    await getter
asyncio.exceptions.CancelledError
2025-08-29 11:52:47 | ERROR    | logging_worker       | LoggingWorker cancelled: 
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
                                                      |     coroutine = await self._queue.get()
                                                      |                 ^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
                                                      |     await getter
                                                      | asyncio.exceptions.CancelledError
2025-08-29 11:52:47 | INFO     | mt_bench             | All judgments have been obtained.
2025-08-29 11:52:47 | INFO     | mt_bench             | Successfully obtained all judgments.
                                                      | 
2025-08-29 11:52:47 | INFO     | mt_bench             | Win rate for category <writing>: 0.5
2025-08-29 11:52:47 | INFO     | mt_bench             | Overall win rate: 0.5
2025-08-29 11:52:47 | INFO     | mt_bench             | Weighted win rate: 0.5
2025-08-29 11:52:47 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:52:49 | INFO     | openai_serving       | Still waiting (80s has elapsed)...
[92m11:52:49 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:49 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:57 | INFO     | openai_serving       | Still waiting (90s has elapsed)...
[92m11:52:57 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:57 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:57 | INFO     | openai_serving       | Still waiting (90s has elapsed)...
[92m11:52:57 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:57 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:59 | INFO     | openai_serving       | Still waiting (90s has elapsed)...
[92m11:52:59 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:52:59 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:07 | INFO     | openai_serving       | Still waiting (100s has elapsed)...
[92m11:53:07 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:07 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:07 | INFO     | openai_serving       | Still waiting (100s has elapsed)...
[92m11:53:07 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:07 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:09 | INFO     | openai_serving       | Still waiting (100s has elapsed)...
[92m11:53:09 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:09 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:17 | INFO     | openai_serving       | Still waiting (110s has elapsed)...
[92m11:53:17 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:17 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:17 | INFO     | openai_serving       | Still waiting (110s has elapsed)...
[92m11:53:17 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:17 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:20 | INFO     | openai_serving       | Still waiting (110s has elapsed)...
[92m11:53:20 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:20 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:28 | INFO     | openai_serving       | Still waiting (120s has elapsed)...
[92m11:53:28 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:28 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:28 | INFO     | openai_serving       | Still waiting (120s has elapsed)...
[92m11:53:28 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:28 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:30 | INFO     | openai_serving       | Still waiting (120s has elapsed)...
[92m11:53:30 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:30 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:38 | INFO     | openai_serving       | Still waiting (130s has elapsed)...
[92m11:53:38 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:38 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:38 | INFO     | openai_serving       | Still waiting (130s has elapsed)...
[92m11:53:38 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:38 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:40 | INFO     | openai_serving       | Still waiting (130s has elapsed)...
[92m11:53:40 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:40 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
2025-08-29 11:53:47 | INFO     | translation          | MetricX WMT24 score: 19.000000
2025-08-29 11:53:47 | INFO     | translation          | MetricX WMT24 with references score: 22.375000
2025-08-29 11:53:47 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:53:47 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_id.jsonl
2025-08-29 11:53:47 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:53:47 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 11:53:47 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: TRANSLATION-XX-EN ----------
2025-08-29 11:53:47 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-XX-EN' using TranslationMetric
2025-08-29 11:53:47 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:53:47 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:53:47 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:53:47 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:53:48 | INFO     | openai_serving       | Still waiting (140s has elapsed)...
[92m11:53:48 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:48 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:48 | INFO     | openai_serving       | Still waiting (140s has elapsed)...
[92m11:53:48 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:48 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:50 | INFO     | openai_serving       | Still waiting (140s has elapsed)...
[92m11:53:50 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:50 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.90it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.86it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.95it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.93it/s]
2025-08-29 11:53:58 | INFO     | openai_serving       | Still waiting (150s has elapsed)...
[92m11:53:58 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:58 | INFO     | openai_serving       | Still waiting (150s has elapsed)...
2025-08-29 11:53:58 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m11:53:58 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:53:58 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:00 | INFO     | translation          | MetricX WMT24 score: 15.375000
2025-08-29 11:54:00 | INFO     | translation          | MetricX WMT24 with references score: 25.000000
2025-08-29 11:54:00 | INFO     | openai_serving       | Still waiting (150s has elapsed)...
[92m11:54:00 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:00 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:00 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:54:00 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_id.jsonl
2025-08-29 11:54:00 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:54:00 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 11:54:00 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: TRANSLATION-EN-XX ----------
2025-08-29 11:54:00 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-EN-XX' using TranslationMetric
2025-08-29 11:54:00 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:54:00 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:54:00 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:54:00 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.88it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.84it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.95it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.92it/s]
2025-08-29 11:54:08 | INFO     | openai_serving       | Still waiting (160s has elapsed)...
[92m11:54:08 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:08 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:08 | INFO     | openai_serving       | Still waiting (160s has elapsed)...
[92m11:54:08 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:08 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:11 | INFO     | openai_serving       | Still waiting (160s has elapsed)...
[92m11:54:11 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:11 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:12 | INFO     | translation          | MetricX WMT24 score: 20.750000
2025-08-29 11:54:12 | INFO     | translation          | MetricX WMT24 with references score: 22.375000
2025-08-29 11:54:12 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:54:12 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_vi.jsonl
2025-08-29 11:54:12 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:54:12 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 11:54:12 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: TRANSLATION-XX-EN ----------
2025-08-29 11:54:12 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-XX-EN' using TranslationMetric
2025-08-29 11:54:12 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:54:12 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:54:12 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:54:12 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.94it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.87it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.96it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.94it/s]
2025-08-29 11:54:19 | INFO     | openai_serving       | Still waiting (170s has elapsed)...
[92m11:54:19 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:19 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:19 | INFO     | openai_serving       | Still waiting (170s has elapsed)...
[92m11:54:19 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:19 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:21 | INFO     | openai_serving       | Still waiting (170s has elapsed)...
[92m11:54:21 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:21 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:24 | INFO     | translation          | MetricX WMT24 score: 16.375000
2025-08-29 11:54:24 | INFO     | translation          | MetricX WMT24 with references score: 25.000000
2025-08-29 11:54:24 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:54:24 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_vi.jsonl
2025-08-29 11:54:24 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:54:24 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 11:54:24 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: TRANSLATION-EN-XX ----------
2025-08-29 11:54:24 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-EN-XX' using TranslationMetric
2025-08-29 11:54:24 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:54:24 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:54:24 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:54:24 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.91it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.89it/s]2025-08-29 11:54:29 | INFO     | openai_serving       | Still waiting (180s has elapsed)...
[92m11:54:29 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:29 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:29 | INFO     | openai_serving       | Still waiting (180s has elapsed)...
[92m11:54:29 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:29 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.97it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.95it/s]
2025-08-29 11:54:31 | INFO     | openai_serving       | Still waiting (180s has elapsed)...
[92m11:54:31 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:31 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:36 | INFO     | translation          | MetricX WMT24 score: 19.125000
2025-08-29 11:54:36 | INFO     | translation          | MetricX WMT24 with references score: 22.375000
2025-08-29 11:54:36 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:54:36 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_th.jsonl
2025-08-29 11:54:36 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:54:36 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 11:54:36 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: TRANSLATION-XX-EN ----------
2025-08-29 11:54:36 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-XX-EN' using TranslationMetric
2025-08-29 11:54:36 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:54:36 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:54:36 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:54:36 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:54:39 | INFO     | openai_serving       | Still waiting (190s has elapsed)...
[92m11:54:39 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:39 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:39 | INFO     | openai_serving       | Still waiting (190s has elapsed)...
[92m11:54:39 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:39 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.92it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.85it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.94it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.92it/s]
2025-08-29 11:54:41 | INFO     | openai_serving       | Still waiting (190s has elapsed)...
[92m11:54:41 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:41 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:48 | INFO     | translation          | MetricX WMT24 score: 8.250000
2025-08-29 11:54:48 | INFO     | translation          | MetricX WMT24 with references score: 23.625000
2025-08-29 11:54:48 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:54:48 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_th.jsonl
2025-08-29 11:54:48 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:54:48 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 11:54:48 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: TRANSLATION-EN-XX ----------
2025-08-29 11:54:48 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-EN-XX' using TranslationMetric
2025-08-29 11:54:48 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:54:48 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:54:48 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:54:48 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 11:54:49 | INFO     | openai_serving       | Still waiting (200s has elapsed)...
[92m11:54:49 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:49 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:49 | INFO     | openai_serving       | Still waiting (200s has elapsed)...
[92m11:54:49 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:49 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-08-29 11:54:51 | INFO     | openai_serving       | Still waiting (200s has elapsed)...
2025-08-29 11:54:51 | INFO     | openai_serving       | OpenAI batch is completed
[92m11:54:51 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:51 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.92it/s][92m11:54:52 - LiteLLM:ERROR[0m: logging_worker.py:61 - LoggingWorker cancelled: 
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
    coroutine = await self._queue.get()
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
    await getter
asyncio.exceptions.CancelledError
2025-08-29 11:54:52 | ERROR    | logging_worker       | LoggingWorker cancelled: 
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
                                                      |     coroutine = await self._queue.get()
                                                      |                 ^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
                                                      |     await getter
                                                      | asyncio.exceptions.CancelledError
2025-08-29 11:54:52 | INFO     | mt_bench             | All judgments have been obtained.
2025-08-29 11:54:52 | INFO     | mt_bench             | Successfully obtained all judgments.
                                                      | 
2025-08-29 11:54:52 | INFO     | mt_bench             | Win rate for category <writing>: 0.5
2025-08-29 11:54:52 | INFO     | mt_bench             | Overall win rate: 0.5
2025-08-29 11:54:52 | INFO     | mt_bench             | Weighted win rate: 0.5
2025-08-29 11:54:52 | INFO     | seahelm_metric       | Metrics calculation completed!
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.86it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.95it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.93it/s]
2025-08-29 11:54:59 | INFO     | openai_serving       | Still waiting (210s has elapsed)...
2025-08-29 11:54:59 | INFO     | openai_serving       | OpenAI batch is completed
[92m11:54:59 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:59 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:59 | INFO     | openai_serving       | Still waiting (210s has elapsed)...
[92m11:54:59 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:54:59 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m11:55:00 - LiteLLM:ERROR[0m: logging_worker.py:61 - LoggingWorker cancelled: 
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
    coroutine = await self._queue.get()
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
    await getter
asyncio.exceptions.CancelledError
2025-08-29 11:55:00 | ERROR    | logging_worker       | LoggingWorker cancelled: 
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
                                                      |     coroutine = await self._queue.get()
                                                      |                 ^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
                                                      |     await getter
                                                      | asyncio.exceptions.CancelledError
2025-08-29 11:55:00 | INFO     | mt_bench             | All judgments have been obtained.
2025-08-29 11:55:00 | INFO     | mt_bench             | Successfully obtained all judgments.
                                                      | 
2025-08-29 11:55:00 | INFO     | mt_bench             | Win rate for category <writing>: 0.5
2025-08-29 11:55:00 | INFO     | mt_bench             | Overall win rate: 0.5
2025-08-29 11:55:00 | INFO     | mt_bench             | Weighted win rate: 0.5
2025-08-29 11:55:00 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:55:00 | INFO     | translation          | MetricX WMT24 score: 20.250000
2025-08-29 11:55:00 | INFO     | translation          | MetricX WMT24 with references score: 22.375000
2025-08-29 11:55:00 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:55:00 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_ta.jsonl
2025-08-29 11:55:00 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:55:00 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 11:55:00 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: TRANSLATION-XX-EN ----------
2025-08-29 11:55:00 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-XX-EN' using TranslationMetric
2025-08-29 11:55:00 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:55:00 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:55:00 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:55:00 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.89it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.87it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.97it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.94it/s]
2025-08-29 11:55:10 | INFO     | openai_serving       | Still waiting (220s has elapsed)...
[92m11:55:10 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:55:10 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:55:12 | INFO     | translation          | MetricX WMT24 score: 16.125000
2025-08-29 11:55:12 | INFO     | translation          | MetricX WMT24 with references score: 25.000000
2025-08-29 11:55:12 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:55:12 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_ta.jsonl
2025-08-29 11:55:12 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:55:12 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 11:55:12 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: TRANSLATION-EN-XX ----------
2025-08-29 11:55:12 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-EN-XX' using TranslationMetric
2025-08-29 11:55:12 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:55:12 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:55:12 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:55:12 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.90it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.88it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.97it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.95it/s]
2025-08-29 11:55:20 | INFO     | openai_serving       | Still waiting (230s has elapsed)...
2025-08-29 11:55:20 | INFO     | openai_serving       | OpenAI batch is completed
[92m11:55:20 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 11:55:20 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m11:55:20 - LiteLLM:ERROR[0m: logging_worker.py:61 - LoggingWorker cancelled: 
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
    coroutine = await self._queue.get()
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
    await getter
asyncio.exceptions.CancelledError
2025-08-29 11:55:20 | ERROR    | logging_worker       | LoggingWorker cancelled: 
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
                                                      |     coroutine = await self._queue.get()
                                                      |                 ^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
                                                      |     await getter
                                                      | asyncio.exceptions.CancelledError
2025-08-29 11:55:20 | INFO     | mt_bench             | All judgments have been obtained.
2025-08-29 11:55:20 | INFO     | mt_bench             | Successfully obtained all judgments.
                                                      | 
2025-08-29 11:55:20 | INFO     | mt_bench             | Win rate for category <Knowledge III>: 0.75
2025-08-29 11:55:20 | INFO     | mt_bench             | Overall win rate: 0.75
2025-08-29 11:55:20 | INFO     | mt_bench             | Weighted win rate: 0.75
2025-08-29 11:55:20 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:55:24 | INFO     | translation          | MetricX WMT24 score: 15.250000
2025-08-29 11:55:24 | INFO     | translation          | MetricX WMT24 with references score: 24.000000
2025-08-29 11:55:24 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:55:24 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_tl.jsonl
2025-08-29 11:55:24 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:55:24 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 11:55:24 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: TRANSLATION-XX-EN ----------
2025-08-29 11:55:24 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-XX-EN' using TranslationMetric
2025-08-29 11:55:24 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 11:55:24 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 11:55:24 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 11:55:24 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.90it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.87it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.96it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.93it/s]
2025-08-29 11:55:36 | INFO     | translation          | MetricX WMT24 score: 15.375000
2025-08-29 11:55:36 | INFO     | translation          | MetricX WMT24 with references score: 25.000000
2025-08-29 11:55:36 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 11:55:36 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_tl.jsonl
2025-08-29 11:55:36 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:55:36 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 11:55:36 | INFO     | seahelm_evaluation   | Waiting for mt-bench evaluation to complete
2025-08-29 11:55:36 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_id.jsonl
2025-08-29 11:55:36 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:55:36 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_vi.jsonl
2025-08-29 11:55:36 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:55:36 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_th.jsonl
2025-08-29 11:55:36 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:55:36 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-11-50-26/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_tl.jsonl
2025-08-29 11:55:36 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 11:55:36 | INFO     | aggregate_metrics    | ---------- Aggregation of metrics ----------
2025-08-29 11:55:36 | INFO     | aggregate_metrics    | ---------- Aggregation | Lang: ID ----------
2025-08-29 11:55:36 | INFO     | aggregate_metrics    | ### Competency: NLU
2025-08-29 11:55:36 | INFO     | aggregate_metrics    | Overall normalized accuracy for <id_nlu>: 13.333333
                                                      | 
2025-08-29 11:55:36 | INFO     | aggregate_metrics    | ### Competency: SAFETY
2025-08-29 11:55:36 | INFO     | aggregate_metrics    | Overall normalized accuracy for <id_safety>: 0.000000
                                                      | 
2025-08-29 11:55:36 | INFO     | aggregate_metrics    | ### Competency: NLG
2025-08-29 11:55:36 | INFO     | aggregate_metrics    | Overall normalized accuracy for <id_nlg>: 22.642544
                                                      | 
2025-08-29 11:55:36 | INFO     | aggregate_metrics    | ### Competency: NLR
2025-08-29 11:55:36 | INFO     | aggregate_metrics    | Overall normalized accuracy for <id_nlr>: 0.000000
                                                      | 
2025-08-29 11:55:36 | INFO     | aggregate_metrics    | ### Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 11:55:36 | INFO     | aggregate_metrics    | ---------- Task: PRAGMATICS (ID) ----------
2025-08-29 11:55:36 | INFO     | aggregate_metrics    | Accuracy for phenomenon <ID_scalar_implicatures>: 0 / 2 : 0.000000
Traceback (most recent call last):
  File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_evaluation.py", line 1059, in <module>
    seahelm_eval.run_evaluation(
  File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_evaluation.py", line 891, in run_evaluation
    metrics = aggregate_metrics(metrics, config=self.config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/aggregate_metrics.py", line 126, in aggregate_metrics
    metrics = aggregate_pragmatics_metrics(metrics, lang)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/aggregate_metrics.py", line 35, in aggregate_pragmatics_metrics
    subset_accuracy = correct_count / total_count
                      ~~~~~~~~~~~~~~^~~~~~~~~~~~~
ZeroDivisionError: division by zero

Execution finished at 08-29 11:55:38
