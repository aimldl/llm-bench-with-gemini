To view the log, run:
  $ tail -f output-vertex_ai/08-29-12-10-02/run-vertex_ai.out

python seahelm_evaluation.py --tasks seahelm --model_type litellm --output_dir output-vertex_ai/08-29-12-10-02 --model_name gemini-2.5-flash --model_args api_provider=vertex_ai --skip_tokenize_prompts --limit 3 
2025-08-29 12:10:11 | INFO     | seahelm_evaluation   | Loading model gemini-2.5-flash using VERTEX_AI...
2025-08-29 12:10:11 | INFO     | seahelm_evaluation   | ---------- Preparation of output folder ----------
                                                      | Preparing output folder ...
                                                      | Folder: output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference
2025-08-29 12:10:11 | INFO     | seahelm_evaluation   | Completed preparation of output folder!
                                                      | 
2025-08-29 12:10:11 | INFO     | seahelm_evaluation   | <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
                                                      | Evaluating gemini-2.5-flash as instruction-tuned model...
                                                      | <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
2025-08-29 12:10:13 | INFO     | seahelm_evaluation   | ---------- Configuration saving ----------
                                                      | Saving run config to output folder...
                                                      | Filepath: output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/gemini-2.5-flash_run_config_2025-08-29T12:10:11.975723.yaml
2025-08-29 12:10:13 | INFO     | seahelm_evaluation   | Config file saved!
                                                      | 
2025-08-29 12:10:13 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: MT-BENCH ----------
                                                      | Testing Competency: MULTI-TURN
2025-08-29 12:10:13 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/multi_turn/mt_bench/data/id_sea_mt_bench.jsonl
2025-08-29 12:10:13 | INFO     | seahelm_evaluation   | Performing inference for task 'MT-BENCH' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:10:13 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.30 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.92 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.85 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.17 examples/s]
[92m12:10:14 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:14 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:14 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:14 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:14 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:14 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:18 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:18 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:10:19 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:19 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:10:20 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:20 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 201.98 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 430.26 examples/s]
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:10:20 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.59 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.98 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.98 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.27 examples/s]
[92m12:10:20 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:20 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:20 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:20 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:20 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:20 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:25 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:25 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:10:26 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:26 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:10:26 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:26 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 313.50 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 418.68 examples/s]
2025-08-29 12:10:26 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_id.jsonl
2025-08-29 12:10:26 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:10:26 | INFO     | seahelm_evaluation   | Inference for task 'MT-BENCH' completed!
                                                      | 
2025-08-29 12:10:26 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: MT-BENCH ----------
                                                      | Testing Competency: MULTI-TURN
2025-08-29 12:10:26 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/multi_turn/mt_bench/data/vi_sea_mt_bench.jsonl
2025-08-29 12:10:26 | INFO     | seahelm_evaluation   | Performing inference for task 'MT-BENCH' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:10:26 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.61 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.67 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.29 examples/s]
[92m12:10:27 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:27 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:27 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:27 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:27 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:27 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:32 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:32 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:10:32 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:32 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:10:33 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:33 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 339.98 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 437.70 examples/s]
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:10:33 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.59 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.92 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.93 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.23 examples/s]
[92m12:10:33 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:33 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:33 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:33 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:33 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:33 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:38 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:38 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:10:39 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:39 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:10:39 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:39 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 304.72 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 421.31 examples/s]
2025-08-29 12:10:39 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_vi.jsonl
2025-08-29 12:10:39 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:10:39 | INFO     | seahelm_evaluation   | Inference for task 'MT-BENCH' completed!
                                                      | 
2025-08-29 12:10:39 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: MT-BENCH ----------
                                                      | Testing Competency: MULTI-TURN
2025-08-29 12:10:39 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/multi_turn/mt_bench/data/mt_bench_thai_full.jsonl
2025-08-29 12:10:39 | INFO     | seahelm_evaluation   | Performing inference for task 'MT-BENCH' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:10:39 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.59 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  7.03 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.96 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.32 examples/s]
[92m12:10:40 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:40 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:40 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:40 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:40 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:40 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:41 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:41 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:10:45 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:45 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:10:45 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:45 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 370.36 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 464.28 examples/s]
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:10:45 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.28 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.77 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.66 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.05 examples/s]
[92m12:10:46 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:46 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:46 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:46 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:46 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:46 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:50 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:50 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:10:51 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:51 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:10:51 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:51 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 336.90 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 454.14 examples/s]
2025-08-29 12:10:51 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_th.jsonl
2025-08-29 12:10:51 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:10:51 | INFO     | seahelm_evaluation   | Inference for task 'MT-BENCH' completed!
                                                      | 
2025-08-29 12:10:51 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: MT-BENCH ----------
                                                      | Testing Competency: MULTI-TURN
2025-08-29 12:10:51 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/multi_turn/mt_bench/data/mt_bench_tagalog_full.jsonl
2025-08-29 12:10:51 | INFO     | seahelm_evaluation   | Performing inference for task 'MT-BENCH' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:10:51 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.55 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.46 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.25 examples/s]
[92m12:10:52 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:52 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:52 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:52 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:52 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:52 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:57 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:57 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:10:57 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:57 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:10:58 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:10:58 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 368.56 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 470.16 examples/s]
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:10:58 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.59 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  7.13 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.90 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.23 examples/s]
[92m12:10:59 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:59 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:59 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:10:59 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:59 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:10:59 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:04 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:04 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:04 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:04 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:05 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:05 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 335.69 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 454.42 examples/s]
2025-08-29 12:11:05 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_tl.jsonl
2025-08-29 12:11:05 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:05 | INFO     | seahelm_evaluation   | Inference for task 'MT-BENCH' completed!
                                                      | 
2025-08-29 12:11:05 | INFO     | seahelm_evaluation   | Starting mt-bench evaluation using multiprocessing
2025-08-29 12:11:05 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: SENTIMENT ----------
                                                      | Testing Competency: NLU
2025-08-29 12:11:05 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/sentiment_analysis/data/id_nusax.jsonl
2025-08-29 12:11:05 | INFO     | mt_bench             | --------- Evaluation | Lang: ID | Task: MT-BENCH ----------
2025-08-29 12:11:05 | INFO     | mt_bench             | Evaluating MT-BENCH using MTBenchMetric
2025-08-29 12:11:05 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:05 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:05 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:05 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:11:05 | INFO     | mt_bench             | First run: processing all judgments.
2025-08-29 12:11:05 | INFO     | mt_bench             | --------- Evaluation | Lang: VI | Task: MT-BENCH ----------
2025-08-29 12:11:05 | INFO     | mt_bench             | Evaluating MT-BENCH using MTBenchMetric
2025-08-29 12:11:05 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:05 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:05 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:05 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:11:05 | INFO     | mt_bench             | --------- Evaluation | Lang: TH | Task: MT-BENCH ----------
2025-08-29 12:11:05 | INFO     | mt_bench             | Evaluating MT-BENCH using MTBenchMetric
2025-08-29 12:11:05 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:05 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:05 | INFO     | mt_bench             | --------- Evaluation | Lang: TL | Task: MT-BENCH ----------
2025-08-29 12:11:05 | INFO     | mt_bench             | Evaluating MT-BENCH using MTBenchMetric
2025-08-29 12:11:05 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:05 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:11:05 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:05 | INFO     | mt_bench             | First run: processing all judgments.
2025-08-29 12:11:05 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:05 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:05 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:11:05 | INFO     | mt_bench             | First run: processing all judgments.
2025-08-29 12:11:05 | INFO     | mt_bench             | First run: processing all judgments.
2025-08-29 12:11:05 | INFO     | seahelm_evaluation   | Performing inference for task 'SENTIMENT' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:05 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.56 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  7.07 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.73 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.17 examples/s]
[92m12:11:06 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:06 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:06 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:06 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:06 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:06 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:06 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:06 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:06 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:06 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:06 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:06 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 388.23 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 545.92 examples/s]
2025-08-29 12:11:06 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_id.jsonl
2025-08-29 12:11:06 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:06 | INFO     | seahelm_evaluation   | Inference for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:11:06 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: SENTIMENT ----------
2025-08-29 12:11:06 | INFO     | seahelm_evaluation   | Evaluating 'SENTIMENT' using SentimentAnalysisMetric
2025-08-29 12:11:06 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:06 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:06 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:06 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:11:06 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:11:06 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 0 1 0]
                                                      |  [0 0 1 0]
                                                      |  [0 0 0 0]
                                                      |  [0 0 1 0]]
2025-08-29 12:11:06 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |     negative       0.00      0.00      0.00       1.0
                                                      |      neutral       0.00      0.00      0.00       1.0
                                                      |         none       0.00      0.00      0.00       0.0
                                                      |     positive       0.00      0.00      0.00       1.0
                                                      | 
                                                      |     accuracy                           0.00       3.0
                                                      |    macro avg       0.00      0.00      0.00       3.0
                                                      | weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:11:06 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:06 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_id.jsonl
2025-08-29 12:11:06 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:06 | INFO     | seahelm_evaluation   | Evaluation for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:11:06 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: QA ----------
                                                      | Testing Competency: NLU
2025-08-29 12:11:06 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/question_answering/data/id_tydiqa_100sample.jsonl
2025-08-29 12:11:06 | INFO     | seahelm_evaluation   | Performing inference for task 'QA' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:06 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.57 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  7.10 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.83 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.21 examples/s]
[92m12:11:07 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:07 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:07 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:07 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:07 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:07 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:08 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:08 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:08 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:08 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:08 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:08 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 410.98 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 529.85 examples/s]
2025-08-29 12:11:08 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_qa_id.jsonl
2025-08-29 12:11:08 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:08 | INFO     | seahelm_evaluation   | Inference for task 'QA' completed!
                                                      | 
2025-08-29 12:11:08 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: QA ----------
2025-08-29 12:11:10 | INFO     | seahelm_evaluation   | Evaluating 'QA' using QuestionAnsweringMetric
2025-08-29 12:11:10 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:10 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:10 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:10 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:11:10 | INFO     | question_answering   | {'exact_match': 66.66666666666667, 'f1': 80.0, 'normalized_f1': 80.0}
2025-08-29 12:11:10 | INFO     | question_answering   | 3 answers out of 3 (100.00%) can be found in the model's predictions.
2025-08-29 12:11:10 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:10 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_qa_id.jsonl
2025-08-29 12:11:10 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:10 | INFO     | seahelm_evaluation   | Evaluation for task 'QA' completed!
                                                      | 
2025-08-29 12:11:10 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: METAPHOR ----------
                                                      | Testing Competency: NLU
2025-08-29 12:11:10 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/metaphor/data/id_multilingual_fig_qa.jsonl
2025-08-29 12:11:11 | INFO     | seahelm_evaluation   | Performing inference for task 'METAPHOR' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:11 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.44 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.85 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.96 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.18 examples/s]
[92m12:11:11 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:11 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:11 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:11 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:11 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:11 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:12 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:12 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:12 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:12 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:12 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:12 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 410.74 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 520.75 examples/s]
2025-08-29 12:11:12 | INFO     | seahelm_evaluation   | Saving inference results for task 'METAPHOR' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_metaphor_id.jsonl
2025-08-29 12:11:12 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:12 | INFO     | seahelm_evaluation   | Inference for task 'METAPHOR' completed!
                                                      | 
2025-08-29 12:11:12 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: METAPHOR ----------
2025-08-29 12:11:12 | INFO     | seahelm_evaluation   | Evaluating 'METAPHOR' using MetaphorUnderstandingMetric
2025-08-29 12:11:12 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:12 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:12 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:12 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:11:12 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:11:12 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 0 2]
                                                      |  [0 0 1]
                                                      |  [0 0 0]]
2025-08-29 12:11:12 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       2.0
                                                      |            1       0.00      0.00      0.00       1.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       3.0
                                                      |    macro avg       0.00      0.00      0.00       3.0
                                                      | weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:11:12 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:12 | INFO     | seahelm_evaluation   | Saving inference results for task 'METAPHOR' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_metaphor_id.jsonl
2025-08-29 12:11:12 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:12 | INFO     | seahelm_evaluation   | Evaluation for task 'METAPHOR' completed!
                                                      | 
2025-08-29 12:11:12 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: TOXICITY ----------
                                                      | Testing Competency: SAFETY
2025-08-29 12:11:12 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/safety/toxicity_detection/data/id_ml-hsd_1000sample.jsonl
2025-08-29 12:11:12 | INFO     | seahelm_evaluation   | Performing inference for task 'TOXICITY' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:12 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.57 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.27 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.12 examples/s]
[92m12:11:13 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:13 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:13 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:13 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:13 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:13 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:13 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:13 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:13 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:13 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:13 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:13 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 328.52 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 404.61 examples/s]
2025-08-29 12:11:13 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_id.jsonl
2025-08-29 12:11:13 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:13 | INFO     | seahelm_evaluation   | Inference for task 'TOXICITY' completed!
                                                      | 
2025-08-29 12:11:13 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: TOXICITY ----------
2025-08-29 12:11:13 | INFO     | seahelm_evaluation   | Evaluating 'TOXICITY' using ToxicityDetectionMetric
2025-08-29 12:11:13 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:13 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:13 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:13 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:11:13 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:11:13 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 3]
                                                      |  [0 0]]
2025-08-29 12:11:13 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       3.0
                                                      |            3       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       3.0
                                                      |    macro avg       0.00      0.00      0.00       3.0
                                                      | weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:11:13 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:13 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_id.jsonl
2025-08-29 12:11:13 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:13 | INFO     | seahelm_evaluation   | Evaluation for task 'TOXICITY' completed!
                                                      | 
2025-08-29 12:11:13 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: TRANSLATION-EN-XX ----------
                                                      | Testing Competency: NLG
2025-08-29 12:11:13 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/en_to_ind_Latn.jsonl
2025-08-29 12:11:13 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-EN-XX' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:13 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.41 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.99 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.78 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.13 examples/s]
[92m12:11:14 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:14 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:14 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:14 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:14 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:14 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:15 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:15 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:15 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:15 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:15 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:15 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 435.29 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 585.63 examples/s]
2025-08-29 12:11:15 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_id.jsonl
2025-08-29 12:11:15 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:15 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:11:15 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: TRANSLATION-XX-EN ----------
                                                      | Testing Competency: NLG
2025-08-29 12:11:15 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/ind_Latn_to_en.jsonl
2025-08-29 12:11:15 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-XX-EN' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:15 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.47 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  7.04 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.84 examples/s]2025-08-29 12:11:16 | INFO     | openai_serving       | Prompts sent via OpenAI batch API
2025-08-29 12:11:16 | INFO     | openai_serving       | Waiting for OpenAI batch to complete...
Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.12 examples/s]
[92m12:11:16 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:16 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:16 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:16 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:16 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:16 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:16 | INFO     | openai_serving       | Prompts sent via OpenAI batch API
2025-08-29 12:11:16 | INFO     | openai_serving       | Waiting for OpenAI batch to complete...
[92m12:11:17 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:17 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:17 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:17 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:17 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:17 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]2025-08-29 12:11:17 | INFO     | openai_serving       | Prompts sent via OpenAI batch API
2025-08-29 12:11:17 | INFO     | openai_serving       | Waiting for OpenAI batch to complete...
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 346.88 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 571.35 examples/s]
2025-08-29 12:11:17 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_id.jsonl
2025-08-29 12:11:17 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:17 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:11:17 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: ABSSUM ----------
                                                      | Testing Competency: NLG
2025-08-29 12:11:17 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/abstractive_summarization/data/id_xlsum_100sample.jsonl
2025-08-29 12:11:17 | INFO     | seahelm_evaluation   | Performing inference for task 'ABSSUM' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:17 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:17 | INFO     | openai_serving       | Prompts sent via OpenAI batch API
2025-08-29 12:11:17 | INFO     | openai_serving       | Waiting for OpenAI batch to complete...
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.39 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.95 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.76 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.11 examples/s]
[92m12:11:18 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:18 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:18 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:18 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:18 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:18 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:20 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:20 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:20 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:20 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:21 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:21 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 406.58 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 523.66 examples/s]
2025-08-29 12:11:21 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_id.jsonl
2025-08-29 12:11:21 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:21 | INFO     | seahelm_evaluation   | Inference for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:11:21 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: ABSSUM ----------
2025-08-29 12:11:22 | INFO     | seahelm_evaluation   | Evaluating 'ABSSUM' using SummarizationMetric
2025-08-29 12:11:22 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:22 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:22 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:22 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:11:22 | INFO     | summarization        | Rouge-L Scores:
2025-08-29 12:11:22 | INFO     | summarization        | Precision: 15.98 | Recall: 17.11 | F1: 15.73
2025-08-29 12:11:22 | INFO     | summarization        | Norm F1 Score: 15.73
2025-08-29 12:11:22 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:22 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_id.jsonl
2025-08-29 12:11:22 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:22 | INFO     | seahelm_evaluation   | Evaluation for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:11:22 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: CAUSAL ----------
                                                      | Testing Competency: NLR
2025-08-29 12:11:22 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/causal/data/id_xcopa.jsonl
2025-08-29 12:11:22 | INFO     | seahelm_evaluation   | Performing inference for task 'CAUSAL' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:22 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.46 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.81 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.95 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.16 examples/s]
[92m12:11:22 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:22 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:22 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:22 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:22 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:22 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:23 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:23 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:23 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:23 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:23 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:23 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 355.93 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 490.73 examples/s]
2025-08-29 12:11:23 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_causal_id.jsonl
2025-08-29 12:11:23 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:23 | INFO     | seahelm_evaluation   | Inference for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:11:23 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: CAUSAL ----------
2025-08-29 12:11:23 | INFO     | seahelm_evaluation   | Evaluating 'CAUSAL' using CausalReasoningMetric
2025-08-29 12:11:23 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:23 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:23 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:23 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:11:23 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:11:23 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 0 2]
                                                      |  [0 0 1]
                                                      |  [0 0 0]]
2025-08-29 12:11:23 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       2.0
                                                      |            1       0.00      0.00      0.00       1.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       3.0
                                                      |    macro avg       0.00      0.00      0.00       3.0
                                                      | weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:11:23 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:23 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_causal_id.jsonl
2025-08-29 12:11:23 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:23 | INFO     | seahelm_evaluation   | Evaluation for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:11:23 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: NLI ----------
                                                      | Testing Competency: NLR
2025-08-29 12:11:23 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/nli/data/id_indonli_lay_1000sample.jsonl
2025-08-29 12:11:23 | INFO     | seahelm_evaluation   | Performing inference for task 'NLI' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:23 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.49 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  7.10 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.90 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.20 examples/s]
[92m12:11:24 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:24 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:24 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:24 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:24 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:24 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:25 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:25 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:25 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:25 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:25 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:25 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 431.71 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 564.79 examples/s]
2025-08-29 12:11:25 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_nli_id.jsonl
2025-08-29 12:11:25 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:25 | INFO     | seahelm_evaluation   | Inference for task 'NLI' completed!
                                                      | 
2025-08-29 12:11:25 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: NLI ----------
2025-08-29 12:11:25 | INFO     | seahelm_evaluation   | Evaluating 'NLI' using NLIMetric
2025-08-29 12:11:25 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:25 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:25 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:25 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:11:25 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:11:25 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 3]
                                                      |  [0 0]]
2025-08-29 12:11:25 | INFO     | seahelm_metric       | Classification report:
                                                      |                precision    recall  f1-score   support
                                                      | 
                                                      | contradiction       0.00      0.00      0.00       3.0
                                                      |          none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |      accuracy                           0.00       3.0
                                                      |     macro avg       0.00      0.00      0.00       3.0
                                                      |  weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:11:25 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:25 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_nli_id.jsonl
2025-08-29 12:11:25 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:25 | INFO     | seahelm_evaluation   | Evaluation for task 'NLI' completed!
                                                      | 
2025-08-29 12:11:25 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: MP-R ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 12:11:25 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/syntax/data/id_syntax_mcq_randomized.jsonl
2025-08-29 12:11:25 | INFO     | seahelm_evaluation   | Performing inference for task 'MP-R' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:25 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.50 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.60 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.18 examples/s]
[92m12:11:26 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:26 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:26 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:26 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:26 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:26 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:26 | INFO     | openai_serving       | Still waiting (10s has elapsed)...
[92m12:11:26 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:26 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:26 | INFO     | openai_serving       | Still waiting (10s has elapsed)...
[92m12:11:26 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:26 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:11:27 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:27 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:27 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:27 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:27 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:27 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 357.38 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 460.27 examples/s]
2025-08-29 12:11:27 | INFO     | seahelm_evaluation   | Saving inference results for task 'MP-R' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_mp-r_id.jsonl
2025-08-29 12:11:27 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:27 | INFO     | seahelm_evaluation   | Inference for task 'MP-R' completed!
                                                      | 
2025-08-29 12:11:27 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: MP-R ----------
2025-08-29 12:11:27 | INFO     | seahelm_evaluation   | Evaluating 'MP-R' using MinimalPairsMetric
2025-08-29 12:11:27 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:27 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:27 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:27 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:11:27 | INFO     | minimal_pairs        | Accuracy for phenomenon <NPIs_and_negation>: 0.0
2025-08-29 12:11:27 | INFO     | minimal_pairs        | Overall Accuracy: 0.0
2025-08-29 12:11:27 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:27 | INFO     | seahelm_evaluation   | Saving inference results for task 'MP-R' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_mp-r_id.jsonl
2025-08-29 12:11:27 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:27 | INFO     | seahelm_evaluation   | Evaluation for task 'MP-R' completed!
                                                      | 
2025-08-29 12:11:27 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: PRAGMATIC-SINGLE ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 12:11:27 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/pragmatics/data/id_pragmatic_reasoning_single.jsonl
2025-08-29 12:11:27 | INFO     | seahelm_evaluation   | Performing inference for task 'PRAGMATIC-SINGLE' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:27 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.43 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  7.03 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.85 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.14 examples/s]
[92m12:11:27 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:27 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:27 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:27 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:27 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:27 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:27 | INFO     | openai_serving       | Still waiting (10s has elapsed)...
[92m12:11:27 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:27 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:28 | INFO     | openai_serving       | Still waiting (10s has elapsed)...
[92m12:11:28 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:28 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:11:28 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:28 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:28 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:28 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:28 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:28 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 349.46 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 450.60 examples/s]
2025-08-29 12:11:28 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-SINGLE' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-single_id.jsonl
2025-08-29 12:11:28 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:28 | INFO     | seahelm_evaluation   | Inference for task 'PRAGMATIC-SINGLE' completed!
                                                      | 
2025-08-29 12:11:28 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: PRAGMATIC-SINGLE ----------
2025-08-29 12:11:28 | INFO     | seahelm_evaluation   | Evaluating 'PRAGMATIC-SINGLE' using PragmaticReasoningSingleSentenceMetric
2025-08-29 12:11:28 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:28 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:28 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:28 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  subset_references = subset[self.label_column].replace(
2025-08-29 12:11:28 | INFO     | pragmatic_reasoning  | Accuracy for phenomenon <scalar_implicatures>: 0.0 / 3
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:93: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  references = self.inference_df[self.label_column].replace(
2025-08-29 12:11:28 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:28 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-SINGLE' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-single_id.jsonl
2025-08-29 12:11:28 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:28 | INFO     | seahelm_evaluation   | Evaluation for task 'PRAGMATIC-SINGLE' completed!
                                                      | 
2025-08-29 12:11:28 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: PRAGMATIC-PAIR ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 12:11:28 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/pragmatics/data/id_pragmatic_reasoning_pair.jsonl
2025-08-29 12:11:29 | INFO     | seahelm_evaluation   | Performing inference for task 'PRAGMATIC-PAIR' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:29 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.52 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.83 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.25 examples/s]
[92m12:11:29 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:29 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:29 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:29 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:29 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:29 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:30 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:30 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:30 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:30 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:30 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:30 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 389.07 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 496.50 examples/s]
2025-08-29 12:11:30 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-PAIR' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-pair_id.jsonl
2025-08-29 12:11:30 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:30 | INFO     | seahelm_evaluation   | Inference for task 'PRAGMATIC-PAIR' completed!
                                                      | 
2025-08-29 12:11:30 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: PRAGMATIC-PAIR ----------
2025-08-29 12:11:30 | INFO     | seahelm_evaluation   | Evaluating 'PRAGMATIC-PAIR' using PragmaticReasoningSentencePairMetric
2025-08-29 12:11:30 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:30 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:30 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:30 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:143: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  subset_references = subset[self.label_column].replace({True: 1, False: 0})
2025-08-29 12:11:30 | INFO     | pragmatic_reasoning  | Accuracy for phenomenon <scalar_implicatures>: 0.0 / 3
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:157: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  references = self.inference_df[self.label_column].replace({True: 1, False: 0})
2025-08-29 12:11:30 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:30 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-PAIR' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-pair_id.jsonl
2025-08-29 12:11:30 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:30 | INFO     | seahelm_evaluation   | Evaluation for task 'PRAGMATIC-PAIR' completed!
                                                      | 
2025-08-29 12:11:30 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: ID | Task: IF-EVAL ----------
                                                      | Testing Competency: INSTRUCTION-FOLLOWING
2025-08-29 12:11:30 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/instruction_following/ifeval/data/id_sea_ifeval.jsonl
2025-08-29 12:11:30 | INFO     | seahelm_evaluation   | Performing inference for task 'IF-EVAL' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:30 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.36 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.89 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.94 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.12 examples/s]
[92m12:11:31 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:31 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:31 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:31 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:31 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:31 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:31 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:31 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:36 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:36 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:36 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:36 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 243.48 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 312.26 examples/s]
2025-08-29 12:11:36 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_id.jsonl
2025-08-29 12:11:36 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:36 | INFO     | seahelm_evaluation   | Inference for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 12:11:36 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: IF-EVAL ----------
2025-08-29 12:11:36 | INFO     | seahelm_evaluation   | Evaluating 'IF-EVAL' using IFEvalMetric
2025-08-29 12:11:36 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:36 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:36 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:36 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:11:36 | INFO     | if_eval              | Overall pass: 1 / 3
2025-08-29 12:11:36 | INFO     | if_eval              | Overall accuracy: 33.333333
2025-08-29 12:11:36 | INFO     | if_eval              | Correct language rate: 0.666667
2025-08-29 12:11:36 | INFO     | if_eval              | Lang normalized accuracy: 33.333333
2025-08-29 12:11:36 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:36 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_id.jsonl
2025-08-29 12:11:36 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:36 | INFO     | seahelm_evaluation   | Evaluation for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 12:11:36 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: SENTIMENT ----------
                                                      | Testing Competency: NLU
2025-08-29 12:11:36 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/sentiment_analysis/data/vi_uit-vsfc_1000sample.jsonl
2025-08-29 12:11:36 | INFO     | seahelm_evaluation   | Performing inference for task 'SENTIMENT' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:36 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:36 | INFO     | openai_serving       | Still waiting (20s has elapsed)...
[92m12:11:36 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:36 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]2025-08-29 12:11:36 | INFO     | openai_serving       | Still waiting (20s has elapsed)...
[92m12:11:36 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:36 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.24 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.54 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.64 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.99 examples/s]
[92m12:11:37 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:37 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:37 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:37 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:37 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:37 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:37 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:37 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:37 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:37 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:37 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:37 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 460.79 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 595.89 examples/s]
2025-08-29 12:11:37 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_vi.jsonl
2025-08-29 12:11:37 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:37 | INFO     | seahelm_evaluation   | Inference for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:11:37 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: SENTIMENT ----------
2025-08-29 12:11:37 | INFO     | seahelm_evaluation   | Evaluating 'SENTIMENT' using SentimentAnalysisMetric
2025-08-29 12:11:37 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:37 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:37 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:37 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:11:37 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:11:37 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 3]
                                                      |  [0 0]]
2025-08-29 12:11:37 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |     negative       0.00      0.00      0.00       3.0
                                                      |         none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       3.0
                                                      |    macro avg       0.00      0.00      0.00       3.0
                                                      | weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:11:37 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:37 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_vi.jsonl
2025-08-29 12:11:37 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:37 | INFO     | seahelm_evaluation   | Evaluation for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:11:37 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: QA ----------
                                                      | Testing Competency: NLU
2025-08-29 12:11:37 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/question_answering/data/vi_xquad_100sample.jsonl
2025-08-29 12:11:37 | INFO     | seahelm_evaluation   | Performing inference for task 'QA' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:37 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]2025-08-29 12:11:38 | INFO     | openai_serving       | Still waiting (20s has elapsed)...
[92m12:11:38 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:38 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:38 | INFO     | openai_serving       | Still waiting (20s has elapsed)...
[92m12:11:38 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:38 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.44 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  7.04 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.87 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.17 examples/s]
[92m12:11:38 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:38 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:38 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:38 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:38 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:38 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:39 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:39 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:39 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:39 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:39 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:39 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 419.84 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 540.27 examples/s]
2025-08-29 12:11:39 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_qa_vi.jsonl
2025-08-29 12:11:39 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:39 | INFO     | seahelm_evaluation   | Inference for task 'QA' completed!
                                                      | 
2025-08-29 12:11:39 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: QA ----------
2025-08-29 12:11:39 | INFO     | seahelm_evaluation   | Evaluating 'QA' using QuestionAnsweringMetric
2025-08-29 12:11:39 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:39 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:39 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:39 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:11:39 | INFO     | question_answering   | {'exact_match': 66.66666666666667, 'f1': 83.33333333333333, 'normalized_f1': 83.33333333333333}
2025-08-29 12:11:39 | INFO     | question_answering   | 2 answers out of 3 (66.67%) can be found in the model's predictions.
2025-08-29 12:11:39 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:39 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_qa_vi.jsonl
2025-08-29 12:11:39 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:39 | INFO     | seahelm_evaluation   | Evaluation for task 'QA' completed!
                                                      | 
2025-08-29 12:11:39 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: TOXICITY ----------
                                                      | Testing Competency: SAFETY
2025-08-29 12:11:39 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/safety/toxicity_detection/data/vi_vihsd_1000sample.jsonl
2025-08-29 12:11:39 | INFO     | seahelm_evaluation   | Performing inference for task 'TOXICITY' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:39 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.39 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.98 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.79 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.11 examples/s]
[92m12:11:40 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:40 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:40 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:40 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:40 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:40 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:40 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:40 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:40 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:40 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:40 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:40 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 448.44 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 576.06 examples/s]
2025-08-29 12:11:40 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_vi.jsonl
2025-08-29 12:11:40 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:40 | INFO     | seahelm_evaluation   | Inference for task 'TOXICITY' completed!
                                                      | 
2025-08-29 12:11:40 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: TOXICITY ----------
2025-08-29 12:11:40 | INFO     | seahelm_evaluation   | Evaluating 'TOXICITY' using ToxicityDetectionMetric
2025-08-29 12:11:40 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:40 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:40 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:40 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:11:40 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:11:40 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 3]
                                                      |  [0 0]]
2025-08-29 12:11:40 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       3.0
                                                      |            3       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       3.0
                                                      |    macro avg       0.00      0.00      0.00       3.0
                                                      | weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:11:40 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:40 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_vi.jsonl
2025-08-29 12:11:40 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:40 | INFO     | seahelm_evaluation   | Evaluation for task 'TOXICITY' completed!
                                                      | 
2025-08-29 12:11:40 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: TRANSLATION-EN-XX ----------
                                                      | Testing Competency: NLG
2025-08-29 12:11:40 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/en_to_vie_Latn.jsonl
2025-08-29 12:11:41 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-EN-XX' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:41 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  4.97 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.76 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.19 examples/s]
[92m12:11:41 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:41 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:41 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:41 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:41 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:41 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:42 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:42 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:42 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:42 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:43 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:43 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 452.18 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 589.53 examples/s]
2025-08-29 12:11:43 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_vi.jsonl
2025-08-29 12:11:43 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:43 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:11:43 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: TRANSLATION-XX-EN ----------
                                                      | Testing Competency: NLG
2025-08-29 12:11:43 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/vie_Latn_to_en.jsonl
2025-08-29 12:11:43 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-XX-EN' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:43 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.56 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.52 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.21 examples/s]
[92m12:11:44 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:44 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:44 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:44 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:44 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:44 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:45 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:45 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:45 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:45 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:45 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:45 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 535.03 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 567.13 examples/s]
2025-08-29 12:11:45 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_vi.jsonl
2025-08-29 12:11:45 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:45 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:11:45 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: ABSSUM ----------
                                                      | Testing Competency: NLG
2025-08-29 12:11:45 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/abstractive_summarization/data/vi_xlsum_100sample.jsonl
2025-08-29 12:11:45 | INFO     | seahelm_evaluation   | Performing inference for task 'ABSSUM' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:45 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.49 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.69 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.18 examples/s]
[92m12:11:46 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:46 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:46 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:46 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:46 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:46 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:46 | INFO     | openai_serving       | Still waiting (30s has elapsed)...
[92m12:11:46 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:46 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:47 | INFO     | openai_serving       | Still waiting (30s has elapsed)...
[92m12:11:47 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:47 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:48 | INFO     | openai_serving       | Still waiting (30s has elapsed)...
[92m12:11:48 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:48 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:48 | INFO     | openai_serving       | Still waiting (30s has elapsed)...
[92m12:11:48 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:48 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:11:48 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:48 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:48 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:48 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:49 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:49 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 405.49 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 523.57 examples/s]
2025-08-29 12:11:49 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_vi.jsonl
2025-08-29 12:11:49 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:49 | INFO     | seahelm_evaluation   | Inference for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:11:49 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: ABSSUM ----------
2025-08-29 12:11:49 | INFO     | seahelm_evaluation   | Evaluating 'ABSSUM' using SummarizationMetric
2025-08-29 12:11:49 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:49 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:49 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:49 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:11:49 | INFO     | summarization        | Rouge-L Scores:
2025-08-29 12:11:49 | INFO     | summarization        | Precision: 12.05 | Recall: 15.62 | F1: 13.59
2025-08-29 12:11:49 | INFO     | summarization        | Norm F1 Score: 13.59
2025-08-29 12:11:49 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:49 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_vi.jsonl
2025-08-29 12:11:49 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:49 | INFO     | seahelm_evaluation   | Evaluation for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:11:49 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: CAUSAL ----------
                                                      | Testing Competency: NLR
2025-08-29 12:11:49 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/causal/data/vi_xcopa.jsonl
2025-08-29 12:11:49 | INFO     | seahelm_evaluation   | Performing inference for task 'CAUSAL' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:49 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.46 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  7.07 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.87 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.17 examples/s]
[92m12:11:49 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:49 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:49 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:49 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:49 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:49 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:50 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:50 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:50 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:50 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:50 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:50 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 399.57 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 505.48 examples/s]
2025-08-29 12:11:50 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_causal_vi.jsonl
2025-08-29 12:11:50 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:50 | INFO     | seahelm_evaluation   | Inference for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:11:50 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: CAUSAL ----------
2025-08-29 12:11:50 | INFO     | seahelm_evaluation   | Evaluating 'CAUSAL' using CausalReasoningMetric
2025-08-29 12:11:50 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:50 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:50 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:50 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:11:50 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:11:50 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 0 2]
                                                      |  [0 0 1]
                                                      |  [0 0 0]]
2025-08-29 12:11:50 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       2.0
                                                      |            1       0.00      0.00      0.00       1.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       3.0
                                                      |    macro avg       0.00      0.00      0.00       3.0
                                                      | weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:11:50 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:50 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_causal_vi.jsonl
2025-08-29 12:11:50 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:50 | INFO     | seahelm_evaluation   | Evaluation for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:11:50 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: NLI ----------
                                                      | Testing Competency: NLR
2025-08-29 12:11:50 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/nli/data/vi_xnli_1000sample.jsonl
2025-08-29 12:11:51 | INFO     | seahelm_evaluation   | Performing inference for task 'NLI' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:51 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.42 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.69 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.15 examples/s]
[92m12:11:51 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:51 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:51 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:51 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:51 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:51 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:52 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:52 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:52 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:52 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:52 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:52 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 441.65 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 558.64 examples/s]
2025-08-29 12:11:52 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_nli_vi.jsonl
2025-08-29 12:11:52 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:52 | INFO     | seahelm_evaluation   | Inference for task 'NLI' completed!
                                                      | 
2025-08-29 12:11:52 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: NLI ----------
2025-08-29 12:11:52 | INFO     | seahelm_evaluation   | Evaluating 'NLI' using NLIMetric
2025-08-29 12:11:52 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:52 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:52 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:52 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:11:52 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:11:52 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 3]
                                                      |  [0 0]]
2025-08-29 12:11:52 | INFO     | seahelm_metric       | Classification report:
                                                      |                precision    recall  f1-score   support
                                                      | 
                                                      | contradiction       0.00      0.00      0.00       3.0
                                                      |          none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |      accuracy                           0.00       3.0
                                                      |     macro avg       0.00      0.00      0.00       3.0
                                                      |  weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:11:52 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:52 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_nli_vi.jsonl
2025-08-29 12:11:52 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:52 | INFO     | seahelm_evaluation   | Evaluation for task 'NLI' completed!
                                                      | 
2025-08-29 12:11:52 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: VI | Task: IF-EVAL ----------
                                                      | Testing Competency: INSTRUCTION-FOLLOWING
2025-08-29 12:11:52 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/instruction_following/ifeval/data/vi_sea_ifeval.jsonl
2025-08-29 12:11:52 | INFO     | seahelm_evaluation   | Performing inference for task 'IF-EVAL' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:52 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.42 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.90 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.97 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.16 examples/s]
[92m12:11:53 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:53 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:53 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:53 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:53 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:53 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:53 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:53 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:55 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:55 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:57 | INFO     | openai_serving       | Still waiting (40s has elapsed)...
[92m12:11:57 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:57 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:57 | INFO     | openai_serving       | Still waiting (40s has elapsed)...
[92m12:11:57 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:57 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:11:58 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:58 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 245.30 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 306.94 examples/s]
2025-08-29 12:11:58 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_vi.jsonl
2025-08-29 12:11:58 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:58 | INFO     | seahelm_evaluation   | Inference for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 12:11:58 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: IF-EVAL ----------
2025-08-29 12:11:58 | INFO     | seahelm_evaluation   | Evaluating 'IF-EVAL' using IFEvalMetric
2025-08-29 12:11:58 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:58 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:58 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:58 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:11:58 | INFO     | if_eval              | Overall pass: 1 / 3
2025-08-29 12:11:58 | INFO     | if_eval              | Overall accuracy: 33.333333
2025-08-29 12:11:58 | INFO     | if_eval              | Correct language rate: 0.666667
2025-08-29 12:11:58 | INFO     | if_eval              | Lang normalized accuracy: 33.333333
2025-08-29 12:11:58 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:58 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_vi.jsonl
2025-08-29 12:11:58 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:58 | INFO     | seahelm_evaluation   | Evaluation for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 12:11:58 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: SENTIMENT ----------
                                                      | Testing Competency: NLU
2025-08-29 12:11:58 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/sentiment_analysis/data/th_wisesight_no_q_1000sample.jsonl
2025-08-29 12:11:58 | INFO     | seahelm_evaluation   | Performing inference for task 'SENTIMENT' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:58 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]2025-08-29 12:11:58 | INFO     | openai_serving       | Still waiting (40s has elapsed)...
[92m12:11:58 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:58 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:58 | INFO     | openai_serving       | Still waiting (40s has elapsed)...
[92m12:11:58 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:11:58 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.32 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.95 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.78 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.10 examples/s]
[92m12:11:59 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:59 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:59 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:59 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:59 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:11:59 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:11:59 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:59 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:59 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:59 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:11:59 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:11:59 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 467.73 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 507.52 examples/s]
2025-08-29 12:11:59 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_th.jsonl
2025-08-29 12:11:59 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:59 | INFO     | seahelm_evaluation   | Inference for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:11:59 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: SENTIMENT ----------
2025-08-29 12:11:59 | INFO     | seahelm_evaluation   | Evaluating 'SENTIMENT' using SentimentAnalysisMetric
2025-08-29 12:11:59 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:11:59 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:11:59 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:11:59 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:11:59 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:11:59 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 3]
                                                      |  [0 0]]
2025-08-29 12:11:59 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |     negative       0.00      0.00      0.00       3.0
                                                      |         none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       3.0
                                                      |    macro avg       0.00      0.00      0.00       3.0
                                                      | weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:11:59 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:11:59 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_th.jsonl
2025-08-29 12:11:59 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:11:59 | INFO     | seahelm_evaluation   | Evaluation for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:11:59 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: QA ----------
                                                      | Testing Competency: NLU
2025-08-29 12:11:59 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/question_answering/data/th_xquad_100sample.jsonl
2025-08-29 12:11:59 | INFO     | seahelm_evaluation   | Performing inference for task 'QA' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:11:59 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.40 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.68 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.12 examples/s]
[92m12:12:00 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:00 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:00 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:00 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:00 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:00 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:01 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:01 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:01 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:01 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:01 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:01 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 416.39 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 534.99 examples/s]
2025-08-29 12:12:01 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_qa_th.jsonl
2025-08-29 12:12:01 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:01 | INFO     | seahelm_evaluation   | Inference for task 'QA' completed!
                                                      | 
2025-08-29 12:12:01 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: QA ----------
2025-08-29 12:12:01 | INFO     | seahelm_evaluation   | Evaluating 'QA' using QuestionAnsweringMetric
2025-08-29 12:12:01 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:01 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:01 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:01 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:12:01 | INFO     | question_answering   | Tokenizing Thai text and re-evaluating...
2025-08-29 12:12:01 | INFO     | question_answering   | {'exact_match': 33.333333333333336, 'f1': 33.333333333333336, 'normalized_f1': 33.333333333333336}
2025-08-29 12:12:01 | INFO     | question_answering   | 1 answers out of 3 (33.33%) can be found in the model's predictions.
2025-08-29 12:12:01 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:01 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_qa_th.jsonl
2025-08-29 12:12:01 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:01 | INFO     | seahelm_evaluation   | Evaluation for task 'QA' completed!
                                                      | 
2025-08-29 12:12:01 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: TOXICITY ----------
                                                      | Testing Competency: SAFETY
2025-08-29 12:12:01 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/safety/toxicity_detection/data/th_toxicity_1000sample.jsonl
2025-08-29 12:12:01 | INFO     | seahelm_evaluation   | Performing inference for task 'TOXICITY' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:01 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.38 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  7.02 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.77 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.10 examples/s]
[92m12:12:02 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:02 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:02 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:02 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:02 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:02 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:02 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:02 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:02 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:02 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:02 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:02 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 409.55 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 522.72 examples/s]
2025-08-29 12:12:02 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_th.jsonl
2025-08-29 12:12:02 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:02 | INFO     | seahelm_evaluation   | Inference for task 'TOXICITY' completed!
                                                      | 
2025-08-29 12:12:02 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: TOXICITY ----------
2025-08-29 12:12:02 | INFO     | seahelm_evaluation   | Evaluating 'TOXICITY' using ToxicityDetectionMetric
2025-08-29 12:12:02 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:02 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:02 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:02 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:12:02 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:12:02 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 3]
                                                      |  [0 0]]
2025-08-29 12:12:02 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       3.0
                                                      |            3       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       3.0
                                                      |    macro avg       0.00      0.00      0.00       3.0
                                                      | weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:12:02 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:02 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_th.jsonl
2025-08-29 12:12:02 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:02 | INFO     | seahelm_evaluation   | Evaluation for task 'TOXICITY' completed!
                                                      | 
2025-08-29 12:12:02 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: TRANSLATION-EN-XX ----------
                                                      | Testing Competency: NLG
2025-08-29 12:12:02 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/en_to_tha_Thai.jsonl
2025-08-29 12:12:03 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-EN-XX' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:03 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  4.89 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.98 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.92 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.11 examples/s]
[92m12:12:03 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:03 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:03 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:03 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:03 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:03 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:04 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:04 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:05 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:05 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:05 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:05 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 490.47 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 538.10 examples/s]
2025-08-29 12:12:05 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_th.jsonl
2025-08-29 12:12:05 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:05 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:12:05 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: TRANSLATION-XX-EN ----------
                                                      | Testing Competency: NLG
2025-08-29 12:12:05 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/tha_Thai_to_en.jsonl
2025-08-29 12:12:05 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-XX-EN' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:05 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.46 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.68 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.19 examples/s]
[92m12:12:06 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:06 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:06 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:06 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:06 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:06 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:07 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:07 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:07 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:07 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:07 | INFO     | openai_serving       | Still waiting (50s has elapsed)...
[92m12:12:07 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:07 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:07 | INFO     | openai_serving       | Still waiting (50s has elapsed)...
[92m12:12:07 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:07 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:12:07 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:07 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 482.60 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 529.45 examples/s]
2025-08-29 12:12:07 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_th.jsonl
2025-08-29 12:12:07 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:07 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:12:07 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: ABSSUM ----------
                                                      | Testing Competency: NLG
2025-08-29 12:12:07 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/abstractive_summarization/data/th_xlsum_100sample.jsonl
2025-08-29 12:12:07 | INFO     | seahelm_evaluation   | Performing inference for task 'ABSSUM' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:07 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.44 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.46 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.16 examples/s]
[92m12:12:08 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:08 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:08 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:08 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:08 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:08 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:08 | INFO     | openai_serving       | Still waiting (50s has elapsed)...
[92m12:12:08 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:08 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:08 | INFO     | openai_serving       | Still waiting (50s has elapsed)...
[92m12:12:08 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:08 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:12:10 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:10 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:11 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:11 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:11 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:11 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 394.04 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 511.00 examples/s]
2025-08-29 12:12:11 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_th.jsonl
2025-08-29 12:12:11 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:11 | INFO     | seahelm_evaluation   | Inference for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:12:11 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: ABSSUM ----------
2025-08-29 12:12:11 | INFO     | seahelm_evaluation   | Evaluating 'ABSSUM' using SummarizationMetric
2025-08-29 12:12:11 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:11 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:11 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:11 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:12:11 | INFO     | summarization        | Rouge-L Scores:
2025-08-29 12:12:11 | INFO     | summarization        | Precision: 0.00 | Recall: 0.00 | F1: 0.00
2025-08-29 12:12:11 | INFO     | summarization        | Norm F1 Score: 0.00
2025-08-29 12:12:11 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:11 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_th.jsonl
2025-08-29 12:12:11 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:11 | INFO     | seahelm_evaluation   | Evaluation for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:12:11 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: CAUSAL ----------
                                                      | Testing Competency: NLR
2025-08-29 12:12:11 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/causal/data/th_xcopa.jsonl
2025-08-29 12:12:11 | INFO     | seahelm_evaluation   | Performing inference for task 'CAUSAL' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:11 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.43 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  7.03 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.86 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.15 examples/s]
[92m12:12:12 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:12 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:12 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:12 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:12 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:12 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:13 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:13 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:13 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:13 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:13 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:13 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 372.69 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 493.91 examples/s]
2025-08-29 12:12:13 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_causal_th.jsonl
2025-08-29 12:12:13 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:13 | INFO     | seahelm_evaluation   | Inference for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:12:13 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: CAUSAL ----------
2025-08-29 12:12:13 | INFO     | seahelm_evaluation   | Evaluating 'CAUSAL' using CausalReasoningMetric
2025-08-29 12:12:13 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:13 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:13 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:13 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:12:13 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:12:13 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 0 2]
                                                      |  [0 0 1]
                                                      |  [0 0 0]]
2025-08-29 12:12:13 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       2.0
                                                      |            1       0.00      0.00      0.00       1.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       3.0
                                                      |    macro avg       0.00      0.00      0.00       3.0
                                                      | weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:12:13 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:13 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_causal_th.jsonl
2025-08-29 12:12:13 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:13 | INFO     | seahelm_evaluation   | Evaluation for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:12:13 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: NLI ----------
                                                      | Testing Competency: NLR
2025-08-29 12:12:13 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/nli/data/th_xnli_1000sample.jsonl
2025-08-29 12:12:13 | INFO     | seahelm_evaluation   | Performing inference for task 'NLI' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:13 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.42 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  7.00 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.81 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.14 examples/s]
[92m12:12:14 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:14 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:14 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:14 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:14 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:14 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:14 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:14 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:14 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:14 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:15 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:15 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 441.74 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 517.86 examples/s]
2025-08-29 12:12:15 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_nli_th.jsonl
2025-08-29 12:12:15 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:15 | INFO     | seahelm_evaluation   | Inference for task 'NLI' completed!
                                                      | 
2025-08-29 12:12:15 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: NLI ----------
2025-08-29 12:12:15 | INFO     | seahelm_evaluation   | Evaluating 'NLI' using NLIMetric
2025-08-29 12:12:15 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:15 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:15 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:15 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:12:15 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:12:15 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 3]
                                                      |  [0 0]]
2025-08-29 12:12:15 | INFO     | seahelm_metric       | Classification report:
                                                      |                precision    recall  f1-score   support
                                                      | 
                                                      | contradiction       0.00      0.00      0.00       3.0
                                                      |          none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |      accuracy                           0.00       3.0
                                                      |     macro avg       0.00      0.00      0.00       3.0
                                                      |  weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:12:15 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:15 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_nli_th.jsonl
2025-08-29 12:12:15 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:15 | INFO     | seahelm_evaluation   | Evaluation for task 'NLI' completed!
                                                      | 
2025-08-29 12:12:15 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TH | Task: IF-EVAL ----------
                                                      | Testing Competency: INSTRUCTION-FOLLOWING
2025-08-29 12:12:15 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/instruction_following/ifeval/data/th_sea_ifeval.jsonl
2025-08-29 12:12:15 | INFO     | seahelm_evaluation   | Performing inference for task 'IF-EVAL' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:15 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.52 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.78 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.83 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.09 examples/s]
[92m12:12:15 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:15 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:15 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:15 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:15 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:15 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:17 | INFO     | openai_serving       | Still waiting (60s has elapsed)...
[92m12:12:17 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:17 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:17 | INFO     | openai_serving       | Still waiting (60s has elapsed)...
[92m12:12:17 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:17 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:19 | INFO     | openai_serving       | Still waiting (60s has elapsed)...
[92m12:12:19 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:19 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:19 | INFO     | openai_serving       | Still waiting (60s has elapsed)...
[92m12:12:19 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:19 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:12:20 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:20 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:21 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:21 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:21 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:21 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 250.41 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 308.44 examples/s]
2025-08-29 12:12:21 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_th.jsonl
2025-08-29 12:12:21 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:21 | INFO     | seahelm_evaluation   | Inference for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 12:12:21 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: IF-EVAL ----------
2025-08-29 12:12:21 | INFO     | seahelm_evaluation   | Evaluating 'IF-EVAL' using IFEvalMetric
2025-08-29 12:12:21 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:21 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:21 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:21 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:12:21 | INFO     | if_eval              | Overall pass: 2 / 3
2025-08-29 12:12:21 | INFO     | if_eval              | Overall accuracy: 66.666667
2025-08-29 12:12:21 | INFO     | if_eval              | Correct language rate: 1.000000
2025-08-29 12:12:21 | INFO     | if_eval              | Lang normalized accuracy: 66.666667
2025-08-29 12:12:21 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:21 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_th.jsonl
2025-08-29 12:12:21 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:21 | INFO     | seahelm_evaluation   | Evaluation for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 12:12:21 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: SENTIMENT ----------
                                                      | Testing Competency: NLU
2025-08-29 12:12:21 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/sentiment_analysis/data/ta_indicsentiment.jsonl
2025-08-29 12:12:21 | INFO     | seahelm_evaluation   | Performing inference for task 'SENTIMENT' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:21 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.44 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.92 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.91 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.17 examples/s]
[92m12:12:22 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:22 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:22 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:22 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:22 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:22 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:22 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:22 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:22 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:22 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:22 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:22 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 458.01 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 502.67 examples/s]
2025-08-29 12:12:22 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_ta.jsonl
2025-08-29 12:12:22 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:22 | INFO     | seahelm_evaluation   | Inference for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:12:22 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: SENTIMENT ----------
2025-08-29 12:12:22 | INFO     | seahelm_evaluation   | Evaluating 'SENTIMENT' using SentimentAnalysisMetric
2025-08-29 12:12:22 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:22 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:22 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:22 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:12:22 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:12:22 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 1 0]
                                                      |  [0 0 0]
                                                      |  [0 2 0]]
2025-08-29 12:12:22 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |     negative       0.00      0.00      0.00       1.0
                                                      |         none       0.00      0.00      0.00       0.0
                                                      |     positive       0.00      0.00      0.00       2.0
                                                      | 
                                                      |     accuracy                           0.00       3.0
                                                      |    macro avg       0.00      0.00      0.00       3.0
                                                      | weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:12:22 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:22 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_ta.jsonl
2025-08-29 12:12:22 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:22 | INFO     | seahelm_evaluation   | Evaluation for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:12:22 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: QA ----------
                                                      | Testing Competency: NLU
2025-08-29 12:12:22 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/question_answering/data/ta_indicqa_100sample.jsonl
2025-08-29 12:12:22 | INFO     | seahelm_evaluation   | Performing inference for task 'QA' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:22 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.39 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.87 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.85 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.13 examples/s]
[92m12:12:23 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:23 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:23 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:23 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:23 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:23 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:24 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:24 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:24 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:24 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:24 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:24 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 404.39 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 529.72 examples/s]
2025-08-29 12:12:24 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_qa_ta.jsonl
2025-08-29 12:12:24 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:24 | INFO     | seahelm_evaluation   | Inference for task 'QA' completed!
                                                      | 
2025-08-29 12:12:24 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: QA ----------
2025-08-29 12:12:24 | INFO     | seahelm_evaluation   | Evaluating 'QA' using QuestionAnsweringMetric
2025-08-29 12:12:24 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:24 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:24 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:24 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:12:24 | INFO     | question_answering   | {'exact_match': 66.66666666666667, 'f1': 66.66666666666667, 'normalized_f1': 66.66666666666667}
2025-08-29 12:12:24 | INFO     | question_answering   | 2 answers out of 3 (66.67%) can be found in the model's predictions.
2025-08-29 12:12:24 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:24 | INFO     | seahelm_evaluation   | Saving inference results for task 'QA' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_qa_ta.jsonl
2025-08-29 12:12:24 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:25 | INFO     | seahelm_evaluation   | Evaluation for task 'QA' completed!
                                                      | 
2025-08-29 12:12:25 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: TRANSLATION-EN-XX ----------
                                                      | Testing Competency: NLG
2025-08-29 12:12:25 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/en_to_tam_Taml.jsonl
2025-08-29 12:12:25 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-EN-XX' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:25 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.44 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  7.03 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.79 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.13 examples/s]
[92m12:12:25 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:25 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:25 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:25 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:25 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:25 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:26 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:26 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:27 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:27 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:27 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:27 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 485.02 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 534.40 examples/s]
2025-08-29 12:12:27 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_ta.jsonl
2025-08-29 12:12:27 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:27 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:12:27 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: TRANSLATION-XX-EN ----------
                                                      | Testing Competency: NLG
2025-08-29 12:12:27 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/tam_Taml_to_en.jsonl
2025-08-29 12:12:27 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-XX-EN' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:27 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.11 examples/s]2025-08-29 12:12:27 | INFO     | openai_serving       | Still waiting (70s has elapsed)...
[92m12:12:27 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:27 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.73 examples/s]2025-08-29 12:12:27 | INFO     | openai_serving       | Still waiting (70s has elapsed)...
[92m12:12:27 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:27 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.64 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.00 examples/s]
[92m12:12:28 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:28 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:28 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:28 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:28 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:28 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:29 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:29 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:29 | INFO     | openai_serving       | Still waiting (70s has elapsed)...
[92m12:12:29 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:29 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:29 | INFO     | openai_serving       | Still waiting (70s has elapsed)...
[92m12:12:29 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:29 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:12:29 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:29 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:29 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:29 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 488.98 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 516.33 examples/s]
2025-08-29 12:12:29 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_ta.jsonl
2025-08-29 12:12:29 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:29 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:12:29 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: ABSSUM ----------
                                                      | Testing Competency: NLG
2025-08-29 12:12:29 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/abstractive_summarization/data/ta_xlsum_100sample.jsonl
2025-08-29 12:12:29 | INFO     | seahelm_evaluation   | Performing inference for task 'ABSSUM' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:29 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.30 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.89 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.67 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.03 examples/s]
[92m12:12:30 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:30 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:30 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:30 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:30 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:30 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:32 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:32 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:32 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:32 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:32 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:32 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 393.65 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 510.13 examples/s]
2025-08-29 12:12:32 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_ta.jsonl
2025-08-29 12:12:32 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:32 | INFO     | seahelm_evaluation   | Inference for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:12:32 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: ABSSUM ----------
2025-08-29 12:12:32 | INFO     | seahelm_evaluation   | Evaluating 'ABSSUM' using SummarizationMetric
2025-08-29 12:12:32 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:32 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:32 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:32 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:12:32 | INFO     | summarization        | Rouge-L Scores:
2025-08-29 12:12:32 | INFO     | summarization        | Precision: 0.00 | Recall: 0.00 | F1: 0.00
2025-08-29 12:12:32 | INFO     | summarization        | Norm F1 Score: 0.00
2025-08-29 12:12:32 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:32 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_ta.jsonl
2025-08-29 12:12:32 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:32 | INFO     | seahelm_evaluation   | Evaluation for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:12:32 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: CAUSAL ----------
                                                      | Testing Competency: NLR
2025-08-29 12:12:32 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/causal/data/ta_xcopa.jsonl
2025-08-29 12:12:32 | INFO     | seahelm_evaluation   | Performing inference for task 'CAUSAL' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:32 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.37 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.88 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.81 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.10 examples/s]
[92m12:12:33 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:33 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:33 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:33 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:33 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:33 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:34 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:34 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:34 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:34 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:34 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:34 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 393.56 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 504.75 examples/s]
2025-08-29 12:12:34 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_causal_ta.jsonl
2025-08-29 12:12:34 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:34 | INFO     | seahelm_evaluation   | Inference for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:12:34 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: CAUSAL ----------
2025-08-29 12:12:34 | INFO     | seahelm_evaluation   | Evaluating 'CAUSAL' using CausalReasoningMetric
2025-08-29 12:12:34 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:34 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:34 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:34 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:12:34 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:12:34 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 0 2]
                                                      |  [0 0 1]
                                                      |  [0 0 0]]
2025-08-29 12:12:34 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       2.0
                                                      |            1       0.00      0.00      0.00       1.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       3.0
                                                      |    macro avg       0.00      0.00      0.00       3.0
                                                      | weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:12:34 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:34 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_causal_ta.jsonl
2025-08-29 12:12:34 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:34 | INFO     | seahelm_evaluation   | Evaluation for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:12:34 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: NLI ----------
                                                      | Testing Competency: NLR
2025-08-29 12:12:34 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/nli/data/ta_xnli_1000sample.jsonl
2025-08-29 12:12:34 | INFO     | seahelm_evaluation   | Performing inference for task 'NLI' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:34 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.36 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.95 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.73 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.09 examples/s]
[92m12:12:35 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:35 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:35 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:35 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:35 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:35 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:36 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:36 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:36 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:36 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:36 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:36 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 466.60 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 505.62 examples/s]
2025-08-29 12:12:36 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_nli_ta.jsonl
2025-08-29 12:12:36 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:36 | INFO     | seahelm_evaluation   | Inference for task 'NLI' completed!
                                                      | 
2025-08-29 12:12:36 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: NLI ----------
2025-08-29 12:12:36 | INFO     | seahelm_evaluation   | Evaluating 'NLI' using NLIMetric
2025-08-29 12:12:36 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:36 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:36 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:36 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:12:36 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:12:36 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 3]
                                                      |  [0 0]]
2025-08-29 12:12:36 | INFO     | seahelm_metric       | Classification report:
                                                      |                precision    recall  f1-score   support
                                                      | 
                                                      | contradiction       0.00      0.00      0.00       3.0
                                                      |          none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |      accuracy                           0.00       3.0
                                                      |     macro avg       0.00      0.00      0.00       3.0
                                                      |  weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:12:36 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:36 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_nli_ta.jsonl
2025-08-29 12:12:36 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:36 | INFO     | seahelm_evaluation   | Evaluation for task 'NLI' completed!
                                                      | 
2025-08-29 12:12:36 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: MP-R ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 12:12:36 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/syntax/data/ta_syntax_mcq_randomized.jsonl
2025-08-29 12:12:36 | INFO     | seahelm_evaluation   | Performing inference for task 'MP-R' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:36 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.43 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.98 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.79 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.12 examples/s]
[92m12:12:37 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:37 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:37 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:37 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:37 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:37 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:38 | INFO     | openai_serving       | Still waiting (80s has elapsed)...
[92m12:12:38 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:38 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:38 | INFO     | openai_serving       | Still waiting (80s has elapsed)...
[92m12:12:38 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:38 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:12:38 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:38 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:38 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:38 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:38 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:38 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 362.47 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 455.61 examples/s]
2025-08-29 12:12:38 | INFO     | seahelm_evaluation   | Saving inference results for task 'MP-R' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_mp-r_ta.jsonl
2025-08-29 12:12:38 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:38 | INFO     | seahelm_evaluation   | Inference for task 'MP-R' completed!
                                                      | 
2025-08-29 12:12:38 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: MP-R ----------
2025-08-29 12:12:38 | INFO     | seahelm_evaluation   | Evaluating 'MP-R' using MinimalPairsMetric
2025-08-29 12:12:38 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:38 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:38 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:38 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:12:38 | INFO     | minimal_pairs        | Accuracy for phenomenon <argument_structure>: 0.0
2025-08-29 12:12:38 | INFO     | minimal_pairs        | Overall Accuracy: 0.0
2025-08-29 12:12:38 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:38 | INFO     | seahelm_evaluation   | Saving inference results for task 'MP-R' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_mp-r_ta.jsonl
2025-08-29 12:12:38 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:38 | INFO     | seahelm_evaluation   | Evaluation for task 'MP-R' completed!
                                                      | 
2025-08-29 12:12:38 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: PRAGMATIC-SINGLE ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 12:12:38 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/pragmatics/data/ta_pragmatic_reasoning_single.jsonl
2025-08-29 12:12:38 | INFO     | seahelm_evaluation   | Performing inference for task 'PRAGMATIC-SINGLE' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:38 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.36 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.97 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.82 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.11 examples/s]
[92m12:12:38 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:38 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:38 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:38 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:38 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:38 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:39 | INFO     | openai_serving       | Still waiting (80s has elapsed)...
[92m12:12:39 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:39 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:39 | INFO     | openai_serving       | Still waiting (80s has elapsed)...
[92m12:12:39 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:39 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:12:39 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:39 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:39 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:39 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:39 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:39 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 354.24 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 449.28 examples/s]
2025-08-29 12:12:39 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-SINGLE' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-single_ta.jsonl
2025-08-29 12:12:39 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:39 | INFO     | seahelm_evaluation   | Inference for task 'PRAGMATIC-SINGLE' completed!
                                                      | 
2025-08-29 12:12:39 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: PRAGMATIC-SINGLE ----------
2025-08-29 12:12:39 | INFO     | seahelm_evaluation   | Evaluating 'PRAGMATIC-SINGLE' using PragmaticReasoningSingleSentenceMetric
2025-08-29 12:12:39 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:39 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:39 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:39 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  subset_references = subset[self.label_column].replace(
2025-08-29 12:12:39 | INFO     | pragmatic_reasoning  | Accuracy for phenomenon <scalar_implicatures>: 0.0 / 3
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:93: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  references = self.inference_df[self.label_column].replace(
2025-08-29 12:12:39 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:39 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-SINGLE' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-single_ta.jsonl
2025-08-29 12:12:39 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:39 | INFO     | seahelm_evaluation   | Evaluation for task 'PRAGMATIC-SINGLE' completed!
                                                      | 
2025-08-29 12:12:39 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TA | Task: PRAGMATIC-PAIR ----------
                                                      | Testing Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 12:12:39 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/lindsea/pragmatics/data/ta_pragmatic_reasoning_pair.jsonl
2025-08-29 12:12:39 | INFO     | seahelm_evaluation   | Performing inference for task 'PRAGMATIC-PAIR' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:39 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.40 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.80 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.95 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.17 examples/s]
[92m12:12:40 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:40 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:40 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:40 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:40 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:40 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:41 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:41 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:41 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:41 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:41 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:41 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 389.32 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 503.30 examples/s]
2025-08-29 12:12:41 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-PAIR' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-pair_ta.jsonl
2025-08-29 12:12:41 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:41 | INFO     | seahelm_evaluation   | Inference for task 'PRAGMATIC-PAIR' completed!
                                                      | 
2025-08-29 12:12:41 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: PRAGMATIC-PAIR ----------
2025-08-29 12:12:41 | INFO     | seahelm_evaluation   | Evaluating 'PRAGMATIC-PAIR' using PragmaticReasoningSentencePairMetric
2025-08-29 12:12:41 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:41 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:41 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:41 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:143: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  subset_references = subset[self.label_column].replace({True: 1, False: 0})
2025-08-29 12:12:41 | INFO     | pragmatic_reasoning  | Accuracy for phenomenon <scalar_implicatures>: 0.0 / 3
/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/lindsea/pragmatics/pragmatic_reasoning.py:157: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  references = self.inference_df[self.label_column].replace({True: 1, False: 0})
2025-08-29 12:12:41 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:41 | INFO     | seahelm_evaluation   | Saving inference results for task 'PRAGMATIC-PAIR' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_pragmatic-pair_ta.jsonl
2025-08-29 12:12:41 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:41 | INFO     | seahelm_evaluation   | Evaluation for task 'PRAGMATIC-PAIR' completed!
                                                      | 
2025-08-29 12:12:41 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: SENTIMENT ----------
                                                      | Testing Competency: NLU
2025-08-29 12:12:41 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/sentiment_analysis/data/tl_elections_sentiment.jsonl
2025-08-29 12:12:41 | INFO     | seahelm_evaluation   | Performing inference for task 'SENTIMENT' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:41 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.40 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  7.01 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.84 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.14 examples/s]
[92m12:12:42 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:42 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:42 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:42 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:42 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:42 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:42 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:42 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:42 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:42 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:42 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:42 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 468.15 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 507.25 examples/s]
2025-08-29 12:12:42 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_tl.jsonl
2025-08-29 12:12:42 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:42 | INFO     | seahelm_evaluation   | Inference for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:12:42 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: SENTIMENT ----------
2025-08-29 12:12:42 | INFO     | seahelm_evaluation   | Evaluating 'SENTIMENT' using SentimentAnalysisMetric
2025-08-29 12:12:42 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:42 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:42 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:42 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:12:42 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:12:42 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 0 1]
                                                      |  [0 0 2]
                                                      |  [0 0 0]]
2025-08-29 12:12:42 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |     negative       0.00      0.00      0.00       1.0
                                                      |      neutral       0.00      0.00      0.00       2.0
                                                      |         none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       3.0
                                                      |    macro avg       0.00      0.00      0.00       3.0
                                                      | weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:12:42 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:42 | INFO     | seahelm_evaluation   | Saving inference results for task 'SENTIMENT' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_sentiment_tl.jsonl
2025-08-29 12:12:42 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:42 | INFO     | seahelm_evaluation   | Evaluation for task 'SENTIMENT' completed!
                                                      | 
2025-08-29 12:12:42 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: BELEBELE-QA-MC ----------
                                                      | Testing Competency: NLU
2025-08-29 12:12:42 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlu/belebele_mcqa/data/eval/tl_belebele.jsonl
2025-08-29 12:12:42 | INFO     | seahelm_evaluation   | Performing inference for task 'BELEBELE-QA-MC' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:42 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.36 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.88 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.87 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.13 examples/s]
[92m12:12:43 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:43 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:43 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:43 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:43 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:43 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:43 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:43 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:44 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:44 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:44 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:44 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 381.98 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 486.07 examples/s]
2025-08-29 12:12:44 | INFO     | seahelm_evaluation   | Saving inference results for task 'BELEBELE-QA-MC' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_belebele-qa-mc_tl.jsonl
2025-08-29 12:12:44 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:44 | INFO     | seahelm_evaluation   | Inference for task 'BELEBELE-QA-MC' completed!
                                                      | 
2025-08-29 12:12:44 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: BELEBELE-QA-MC ----------
2025-08-29 12:12:44 | INFO     | seahelm_evaluation   | Evaluating 'BELEBELE-QA-MC' using QuestionAnsweringMultipleChoiceMetric
2025-08-29 12:12:44 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:44 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:44 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:44 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:12:44 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:12:44 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 0 0 1]
                                                      |  [0 0 0 1]
                                                      |  [0 0 0 1]
                                                      |  [0 0 0 0]]
2025-08-29 12:12:44 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       1.0
                                                      |            1       0.00      0.00      0.00       1.0
                                                      |            3       0.00      0.00      0.00       1.0
                                                      |            4       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       3.0
                                                      |    macro avg       0.00      0.00      0.00       3.0
                                                      | weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:12:44 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:44 | INFO     | seahelm_evaluation   | Saving inference results for task 'BELEBELE-QA-MC' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_belebele-qa-mc_tl.jsonl
2025-08-29 12:12:44 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:44 | INFO     | seahelm_evaluation   | Evaluation for task 'BELEBELE-QA-MC' completed!
                                                      | 
2025-08-29 12:12:44 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: TOXICITY ----------
                                                      | Testing Competency: SAFETY
2025-08-29 12:12:44 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/safety/toxicity_detection/data/tl_elections_hsd.jsonl
2025-08-29 12:12:44 | INFO     | seahelm_evaluation   | Performing inference for task 'TOXICITY' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:44 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.02 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.85 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.19 examples/s]
[92m12:12:44 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:44 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:44 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:44 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:44 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:44 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:45 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:45 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:45 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:45 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:45 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:45 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 466.45 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 503.90 examples/s]
2025-08-29 12:12:45 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_tl.jsonl
2025-08-29 12:12:45 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:45 | INFO     | seahelm_evaluation   | Inference for task 'TOXICITY' completed!
                                                      | 
2025-08-29 12:12:45 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: TOXICITY ----------
2025-08-29 12:12:45 | INFO     | seahelm_evaluation   | Evaluating 'TOXICITY' using ToxicityDetectionMetric
2025-08-29 12:12:45 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:45 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:45 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:45 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:12:45 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:12:45 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 0 2]
                                                      |  [0 0 1]
                                                      |  [0 0 0]]
2025-08-29 12:12:45 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       2.0
                                                      |            1       0.00      0.00      0.00       1.0
                                                      |            3       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       3.0
                                                      |    macro avg       0.00      0.00      0.00       3.0
                                                      | weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:12:45 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:45 | INFO     | seahelm_evaluation   | Saving inference results for task 'TOXICITY' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_toxicity_tl.jsonl
2025-08-29 12:12:45 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:45 | INFO     | seahelm_evaluation   | Evaluation for task 'TOXICITY' completed!
                                                      | 
2025-08-29 12:12:45 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: TRANSLATION-EN-XX ----------
                                                      | Testing Competency: NLG
2025-08-29 12:12:45 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/en_to_tgl_Latn.jsonl
2025-08-29 12:12:45 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-EN-XX' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:45 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.42 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  7.02 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.87 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.16 examples/s]
[92m12:12:46 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:46 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:46 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:46 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:46 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:46 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:47 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:47 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:47 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:47 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:47 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:47 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 464.83 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 508.05 examples/s]
2025-08-29 12:12:47 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_tl.jsonl
2025-08-29 12:12:47 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:47 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:12:47 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: TRANSLATION-XX-EN ----------
                                                      | Testing Competency: NLG
2025-08-29 12:12:47 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/translation/data/flores200_dataset/devtest/tgl_Latn_to_en.jsonl
2025-08-29 12:12:47 | INFO     | seahelm_evaluation   | Performing inference for task 'TRANSLATION-XX-EN' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:47 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.38 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.81 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.78 examples/s]2025-08-29 12:12:48 | INFO     | openai_serving       | Still waiting (90s has elapsed)...
[92m12:12:48 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:48 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:48 | INFO     | openai_serving       | Still waiting (90s has elapsed)...
[92m12:12:48 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:48 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.08 examples/s]
[92m12:12:48 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:48 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:48 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:48 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:48 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:48 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:49 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:49 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:49 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:49 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:49 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:49 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 471.80 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 509.64 examples/s]
2025-08-29 12:12:49 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_tl.jsonl
2025-08-29 12:12:49 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:49 | INFO     | seahelm_evaluation   | Inference for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:12:49 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: ABSSUM ----------
                                                      | Testing Competency: NLG
2025-08-29 12:12:49 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlg/abstractive_summarization/data/tl_xlsum.jsonl
2025-08-29 12:12:49 | INFO     | seahelm_evaluation   | Performing inference for task 'ABSSUM' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:49 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:49 | INFO     | openai_serving       | Still waiting (90s has elapsed)...
[92m12:12:49 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:49 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:49 | INFO     | openai_serving       | Still waiting (90s has elapsed)...
[92m12:12:49 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:49 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.13 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.56 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.47 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.88 examples/s]
[92m12:12:50 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:50 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:50 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:50 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:50 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:50 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:52 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:52 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:52 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:52 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:53 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:53 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 396.84 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 521.64 examples/s]
2025-08-29 12:12:53 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_tl.jsonl
2025-08-29 12:12:53 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:53 | INFO     | seahelm_evaluation   | Inference for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:12:53 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: ABSSUM ----------
2025-08-29 12:12:53 | INFO     | seahelm_evaluation   | Evaluating 'ABSSUM' using SummarizationMetric
2025-08-29 12:12:53 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:53 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:53 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:53 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:12:53 | INFO     | summarization        | Rouge-L Scores:
2025-08-29 12:12:53 | INFO     | summarization        | Precision: 20.71 | Recall: 17.04 | F1: 10.27
2025-08-29 12:12:53 | INFO     | summarization        | Norm F1 Score: 10.27
2025-08-29 12:12:53 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:53 | INFO     | seahelm_evaluation   | Saving inference results for task 'ABSSUM' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_abssum_tl.jsonl
2025-08-29 12:12:53 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:53 | INFO     | seahelm_evaluation   | Evaluation for task 'ABSSUM' completed!
                                                      | 
2025-08-29 12:12:53 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: CAUSAL ----------
                                                      | Testing Competency: NLR
2025-08-29 12:12:53 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/causal/data/tl_balanced_copa.jsonl
2025-08-29 12:12:53 | INFO     | seahelm_evaluation   | Performing inference for task 'CAUSAL' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:53 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  4.80 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.88 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.13 examples/s]
[92m12:12:54 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:54 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:54 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:54 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:54 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:54 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:55 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:55 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:55 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:55 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:55 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:55 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 388.16 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 490.41 examples/s]
2025-08-29 12:12:55 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_causal_tl.jsonl
2025-08-29 12:12:55 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:55 | INFO     | seahelm_evaluation   | Inference for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:12:55 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: CAUSAL ----------
2025-08-29 12:12:55 | INFO     | seahelm_evaluation   | Evaluating 'CAUSAL' using CausalReasoningMetric
2025-08-29 12:12:55 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:55 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:55 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:55 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:12:55 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:12:55 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 0 2]
                                                      |  [0 0 1]
                                                      |  [0 0 0]]
2025-08-29 12:12:55 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            0       0.00      0.00      0.00       2.0
                                                      |            1       0.00      0.00      0.00       1.0
                                                      |            2       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       3.0
                                                      |    macro avg       0.00      0.00      0.00       3.0
                                                      | weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:12:55 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:55 | INFO     | seahelm_evaluation   | Saving inference results for task 'CAUSAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_causal_tl.jsonl
2025-08-29 12:12:55 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:55 | INFO     | seahelm_evaluation   | Evaluation for task 'CAUSAL' completed!
                                                      | 
2025-08-29 12:12:55 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: NLI ----------
                                                      | Testing Competency: NLR
2025-08-29 12:12:55 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/nlr/nli/data/tl_xnli.jsonl
2025-08-29 12:12:55 | INFO     | seahelm_evaluation   | Performing inference for task 'NLI' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:55 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.41 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.40 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.05 examples/s]
[92m12:12:56 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:56 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:56 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:56 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:56 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:56 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:57 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:57 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:57 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:57 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:12:57 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:12:57 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 423.81 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 545.52 examples/s]
2025-08-29 12:12:57 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_nli_tl.jsonl
2025-08-29 12:12:57 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:57 | INFO     | seahelm_evaluation   | Inference for task 'NLI' completed!
                                                      | 
2025-08-29 12:12:57 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: NLI ----------
2025-08-29 12:12:57 | INFO     | seahelm_evaluation   | Evaluating 'NLI' using NLIMetric
2025-08-29 12:12:57 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:12:57 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:12:57 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:12:57 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:12:57 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:12:57 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 3]
                                                      |  [0 0]]
2025-08-29 12:12:57 | INFO     | seahelm_metric       | Classification report:
                                                      |                precision    recall  f1-score   support
                                                      | 
                                                      | contradiction       0.00      0.00      0.00       3.0
                                                      |          none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |      accuracy                           0.00       3.0
                                                      |     macro avg       0.00      0.00      0.00       3.0
                                                      |  weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:12:57 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:12:57 | INFO     | seahelm_evaluation   | Saving inference results for task 'NLI' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_nli_tl.jsonl
2025-08-29 12:12:57 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:12:57 | INFO     | seahelm_evaluation   | Evaluation for task 'NLI' completed!
                                                      | 
2025-08-29 12:12:57 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: IF-EVAL ----------
                                                      | Testing Competency: INSTRUCTION-FOLLOWING
2025-08-29 12:12:57 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/instruction_following/ifeval/data/tl_sea_ifeval.jsonl
2025-08-29 12:12:57 | INFO     | seahelm_evaluation   | Performing inference for task 'IF-EVAL' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:12:57 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.51 examples/s]Map (num_proc=3):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.57 examples/s]2025-08-29 12:12:58 | INFO     | openai_serving       | Still waiting (100s has elapsed)...
[92m12:12:58 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:58 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:58 | INFO     | openai_serving       | Still waiting (100s has elapsed)...
Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.07 examples/s]
[92m12:12:58 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:58 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:12:58 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:58 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:58 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:12:58 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:58 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:58 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:12:59 | INFO     | openai_serving       | Still waiting (100s has elapsed)...
[92m12:12:59 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:59 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:59 | INFO     | openai_serving       | Still waiting (100s has elapsed)...
[92m12:12:59 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:12:59 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:13:02 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:13:02 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:13:03 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:13:03 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:13:04 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:13:04 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 256.16 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 309.28 examples/s]
2025-08-29 12:13:04 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_tl.jsonl
2025-08-29 12:13:04 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:13:04 | INFO     | seahelm_evaluation   | Inference for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 12:13:04 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: IF-EVAL ----------
2025-08-29 12:13:04 | INFO     | seahelm_evaluation   | Evaluating 'IF-EVAL' using IFEvalMetric
2025-08-29 12:13:04 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:13:04 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:13:04 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:13:04 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:13:04 | INFO     | if_eval              | Overall pass: 2 / 3
2025-08-29 12:13:04 | INFO     | if_eval              | Overall accuracy: 66.666667
2025-08-29 12:13:04 | INFO     | if_eval              | Correct language rate: 1.000000
2025-08-29 12:13:04 | INFO     | if_eval              | Lang normalized accuracy: 66.666667
2025-08-29 12:13:04 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:13:04 | INFO     | seahelm_evaluation   | Saving inference results for task 'IF-EVAL' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_if-eval_tl.jsonl
2025-08-29 12:13:04 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:13:04 | INFO     | seahelm_evaluation   | Evaluation for task 'IF-EVAL' completed!
                                                      | 
2025-08-29 12:13:04 | INFO     | seahelm_evaluation   | ---------- Inference | Lang: TL | Task: KALAHI-MC ----------
                                                      | Testing Competency: CULTURAL
2025-08-29 12:13:04 | INFO     | seahelm_evaluation   | Drawing and preparing instances from seahelm_tasks/cultural/kalahi/data/tl_kalahi_mc.jsonl
2025-08-29 12:13:04 | INFO     | seahelm_evaluation   | Performing inference for task 'KALAHI-MC' with 0 examples
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
2025-08-29 12:13:04 | WARNING  | arrow_dataset        | num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.14 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.88 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.23 examples/s]
[92m12:13:05 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:13:05 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:13:05 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:13:05 - LiteLLM:INFO[0m: utils.py:3341 - 
LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:13:05 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
2025-08-29 12:13:05 | INFO     | utils                | 
                                                      | LiteLLM completion() model= gemini-2.5-flash; provider = vertex_ai
[92m12:13:05 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:13:05 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:13:05 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:13:05 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
[92m12:13:05 - LiteLLM:INFO[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler
2025-08-29 12:13:05 | INFO     | utils                | Wrapper: Completed Call, calling success_handler
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 379.79 examples/s]
Map:   0%|          | 0/3 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 496.94 examples/s]
2025-08-29 12:13:05 | INFO     | seahelm_evaluation   | Saving inference results for task 'KALAHI-MC' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_kalahi-mc_tl.jsonl
2025-08-29 12:13:05 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:13:05 | INFO     | seahelm_evaluation   | Inference for task 'KALAHI-MC' completed!
                                                      | 
2025-08-29 12:13:05 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: KALAHI-MC ----------
2025-08-29 12:13:05 | INFO     | seahelm_evaluation   | Evaluating 'KALAHI-MC' using KalahiMCMetric
2025-08-29 12:13:05 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:13:05 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:13:05 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:13:05 | INFO     | seahelm_metric       | Calculating metrics...
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2025-08-29 12:13:05 | INFO     | seahelm_metric       | Balanced Acc = 0.00 | Macro-F1 = 0.00 | Null-Weighted-F1 = 0.00
2025-08-29 12:13:05 | INFO     | seahelm_metric       | Confusion matrix:
                                                      | [[0 0 0 1]
                                                      |  [0 0 0 1]
                                                      |  [0 0 0 1]
                                                      |  [0 0 0 0]]
2025-08-29 12:13:05 | INFO     | seahelm_metric       | Classification report:
                                                      |               precision    recall  f1-score   support
                                                      | 
                                                      |            A       0.00      0.00      0.00       1.0
                                                      |            B       0.00      0.00      0.00       1.0
                                                      |            C       0.00      0.00      0.00       1.0
                                                      |         none       0.00      0.00      0.00       0.0
                                                      | 
                                                      |     accuracy                           0.00       3.0
                                                      |    macro avg       0.00      0.00      0.00       3.0
                                                      | weighted avg       0.00      0.00      0.00       3.0
                                                      | 
2025-08-29 12:13:05 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:13:05 | INFO     | seahelm_evaluation   | Saving inference results for task 'KALAHI-MC' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_kalahi-mc_tl.jsonl
2025-08-29 12:13:05 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:13:05 | INFO     | seahelm_evaluation   | Evaluation for task 'KALAHI-MC' completed!
                                                      | 
2025-08-29 12:13:05 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: TRANSLATION-EN-XX ----------
2025-08-29 12:13:05 | WARNING  | translation          | COMET not installed. Please install COMET to use the COMET metrics.
2025-08-29 12:13:05 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-EN-XX' using TranslationMetric
2025-08-29 12:13:05 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:13:05 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:13:05 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:13:05 | INFO     | seahelm_metric       | Calculating metrics...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.72it/s]2025-08-29 12:13:08 | INFO     | openai_serving       | Still waiting (110s has elapsed)...
[92m12:13:08 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:08 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:08 | INFO     | openai_serving       | Still waiting (110s has elapsed)...
[92m12:13:08 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:08 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.78it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.90it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.86it/s]
2025-08-29 12:13:10 | INFO     | openai_serving       | Still waiting (110s has elapsed)...
[92m12:13:10 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:10 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:10 | INFO     | openai_serving       | Still waiting (110s has elapsed)...
[92m12:13:10 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:10 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
2025-08-29 12:13:17 | INFO     | translation          | MetricX WMT24 score: 19.416667
2025-08-29 12:13:18 | INFO     | translation          | MetricX WMT24 with references score: 24.833333
2025-08-29 12:13:18 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:13:18 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_id.jsonl
2025-08-29 12:13:18 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:13:18 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:13:18 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: ID | Task: TRANSLATION-XX-EN ----------
2025-08-29 12:13:18 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-XX-EN' using TranslationMetric
2025-08-29 12:13:18 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:13:18 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:13:18 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:13:18 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:13:18 | INFO     | openai_serving       | Still waiting (120s has elapsed)...
[92m12:13:18 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:18 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:18 | INFO     | openai_serving       | Still waiting (120s has elapsed)...
[92m12:13:18 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:18 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:20 | INFO     | openai_serving       | Still waiting (120s has elapsed)...
[92m12:13:20 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:20 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-08-29 12:13:20 | INFO     | openai_serving       | Still waiting (120s has elapsed)...
[92m12:13:20 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:20 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.89it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.86it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.95it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.93it/s]
2025-08-29 12:13:29 | INFO     | translation          | MetricX WMT24 score: 15.083333
2025-08-29 12:13:29 | INFO     | openai_serving       | Still waiting (130s has elapsed)...
[92m12:13:29 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:29 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:29 | INFO     | openai_serving       | Still waiting (130s has elapsed)...
[92m12:13:29 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:29 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:29 | INFO     | translation          | MetricX WMT24 with references score: 25.000000
2025-08-29 12:13:29 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:13:29 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_id.jsonl
2025-08-29 12:13:29 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:13:29 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:13:29 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: TRANSLATION-EN-XX ----------
2025-08-29 12:13:29 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-EN-XX' using TranslationMetric
2025-08-29 12:13:29 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:13:29 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:13:29 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:13:29 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:13:30 | INFO     | openai_serving       | Still waiting (130s has elapsed)...
[92m12:13:30 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:30 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:30 | INFO     | openai_serving       | Still waiting (130s has elapsed)...
[92m12:13:30 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:30 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.90it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.87it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.95it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.93it/s]
2025-08-29 12:13:39 | INFO     | openai_serving       | Still waiting (140s has elapsed)...
[92m12:13:39 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:39 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:39 | INFO     | openai_serving       | Still waiting (140s has elapsed)...
[92m12:13:39 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:39 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:40 | INFO     | translation          | MetricX WMT24 score: 12.472656
2025-08-29 12:13:40 | INFO     | translation          | MetricX WMT24 with references score: 16.794271
2025-08-29 12:13:40 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:13:40 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_vi.jsonl
2025-08-29 12:13:40 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:13:40 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:13:40 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: VI | Task: TRANSLATION-XX-EN ----------
2025-08-29 12:13:40 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-XX-EN' using TranslationMetric
2025-08-29 12:13:40 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:13:40 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:13:40 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:13:40 | INFO     | seahelm_metric       | Calculating metrics...
2025-08-29 12:13:40 | INFO     | openai_serving       | Still waiting (140s has elapsed)...
2025-08-29 12:13:40 | INFO     | openai_serving       | OpenAI batch is completed
[92m12:13:40 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:40 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:40 | INFO     | openai_serving       | Still waiting (140s has elapsed)...
[92m12:13:40 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:40 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:13:41 - LiteLLM:ERROR[0m: logging_worker.py:61 - LoggingWorker cancelled: 
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
    coroutine = await self._queue.get()
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
    await getter
asyncio.exceptions.CancelledError
2025-08-29 12:13:41 | ERROR    | logging_worker       | LoggingWorker cancelled: 
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
                                                      |     coroutine = await self._queue.get()
                                                      |                 ^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
                                                      |     await getter
                                                      | asyncio.exceptions.CancelledError
2025-08-29 12:13:41 | INFO     | mt_bench             | All judgments have been obtained.
2025-08-29 12:13:41 | INFO     | mt_bench             | Successfully obtained all judgments.
                                                      | 
2025-08-29 12:13:41 | INFO     | mt_bench             | Win rate for category <writing>: 0.4166666666666667
2025-08-29 12:13:41 | INFO     | mt_bench             | Overall win rate: 0.4166666666666667
2025-08-29 12:13:41 | INFO     | mt_bench             | Weighted win rate: 0.4166666666666667
2025-08-29 12:13:41 | INFO     | seahelm_metric       | Metrics calculation completed!
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.91it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.86it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.95it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.93it/s]
2025-08-29 12:13:49 | INFO     | openai_serving       | Still waiting (150s has elapsed)...
[92m12:13:49 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:49 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:49 | INFO     | openai_serving       | Still waiting (150s has elapsed)...
[92m12:13:49 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:49 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:50 | INFO     | openai_serving       | Still waiting (150s has elapsed)...
[92m12:13:50 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:50 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:51 | INFO     | translation          | MetricX WMT24 score: 15.125000
2025-08-29 12:13:51 | INFO     | translation          | MetricX WMT24 with references score: 23.125000
2025-08-29 12:13:51 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:13:51 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_vi.jsonl
2025-08-29 12:13:51 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:13:51 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:13:51 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: TRANSLATION-EN-XX ----------
2025-08-29 12:13:51 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-EN-XX' using TranslationMetric
2025-08-29 12:13:51 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:13:51 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:13:51 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:13:51 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.83it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.82it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.91it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.89it/s]
2025-08-29 12:13:59 | INFO     | openai_serving       | Still waiting (160s has elapsed)...
[92m12:13:59 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:59 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:59 | INFO     | openai_serving       | Still waiting (160s has elapsed)...
[92m12:13:59 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:13:59 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:01 | INFO     | openai_serving       | Still waiting (160s has elapsed)...
[92m12:14:01 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:01 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:02 | INFO     | translation          | MetricX WMT24 score: 19.375000
2025-08-29 12:14:02 | INFO     | translation          | MetricX WMT24 with references score: 24.833333
2025-08-29 12:14:02 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:14:02 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_th.jsonl
2025-08-29 12:14:02 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:14:02 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:14:02 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TH | Task: TRANSLATION-XX-EN ----------
2025-08-29 12:14:02 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-XX-EN' using TranslationMetric
2025-08-29 12:14:02 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:14:02 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:14:02 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:14:02 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.89it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.85it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.93it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.92it/s]
2025-08-29 12:14:09 | INFO     | openai_serving       | Still waiting (170s has elapsed)...
[92m12:14:09 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:09 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:10 | INFO     | openai_serving       | Still waiting (170s has elapsed)...
[92m12:14:10 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:10 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:11 | INFO     | openai_serving       | Still waiting (170s has elapsed)...
[92m12:14:11 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:11 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:13 | INFO     | translation          | MetricX WMT24 score: 11.510417
2025-08-29 12:14:14 | INFO     | translation          | MetricX WMT24 with references score: 23.625000
2025-08-29 12:14:14 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:14:14 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_th.jsonl
2025-08-29 12:14:14 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:14:14 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:14:14 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: TRANSLATION-EN-XX ----------
2025-08-29 12:14:14 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-EN-XX' using TranslationMetric
2025-08-29 12:14:14 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:14:14 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:14:14 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:14:14 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.86it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.84it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.92it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.90it/s]
2025-08-29 12:14:20 | INFO     | openai_serving       | Still waiting (180s has elapsed)...
[92m12:14:20 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:20 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:20 | INFO     | openai_serving       | Still waiting (180s has elapsed)...
[92m12:14:20 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:20 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:21 | INFO     | openai_serving       | Still waiting (180s has elapsed)...
[92m12:14:21 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:21 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:25 | INFO     | translation          | MetricX WMT24 score: 20.458333
2025-08-29 12:14:25 | INFO     | translation          | MetricX WMT24 with references score: 24.833333
2025-08-29 12:14:25 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:14:25 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_ta.jsonl
2025-08-29 12:14:25 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:14:25 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:14:25 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TA | Task: TRANSLATION-XX-EN ----------
2025-08-29 12:14:25 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-XX-EN' using TranslationMetric
2025-08-29 12:14:25 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:14:25 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:14:25 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:14:25 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.88it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.86it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.94it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.92it/s]
2025-08-29 12:14:30 | INFO     | openai_serving       | Still waiting (190s has elapsed)...
[92m12:14:30 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:30 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:30 | INFO     | openai_serving       | Still waiting (190s has elapsed)...
2025-08-29 12:14:30 | INFO     | openai_serving       | OpenAI batch is completed
[92m12:14:30 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:30 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:14:30 - LiteLLM:ERROR[0m: logging_worker.py:61 - LoggingWorker cancelled: 
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
    coroutine = await self._queue.get()
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
    await getter
asyncio.exceptions.CancelledError
2025-08-29 12:14:30 | ERROR    | logging_worker       | LoggingWorker cancelled: 
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
                                                      |     coroutine = await self._queue.get()
                                                      |                 ^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
                                                      |     await getter
                                                      | asyncio.exceptions.CancelledError
2025-08-29 12:14:30 | INFO     | mt_bench             | All judgments have been obtained.
2025-08-29 12:14:30 | INFO     | mt_bench             | Successfully obtained all judgments.
                                                      | 
2025-08-29 12:14:30 | INFO     | mt_bench             | Win rate for category <writing>: 0.3333333333333333
2025-08-29 12:14:30 | INFO     | mt_bench             | Overall win rate: 0.3333333333333333
2025-08-29 12:14:30 | INFO     | mt_bench             | Weighted win rate: 0.3333333333333333
2025-08-29 12:14:30 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:14:31 | INFO     | openai_serving       | Still waiting (190s has elapsed)...
2025-08-29 12:14:31 | INFO     | openai_serving       | OpenAI batch is completed
[92m12:14:31 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:31 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:14:32 - LiteLLM:ERROR[0m: logging_worker.py:61 - LoggingWorker cancelled: 
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
    coroutine = await self._queue.get()
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
    await getter
asyncio.exceptions.CancelledError
2025-08-29 12:14:32 | ERROR    | logging_worker       | LoggingWorker cancelled: 
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
                                                      |     coroutine = await self._queue.get()
                                                      |                 ^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
                                                      |     await getter
                                                      | asyncio.exceptions.CancelledError
2025-08-29 12:14:32 | INFO     | mt_bench             | All judgments have been obtained.
2025-08-29 12:14:32 | INFO     | mt_bench             | Successfully obtained all judgments.
                                                      | 
2025-08-29 12:14:32 | INFO     | mt_bench             | Win rate for category <Knowledge III>: 0.5
2025-08-29 12:14:32 | INFO     | mt_bench             | Overall win rate: 0.5
2025-08-29 12:14:32 | INFO     | mt_bench             | Weighted win rate: 0.5
2025-08-29 12:14:32 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:14:36 | INFO     | translation          | MetricX WMT24 score: 19.854167
2025-08-29 12:14:36 | INFO     | translation          | MetricX WMT24 with references score: 25.000000
2025-08-29 12:14:36 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:14:36 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_ta.jsonl
2025-08-29 12:14:36 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:14:36 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:14:36 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: TRANSLATION-EN-XX ----------
2025-08-29 12:14:36 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-EN-XX' using TranslationMetric
2025-08-29 12:14:36 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:14:36 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:14:36 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:14:36 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.90it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.85it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.95it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.93it/s]
2025-08-29 12:14:40 | INFO     | openai_serving       | Still waiting (200s has elapsed)...
[92m12:14:40 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:40 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:47 | INFO     | translation          | MetricX WMT24 score: 16.583333
2025-08-29 12:14:47 | INFO     | translation          | MetricX WMT24 with references score: 24.083333
2025-08-29 12:14:47 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:14:47 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-EN-XX' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-en-xx_tl.jsonl
2025-08-29 12:14:47 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:14:47 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-EN-XX' completed!
                                                      | 
2025-08-29 12:14:47 | INFO     | seahelm_evaluation   | --------- Evaluation | Lang: TL | Task: TRANSLATION-XX-EN ----------
2025-08-29 12:14:47 | INFO     | seahelm_evaluation   | Evaluating 'TRANSLATION-XX-EN' using TranslationMetric
2025-08-29 12:14:47 | INFO     | seahelm_metric       | Replacing error responses with ""
2025-08-29 12:14:47 | INFO     | seahelm_metric       | Post processing responses...
2025-08-29 12:14:47 | INFO     | seahelm_metric       | Post processing of responses completed!
2025-08-29 12:14:47 | INFO     | seahelm_metric       | Calculating metrics...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.89it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.85it/s]2025-08-29 12:14:50 | INFO     | openai_serving       | Still waiting (210s has elapsed)...
[92m12:14:50 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:14:50 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.94it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.92it/s]
2025-08-29 12:14:58 | INFO     | translation          | MetricX WMT24 score: 15.520833
2025-08-29 12:14:58 | INFO     | translation          | MetricX WMT24 with references score: 24.750000
2025-08-29 12:14:58 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:14:58 | INFO     | seahelm_evaluation   | Saving inference results for task 'TRANSLATION-XX-EN' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_translation-xx-en_tl.jsonl
2025-08-29 12:14:58 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:14:58 | INFO     | seahelm_evaluation   | Evaluation for task 'TRANSLATION-XX-EN' completed!
                                                      | 
2025-08-29 12:14:58 | INFO     | seahelm_evaluation   | Waiting for mt-bench evaluation to complete
2025-08-29 12:15:00 | INFO     | openai_serving       | Still waiting (220s has elapsed)...
[92m12:15:00 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:15:00 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:15:11 | INFO     | openai_serving       | Still waiting (230s has elapsed)...
[92m12:15:11 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:15:11 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:15:21 | INFO     | openai_serving       | Still waiting (240s has elapsed)...
[92m12:15:21 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:15:21 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:15:31 | INFO     | openai_serving       | Still waiting (250s has elapsed)...
[92m12:15:31 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:15:31 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:15:41 | INFO     | openai_serving       | Still waiting (260s has elapsed)...
[92m12:15:41 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:15:41 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:15:52 | INFO     | openai_serving       | Still waiting (270s has elapsed)...
[92m12:15:52 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:15:52 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:16:02 | INFO     | openai_serving       | Still waiting (280s has elapsed)...
[92m12:16:02 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:16:02 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:16:12 | INFO     | openai_serving       | Still waiting (290s has elapsed)...
[92m12:16:12 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:16:12 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:16:22 | INFO     | openai_serving       | Still waiting (300s has elapsed)...
[92m12:16:22 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:16:22 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:16:32 | INFO     | openai_serving       | Still waiting (310s has elapsed)...
[92m12:16:32 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:16:32 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:16:43 | INFO     | openai_serving       | Still waiting (320s has elapsed)...
[92m12:16:43 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:16:43 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:16:53 | INFO     | openai_serving       | Still waiting (330s has elapsed)...
2025-08-29 12:16:53 | INFO     | openai_serving       | OpenAI batch is completed
[92m12:16:53 - LiteLLM:ERROR[0m: logging_worker.py:55 - LoggingWorker error: 'NoneType' object has no attribute 'get'
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
    await asyncio.wait_for(coroutine, timeout=self.timeout)
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
    litellm_metadata.get("batch_ignore_default_logging", False) is True
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'
2025-08-29 12:16:53 | ERROR    | logging_worker       | LoggingWorker error: 'NoneType' object has no attribute 'get'
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 53, in _worker_loop
                                                      |     await asyncio.wait_for(coroutine, timeout=self.timeout)
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
                                                      |     return fut.result()
                                                      |            ^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 2098, in async_success_handler
                                                      |     litellm_metadata.get("batch_ignore_default_logging", False) is True
                                                      |     ^^^^^^^^^^^^^^^^^^^^
                                                      | AttributeError: 'NoneType' object has no attribute 'get'
[92m12:16:53 - LiteLLM:ERROR[0m: logging_worker.py:61 - LoggingWorker cancelled: 
Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
    coroutine = await self._queue.get()
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
    await getter
asyncio.exceptions.CancelledError
2025-08-29 12:16:53 | ERROR    | logging_worker       | LoggingWorker cancelled: 
                                                      | Traceback (most recent call last):
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py", line 51, in _worker_loop
                                                      |     coroutine = await self._queue.get()
                                                      |                 ^^^^^^^^^^^^^^^^^^^^^^^
                                                      |   File "/home/user/.pyenv/versions/3.11.9/lib/python3.11/asyncio/queues.py", line 158, in get
                                                      |     await getter
                                                      | asyncio.exceptions.CancelledError
2025-08-29 12:16:53 | INFO     | mt_bench             | All judgments have been obtained.
2025-08-29 12:16:53 | INFO     | mt_bench             | Successfully obtained all judgments.
                                                      | 
2025-08-29 12:16:53 | INFO     | mt_bench             | Win rate for category <writing>: 0.3333333333333333
2025-08-29 12:16:53 | INFO     | mt_bench             | Overall win rate: 0.3333333333333333
2025-08-29 12:16:53 | INFO     | mt_bench             | Weighted win rate: 0.3333333333333333
2025-08-29 12:16:53 | INFO     | seahelm_metric       | Metrics calculation completed!
2025-08-29 12:16:53 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_id.jsonl
2025-08-29 12:16:53 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:16:53 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_vi.jsonl
2025-08-29 12:16:53 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:16:53 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_th.jsonl
2025-08-29 12:16:53 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:16:53 | INFO     | seahelm_evaluation   | Saving inference results for task 'MT-BENCH' to output-vertex_ai/08-29-12-10-02/gemini-2.5-flash/inference/gemini-2.5-flash_mt-bench_tl.jsonl
2025-08-29 12:16:53 | INFO     | seahelm_evaluation   | Inference results saved!
2025-08-29 12:16:53 | INFO     | aggregate_metrics    | ---------- Aggregation of metrics ----------
2025-08-29 12:16:53 | INFO     | aggregate_metrics    | ---------- Aggregation | Lang: ID ----------
2025-08-29 12:16:53 | INFO     | aggregate_metrics    | ### Competency: NLU
2025-08-29 12:16:53 | INFO     | aggregate_metrics    | Overall normalized accuracy for <id_nlu>: 26.666667
                                                      | 
2025-08-29 12:16:53 | INFO     | aggregate_metrics    | ### Competency: SAFETY
2025-08-29 12:16:53 | INFO     | aggregate_metrics    | Overall normalized accuracy for <id_safety>: 0.000000
                                                      | 
2025-08-29 12:16:53 | INFO     | aggregate_metrics    | ### Competency: NLG
2025-08-29 12:16:53 | INFO     | aggregate_metrics    | Overall normalized accuracy for <id_nlg>: 23.362991
                                                      | 
2025-08-29 12:16:53 | INFO     | aggregate_metrics    | ### Competency: NLR
2025-08-29 12:16:53 | INFO     | aggregate_metrics    | Overall normalized accuracy for <id_nlr>: 0.000000
                                                      | 
2025-08-29 12:16:53 | INFO     | aggregate_metrics    | ### Competency: LINGUISTIC-DIAGNOSTICS
2025-08-29 12:16:53 | INFO     | aggregate_metrics    | ---------- Task: PRAGMATICS (ID) ----------
2025-08-29 12:16:53 | INFO     | aggregate_metrics    | Accuracy for phenomenon <ID_scalar_implicatures>: 0 / 6 : 0.000000
Traceback (most recent call last):
  File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_evaluation.py", line 1059, in <module>
    seahelm_eval.run_evaluation(
  File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_evaluation.py", line 891, in run_evaluation
    metrics = aggregate_metrics(metrics, config=self.config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/aggregate_metrics.py", line 126, in aggregate_metrics
    metrics = aggregate_pragmatics_metrics(metrics, lang)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/llm-bench-with-gemini/SEA-HELM/seahelm_tasks/aggregate_metrics.py", line 35, in aggregate_pragmatics_metrics
    subset_accuracy = correct_count / total_count
                      ~~~~~~~~~~~~~~^~~~~~~~~~~~~
ZeroDivisionError: division by zero

Execution finished at 08-29 12:16:56
